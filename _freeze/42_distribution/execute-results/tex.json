{
  "hash": "01023388518cd7d2961a9a7877af227f",
  "result": {
    "markdown": "# Summarizing Distributions {#distribution}\n\n\n\n\n\n\n## Expectation \n\nWe often want to summarize some characteristics of the distribution of a random variable.  The most important summary is the expectation (or expected value, or mean), in which the possible values of a random variable are weighted by their probabilities.\n\n:::{#def-}\n### Expectation of a Discrete Random Variable\nThe expected value of a discrete random variable $Y$ is \n\n$$E(Y)=\\sum\\limits_{y} y P(Y=y)= \\sum\\limits_{y} y p(y)$$  \n\nIn words, it is the weighted average of all possible values of $Y$, weighted by the probability that $y$ occurs.  It is not necessarily the number we would expect $Y$ to take on, but the average value of $Y$ after a large number of repetitions of an experiment.\n:::\n\n\n:::{#exm-expectdiscrete}\n\nWhat is the expectation of a fair, six-sided die?\n:::\n\n\n__Expectation of a Continuous Random Variable__:  The expected\nvalue of a continuous random variable is similar in concept to that of\nthe discrete random variable, except that instead of summing using\nprobabilities as weights, we integrate using the density to weight.\nHence, the expected value of the continuous variable $Y$ is defined by\n\n$$E(Y)=\\int\\limits_{y} y f(y) dy$$\n\n\n\n:::{#exm-expectconti}\n### Expectation of a Continuous Random Variable\nFind $E(Y)$ for $f(y)=\\frac{1}{1.5}, \\quad 0<y<1.5$.\n:::\n\n\n### Expected Value of a Function {-}\n\nRemember: An Expected Value is a type of weighted average. We can extend this to composite functions. For random variable $Y$,\n\nIf $Y$ is Discrete with PMF $p(y)$,  \n\n\n$$E[g(Y)]=\\sum\\limits_y g(y)p(y)$$\n\n\nIf $Y$ is Continuous with PDF $f(y)$,\n\n\n$$E[g(Y)]=\\int\\limits_{-\\infty}^\\infty g(y)f(y)dy$$\n\n\n\n### Properties of Expected Values {-}\n\nDealing with Expectations is easier when the thing inside is a sum. The intuition behind this that Expectation is an integral, which is a type of sum. \n\n:::{#prop-}\n1. Expectation of a constant is a constant $$E(c)=c$$\n2. Constants come out $$E(c g(Y))= c E(g(Y))$$\n4. Expectation is Linear $$E(g(Y_1) + \\cdots + g(Y_n))=E(g(Y_1)) +\\cdots+E(g(Y_n)),$$ regardless of independence\n2. Expected Value of Expected Values: $$E(E(Y)) = E(Y)$$ (because the expected value of a random variable is a constant)\n\nFinally, if $X$ and $Y$ are independent, even products are easy:\n\n\n$$X \\;\\; \\mathrm{ and } \\;\\; Y \\mathrm{are independent } \\;\\Rightarrow\\; E(XY) = E(X)E(Y)$$\n\n:::\n\n__Conditional Expectation__: With joint distributions, we are often interested in the expected value of a variable $Y$ if we could hold the other variable $X$ fixed.  This is the conditional expectation of $Y$ given $X = x$:\n\n1. $Y$ discrete:  $E(Y|X = x) = \\sum_y yp_{Y|X}(y|x)$\n2. $Y$ continuous: $E(Y|X = x) = \\int_y yf_{Y|X}(y|x)dy$\n\n\nThe conditional expectation is often used for prediction when one knows the value of $X$ but not $Y$\n\n\n## Variance and Covariance\n\nWe can also look at other summaries of the distribution, which build on the idea of taking expectations. Variance tells us about the \"spread\" of the distribution; it is the expected value of the squared deviations from the mean of the distribution.  The standard deviation is simply the square root of the variance.\n\n:::{#def-}\n### Variance\nThe Variance of a Random Variable $Y$ is\n  \n\n$$\\text{Var}(Y) = E[(Y - E(Y))^2] =  E(Y^2)-[E(Y)]^2$$\n\n  \nThe Standard Deviation is the square root of the variance : \n\n\n$$SD(Y) = \\sigma_Y= \\sqrt{\\text{Var}(Y)}$$\n\n:::\n\n\n\n:::{#exm-var}\n\nGiven the following PMF: \n\n$$f(x) = \\begin{cases}\n    \\frac{3!}{x!(3-x)!}(\\frac{1}{2})^3 \\quad x = 0,1,2,3\\\\\n      0 \\quad otherwise\n  \\end{cases}$$\n\nWhat is $\\text{Var}(x)$?\n  \n__Hint:__ First calculate $E(X)$ and $E(X^2)$\n\n:::\n\n:::{#def-}\n### Covariance\nThe covariance measures the degree to which two random variables vary together; if the covariance between $X$ and $Y$ is positive, X tends to be larger than its mean when Y is larger than its mean.  \n\n\n$$\\text{Cov}(X,Y) = E[(X - E(X))(Y - E(Y))] $$\n\nWe can also write this as \n\n\\begin{align*}\n\\text{Cov}(X,Y) &= E\\left(XY - XE(Y) - E(X)Y + E(X)E(Y)\\right)\\\\\n&= E(XY) - E(X)E(Y) - E(X)E(Y) + E(X)E(Y)\\\\\n&= E(XY) - E(X)E(Y)\n\\end{align*}\n\n:::\n\nThe covariance of a variable with itself is the variance of that variable.\n\nThe Covariance is unfortunately hard to interpret in magnitude. The correlation is a standardized version of the covariance, and always ranges from -1 to 1. \n\n:::{#def-}\n### Correlation\nThe correlation coefficient is the covariance divided by the standard deviations of $X$ and $Y$.  It is a unitless measure and always takes on values in the interval $[-1,1]$.\n\n\n$$\\text{Corr}(X, Y) = \\frac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}(X)\\text{Var}(Y)}} = \\frac{\\text{Cov}(X,Y)}{SD(X)SD(Y)}$$\n\n:::\n\n___Properties of Variance and Covariance:___\n\n:::{#prop-}\n1. $\\text{Var}(c) = 0$\n2. $\\text{Var}(cY) = c^2 \\text{Var}(Y)$\n3. $\\text{Cov}(Y,Y) = \\text{Var}(Y)$\n4. $\\text{Cov}(X,Y) = \\text{Cov}(Y,X)$\n5. $\\text{Cov}(aX,bY) = ab \\text{Cov}(X,Y)$\n6. $\\text{Cov}(X+a,Y) =  \\text{Cov}(X,Y)$\n7. $\\text{Cov}(X+Z,Y+W) = \\text{Cov}(X,Y) + \\text{Cov}(X,W) + \\text{Cov}(Z,Y) + \\text{Cov}(Z,W)$\n8. $\\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y) + 2\\text{Cov}(X,Y)$\n:::\n\n\n:::{#exr-expvar}\n### Expectation and Variance\nSuppose we have a PMF with the following characteristics:\n\\begin{align*}\n  P(X = -2) &= \\frac{1}{5}\\\\\n  P(X = -1) &= \\frac{1}{6}\\\\\n  P(X = 0) &= \\frac{1}{5}\\\\\n  P(X = 1) &= \\frac{1}{15}\\\\\n  P(X = 2) &= \\frac{11}{30}\n\\end{align*}\n  \n1. Calculate the expected value of X\n\nDefine the random variable $Y = X^2$. \n\n2. Calculate the expected value of $Y$. (Hint: It would help to derive the PMF of $Y$ first in order to calculate the expected value of $Y$ in a straightforward way)\n\n3. Calculate the variance of $X$.\n\n:::\n\n:::{#exr-expvar2}\nGiven the following PDF: \n\n$$f(x) = \\begin{cases} \\frac{3}{10}(3x - x^2) \\quad 0 \\leq x \\leq 2\\\\ 0 \\quad \\mathrm{ otherwise} \\end{cases}$$\n\nFind the expectation and variance of $X$.\n:::\n\n:::{#exr-expvar3}\n\nFind the mean and standard deviation of random variable $X$. The PDF of this $X$ is as follows:\n\n$$f(x) = \\begin{cases} \\frac{1}{4}x \\quad 0 \\leq x \\leq 2\\\\ \\frac{1}{4}(4 - x)  \\quad 2 \\leq x \\leq 4\\\\ 0 \\quad \\mathrm{ otherwise} \\end{cases}$$\n\nNext, calculate $P(X < \\mu - \\sigma)$ Remember, $\\mu$ is the mean and $\\sigma$ is the standard deviation.\n:::\n\n\n## Common Distributions\n\nTwo _discrete_ distributions used often are:  \n\n\n:::{#def-}\n### Binomial Distribution\n$Y$ is distributed binomial if it represents the number of \"successes\" observed in $n$ independent, identical \"trials,\" where the probability of success in any trial is $p$ and the probability of failure is $q=1-p$.\n:::\n\nFor any particular sequence of $y$ successes and $n-y$ failures, the probability of obtaining that sequence is $p^y q^{n-y}$ (by the multiplicative law and independence).  However, there are $\\binom{n}{y}=\\frac{n!}{(n-y)!y!}$ ways of obtaining a sequence with $y$ successes and $n-y$ failures.  So the binomial distribution is given by $$p(y)=\\binom{n}{y}p^y q^{n-y}, \\quad y=0,1,2,\\ldots,n$$ with mean $\\mu=E(Y)=np$ and variance $\\sigma^2=\\text{Var}(Y)=npq$.\n\n:::{#exm-}\nRepublicans vote for Democrat-sponsored bills 2\\% of the time. What is the probability that out of 10 Republicans questioned, half voted for a particular Democrat-sponsored bill?  What is the mean number of Republicans voting for Democrat-sponsored bills?  The variance?\n1. $P(Y=5)=$\n2. $E(Y)=$ \n3. $\\text{Var}(Y)=6$\n:::\n\n\n:::{#def-}\n### Poisson Distribution\nA random variable $Y$ has a Poisson distribution if \n\n\n$$P(Y = y)=\\frac{\\lambda^y}{y!}e^{-\\lambda}, \\quad y=0,1,2,\\ldots, \\quad \\lambda>0$$\n\n\nThe Poisson has the unusual feature that its expectation equals its variance: $E(Y)=\\text{Var}(Y)=\\lambda$. The Poisson distribution is often used to model rare event counts:  counts of the number of events that occur during some unit of time.  $\\lambda$ is often called the \"arrival rate.\"\n:::\n   \n\n:::{#exm-}\nBorder disputes occur between two countries through a Poisson Distribution, at a rate of 2 per month.  What is the probability of 0, 2, and less than 5 disputes occurring in a month?\n:::\n\n\nTwo _continuous_  distributions used often are: \n\n\n:::{#def-}\n### Uniform Distribution\nA random variable $Y$ has a continuous uniform distribution on the interval $(\\alpha,\\beta)$ if its density is given by $$f(y)=\\frac{1}{\\beta-\\alpha}, \\quad \\alpha\\le y\\le \\beta$$  The mean and variance of $Y$ are $E(Y)=\\frac{\\alpha+\\beta}{2}$ and $\\text{Var}(Y)=\\frac{(\\beta-\\alpha)^2}{12}$.\n:::\n\n:::{#exm-}\nFor $Y$ uniformly distributed over $(1,3)$, what are the following probabilities?\n\n1. $P(Y=2)$\n2. Its density evaluated at 2, or $f(2)$\n3. $P(Y \\le 2)$\n4. $P(Y > 2)$\n\n:::\n\n\n:::{#def-}\n### Normal Distribution\nA random variable $Y$ is normally distributed with mean $E(Y)=\\mu$ and variance $\\text{Var}(Y)=\\sigma^2$ if its density is \n\n\n$$f(y)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y-\\mu)^2}{2\\sigma^2}}$$\n\n:::\n\nSee Figure @fig-normaldens are various Normal Distributions with the same $\\mu = 1$ and two versions of the variance. \n\n\n::: {.cell hash='42_distribution_cache/pdf/fig-normaldens_13bbebea7a9655730205e064716598eb'}\n::: {.cell-output-display}\n![Normal Distribution Density](42_distribution_files/figure-pdf/fig-normaldens-1.pdf){#fig-normaldens}\n:::\n:::\n\n\n## Joint Distributions\n\nOften, we are interested in two or more random variables defined on the same sample space.  The distribution of these variables is called a joint distribution.  Joint distributions can be made up of any combination of discrete and continuous random variables.\n\n__Joint Probability Distribution__: If both $X$ and $Y$ are random variable, their joint probability mass/density function assigns probabilities to each pair of outcomes\n\nDiscrete: \n\n\n$$p(x, y) = P(X = x, Y = y)$$\n\n\nsuch that  $p(x,y) \\in [0,1]$ and $$\\sum\\sum p(x,y) = 1$$\n\n\nContinuous: \n\n\n$$f(x,y);P((X,Y) \\in A) = \\int\\!\\!\\!\\int_A f(x,y)dx dy $$\n\n\ns.t. $f(x,y)\\ge 0$ and \n\n\n$$\\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty f(x,y)dxdy = 1$$\n\n\nIf X and Y are independent, then $P(X=x,Y=y) = P(X=x)P(Y=y)$ and $f(x,y) = f(x)f(y)$\n\n\n__Marginal Probability Distribution__: probability distribution of only one of the two variables (ignoring information about the other variable), we can obtain the marginal distribution by summing/integrating across the variable that we don't care about:\n\n* Discrete: $p_X(x) = \\sum_i p(x, y_i)$\n* Continuous: $f_X(x) = \\int_{-\\infty}^\\infty f(x,y)dy$\n\n__Conditional Probability Distribution__: probability distribution for one variable, holding the other variable fixed.  Recalling from the previous lecture that $P(A|B)=\\frac{P(A\\cap B)}{P(B)}$, we can write the conditional distribution as\n\n* Discrete: $p_{Y|X}(y|x) = \\frac{p(x,y)}{p_X(x)}, \\quad p_X(x) > 0$\n* Continuous: $f_{Y|X}(y|x) = \\frac{f(x,y)}{f_X(x)},\\quad f_X(x) > 0$\n\n\n:::{#exr-}\nSuppose we are interested in the outcomes of flipping a coin and rolling a 6-sided die at the same time.  The sample space for this process contains 12 elements: $$\\{(H, 1), (H, 2), (H, 3), (H, 4), (H, 5), (H, 6), (T, 1), (T, 2), (T, 3), (T, 4), (T, 5), (T, 6)\\}$$  We can define two random variables $X$ and $Y$ such that $X = 1$ if heads and $X = 0$ if tails, while $Y$ equals the number on the die.  \n\nWe can then make statements about the joint distribution of $X$ and $Y$. What are the following?\n\n1. $P(X=x)$\n2. $P(Y=y)$\n3. $P(X=x, Y=y)$\n4. $P(X=x|Y=y)$\n5. Are X and Y independent? \n:::\n\n\n\n## Answers to Examples and Exercises  {-}\n\nAnswer to Example @exm-expectdiscrete:\n\n$E(Y)=7/2$ \n\nWe would never expect the result of a rolled die to be $7/2$, but that would be the average over a large number of rolls of the die.\n\n\nAnswer to @exm-expectconti\n\n0.75\n\n\n\nAnswer to @exm-var:\n\n\n$E(X) = 0 \\times \\frac{1}{8} + 1 \\times \\frac{3}{8} + 2 \\times \\frac{3}{8} + 3 \\times \\frac{1}{8} = \\frac{3}{2}$\n\nSince there is a 1 to 1 mapping from $X$ to $X^2:$ $E(X^2) = 0 \\times \\frac{1}{8} + 1 \\times \\frac{3}{8} + 4 \\times \\frac{3}{8} + 9 \\times \\frac{1}{8} = \\frac{24}{8} = 3$\n\n\\begin{align*}\n\\text{Var}(x) &= E(X^2) - E(x)^2\\\\\n&= 3 - (\\frac{3}{2})^2\\\\\n&= \\frac{3}{4}\n\\end{align*}\n\n\nAnswer to @exr-expvar:\n\n1. $E(X) = -2(\\frac{1}{5}) + -1(\\frac{1}{6}) + 0(\\frac{1}{5}) + 1(\\frac{1}{15}) + 2(\\frac{11}{30}) = \\frac{7}{30}$\n\n2. $E(Y) = 0(\\frac{1}{5}) + 1(\\frac{7}{30}) + 4(\\frac{17}{30}) = \\frac{5}{2}$\n\n3. \n\n\\begin{align*}\n\\text{Var}(X) &= E[X^2] - E[X]^2\\\\\n&= E(Y) - E(X)^2\\\\\n&= \\frac{5}{2} - \\frac{7}{30}^2 \\approx 2.45\n\\end{align*}\n\nAnswer to @exr-expvar2:\n\n1. expectation = $\\frac{6}{5}$, variance = $\\frac{6}{25}$\n\nAnswer to @exr-expvar3:\n\n1. mean = 2, standard deviation = $\\sqrt{\\frac{2}{3}}$\n\n2. $\\frac{1}{8}(2 - \\sqrt{\\frac{2}{3}})^2$",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}