{
  "hash": "e5a41382c5fa2d25356dac9777c6ed08",
  "result": {
    "markdown": "# Optimization {#optim}\n\n\n\n\n\n\nTo optimize, we use derivatives and calculus. Optimization is to find the maximum or minimum of a functon, and to find what value of an input gives that extremum. This has obvious uses in engineering. Many tools in the statistical toolkit use optimization. One of the most common ways of estimating a model is through \"Maximum Likelihood Estimation\", done via optimizing a function (the likelihood). \n\nOptimization also comes up in Economics, Formal Theory, and Political Economy all the time. A go-to model of human behavior is that they optimize a certain utility function. Humans are not pure utility maximizers, of course, but nuanced models of optimization -- for example, adding constraints and adding uncertainty -- will prove to be quite useful.\n\n\n\n\n\n## Maxima and Minima\n\nThe first derivative, $f'(x)$, quantifies the slope of a function. Therefore, it can be used to check whether the function $f(x)$ at the point $x$ is increasing or decreasing at $x$.\n\n\n\n1. __Increasing:__ $f'(x)>0$\n1. __Decreasing:__ $f'(x)<0$\n1. __Neither increasing nor decreasing__: $f'(x)=0$\n             i.e. a maximum, minimum, or saddle point\n\nSo for example, $f(x) = x^2 + 2$ and $f^\\prime(x) = 2x$ \n\n\n\n::: {.cell hash='33_optimization_cache/pdf/unnamed-chunk-2_58ef4974a5021db23cad340af5e9d007'}\n::: {.cell-output-display}\n![Maxima and Minima](33_optimization_files/figure-pdf/unnamed-chunk-2-1.pdf)\n:::\n:::\n\n\n\n:::{#exr-}\n### Plotting a mazimum and minimum\nPlot $f(x)=x^3+ x^2 + 2$, plot its derivative, and identifiy where the derivative is zero. Is there a maximum or minimum?\n:::\n\n\n\n::: {.cell hash='33_optimization_cache/pdf/unnamed-chunk-3_305d9588879cb4c51d8981ecfb280512'}\n\n:::\n\n\n\nThe second derivative $f''(x)$ identifies whether the function $f(x)$ at the point $x$ is\n\n1. Concave / concave down: $f''(x)<0$\n2. Convex / Concave up: $f''(x)>0$\n\n__Maximum (Minimum)__: $x_0$ is a __local maximum (minimum)__ if $f(x_0)>f(x)$ ($f(x_0)<f(x))$ for all $x$ within some open interval containing $x_0$.  $x_0$ is a __global maximum (minimum)__ if $f(x_0)>f(x)$ ($f(x_0)<f(x))$ for all $x$ in the domain of $f$.\n\nGiven the function $f$ defined over domain $D$, all of the following are defined as __critical points__:\n\n1. Any interior point of $D$ where $f'(x)=0$.\n2. Any interior point of $D$ where $f'(x)$ does not exist.\n3. Any endpoint that is in $D$.\n\nThe maxima and minima will be a subset of the critical points.\n\n__Second Derivative Test of Maxima/Minima__:  We can use the second derivative to tell us whether a point is a maximum or minimum of $f(x)$. \n\n1. Local Maximum: $f'(x)=0$ and $f''(x)<0$\n1. Local Minimum: $f'(x)=0$ and $f''(x)>0$\n1. Need more info: $f'(x)=0$ and $f''(x)=0$\n        \n\n__Global Maxima and Minima__ Sometimes no global max or min exists --- e.g., $f(x)$ not bounded above or below.  However, there are three situations where we can fairly easily identify global max or min.\n\n1. __Functions with only one critical point.__ If $x_0$ is a local max or min of $f$ and it is the only critical point, then it is the global max or min.\n2. __Globally concave up or concave down functions.__  If $f''(x)$ is never zero, then there is at most one critical point. That critical point is a global maximum if $f''<0$ and a global minimum if $f''>0$.\n3. __Functions over closed and bounded intervals__ must have both a global maximum and a global minimum.\n\n\n\n:::{#exm-}\n### Maxima and Minima by drawing\n\nFind any critical points and identify whether they are a max, min, or saddle point:\n\n1. $f(x)=x^2+2$\n2. $f(x)=x^3+2$\n3. $f(x)=|x^2-1|$, $x\\in [-2,2]$\n\n:::\n\n\n## Concavity of a Function\n\nConcavity helps identify the curvature of a function, $f(x)$, in 2 dimensional space.\n\n:::{#def-}\n### Concave Function\nA function $f$ is strictly concave over the set S \\underline{if} $\\forall x_1,x_2 \\in S$ and $\\forall a \\in (0,1)$, $$f(ax_1 + (1-a)x_2) > af(x_1) + (1-a)f(x_2)$$\n\\textit{Any} line connecting two points on a concave function will lie \\textit{below} the function.\n:::\n\n\n\n::: {.cell hash='33_optimization_cache/pdf/unnamed-chunk-4_e010fac6c6c494d4d9d15c7dfee7095d'}\n::: {.cell-output-display}\n![](33_optimization_files/figure-pdf/unnamed-chunk-4-1.pdf)\n:::\n:::\n\n\n\n:::{#def-}\n### Convex Function\nConvex: A function f is strictly convex over the set S \\underline{if} $\\forall x_1,x_2 \\in S$ and $\\forall a \\in (0,1)$, $$f(ax_1 + (1-a)x_2) < af(x_1) + (1-a)f(x_2)$$\n\n  Any line connecting two points on a convex function will lie above the function.\n:::\n\n\n**Second Derivative Test of Concavity**: The second derivative can be used to understand concavity. \n\nIf\n\n$$\\left\\{\\begin{array}{lll}\nf''(x) < 0 & \\Rightarrow & \\text{Concave}\\\\\nf''(x) > 0 & \\Rightarrow & \\text{Convex}\n\\end{array}\\right.$$\n\n\n### Quadratic Forms {-}\n\nQuadratic forms is shorthand for a way to summarize a function. This is important for finding concavity because\n\n1. Approximates local curvature around a point --- e.g., used to\nidentify max vs min vs saddle point.\n2. They are simple to express even in $n$ dimensions:\n3. Have a matrix representation.\n\n__Quadratic Form__:  A polynomial where each term is a monomial\nof degree 2 in any number of variables:\n\n\\begin{align*}\n\\text{One variable: }& Q(x_1) = a_{11}x_1^2\\\\\n\\text{Two variables: }& Q(x_1,x_2) = a_{11}x_1^2 + a_{12}x_1x_2 + a_{22}x_2^2\\\\\n\\text{N variables: }& Q(x_1,\\cdots,x_n)=\\sum\\limits_{i\\le j} a_{ij}x_i x_j\n\\end{align*}\n\nwhich can be written in matrix terms:\n\nOne variable\n\n\n$$Q(\\mathbf{x}) = x_1^\\top a_{11} x_1$$\n\n\nN variables: \n\\begin{align*}\nQ(\\mathbf{x}) &=\\begin{bmatrix} x_1 & x_2 & \\cdots & x_n \\end{bmatrix}\\begin{bmatrix}\na_{11}&\\frac{1}{2}a_{12}&\\cdots&\\frac{1}{2}a_{1n}\\\\\n\\frac{1}{2}a_{12}&a_{22}&\\cdots&\\frac{1}{2}a_{2n}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\n\\frac{1}{2}a_{1n}&\\frac{1}{2}a_{2n}&\\cdots&a_{nn}\n\\end{bmatrix}\n\\begin{bmatrix} x_1\\\\x_2\\\\\\vdots\\\\x_n\\end{bmatrix}\\\\\n&= \\mathbf{x}^\\top\\mathbf{Ax}\n\\end{align*}\n\nFor example, the  Quadratic on $\\mathbb{R}^2$:\n\\begin{align*}\n  Q(x_1,x_2)&=\\begin{bmatrix} x_1& x_2 \\end{bmatrix} \\begin{bmatrix} a_{11}&\\frac{1}{2} a_{12}\\\\\n  \\frac{1}{2}a_{12}&a_{22}\\end{bmatrix} \\begin{bmatrix} x_1\\\\x_2 \\end{bmatrix} \\\\\n  &= a_{11}x_1^2 + a_{12}x_1x_2 + a_{22}x_2^2\n\\end{align*}\n\n\n\n\n### Definiteness of Quadratic Forms {-}\n\n\nWhen the function $f(\\mathbf{x})$ has more than two inputs, determining whether it has a maxima and minima (remember, functions may have many inputs but they have only one output) is a bit more tedious.  Definiteness helps identify the curvature of a function, $Q(\\textbf{x})$, in n dimensional space.\n\n__Definiteness__:  By definition, a quadratic form always takes on the value of zero when $x = 0$, $Q(\\textbf{x})=0$ at $\\textbf{x}=0$. The definiteness of the matrix $\\textbf{A}$ is  determined by whether the\nquadratic form $Q(\\textbf{x})=\\textbf{x}^\\top\\textbf{A}\\textbf{x}$ is greater than zero, less than\nzero, or sometimes both over all $\\mathbf{x}\\ne 0$.\n\n\n\n## Gradient and FOC\n\nWe can see from a graphical representation that if a point is a local maxima or minima, it must meet certain conditions regarding its derivative. These are so commonly used that we refer these to \"First Order Conditions\" (FOCs) and \"Second Order Conditions\" (SOCs) in the economic tradition.\n\nWhen we examined functions of one variable $x$, we found critical points by taking the first derivative,  setting it to zero, and solving for $x$.  For functions of $n$ variables, the critical points are found in much the same way, except now we set the partial derivatives equal to zero. Note: We will only consider critical points on the interior of a function's domain. \n\nIn a derivative, we only took the derivative with respect to one variable at a time. When we take the derivative separately with respect to all variables in the elements of $\\mathbf{x}$ and then express the result as a vector, we use the term Gradient and Hessian. \n\n\n:::{#def-}\n\n### Gradient\n\nGiven a function $f(\\textbf{x})$ in $n$ variables, the gradient $\\nabla f(\\mathbf{x})$ (the greek letter nabla ) is a row vector, where the $i$th element is the partial derivative of $f(\\textbf{x})$ with respect to $x_i$: \n\n\n$$\\nabla f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f(\\mathbf{x})}{\\partial x_1} &  \\frac{\\partial f(\\mathbf{x})}{\\partial x_2} & \\cdots & \\frac{\\partial f(\\mathbf{x})}{\\partial x_n} \\end{bmatrix}$$\n\n\n:::\n\n\nThe gradient points in the direction of the steepest rate of increase at each point $\\mathbf{x}$.\n\nBefore we know whether a point is a maxima or minima, if it meets the FOC it is a \"Critical Point\".\n\n:::{#def-}\n\n### Critical Point\n\n$\\mathbf{x}^*$ is a critical point if and only if $\\nabla f(\\mathbf{x}^*)=\\mathbf{0}$ (the vector of zeros). If the partial derivative of f(x) with respect to $x^*$ is 0, then $\\mathbf{x}^*$ is a critical point. To solve for $\\mathbf{x}^*$, find the gradient, set each element equal to 0, and solve the system of equations. $$\\mathbf{x}^* = \\begin{bmatrix} x_1^*\\\\x_2^*\\\\ \\vdots \\\\ x_n^*\\end{bmatrix}$$\n  \n:::\n\n\n\n\n\n:::{#exm-}\n\nExample: Given a function $f(\\mathbf{x})=(x_1-1)^2+x_2^2+1$, find the (1) Gradient and (2) Critical point of $f(\\mathbf{x})$.\n\n:::\n\n:::{.solution}\nGradient \n\n\\begin{align*}\n\\nabla f(\\mathbf{x}) &= \n  \\begin{bmatrix}\n  \\frac{\\partial f(\\mathbf{x})}{\\partial x_1} & \n  \\frac{\\partial f(\\mathbf{x})}{\\partial x_2} \n  \\end{bmatrix}\n  = \n  \\begin{bmatrix} 2(x_1-1) &\n  2x_2 \n  \\end{bmatrix}\n\\end{align*}\n\n\nCritical Point $\\mathbf{x}^*$:\n  \n\\begin{align*}\n&\\frac{\\partial f(\\mathbf{x})}{\\partial x_1} = 2(x_1-1) = 0 & \\Rightarrow x_1^* = 1\\\\\n&\\frac{\\partial f(\\mathbf{x})}{\\partial x_2} = 2x_2 = 0 & \\Rightarrow   x_2^* = 0\\\\\n\\end{align*}\n\n\nSo $$\\mathbf{x}^* = (1,0)$$\n\n:::\n\n\n\n\n## Hessian and SOC\n\nWhen we found a critical point for a function of one variable, we used the second derivative as a indicator of the curvature at the point in order to determine whether the point was a min, max, or saddle (second derivative test of concavity).  For functions of $n$ variables, we use _second order partial derivatives_ as an indicator of curvature.\n\n:::{#def-}\n\n### Hessian\n\nGiven a function $f(\\mathbf{x})$ in $n$ variables, the hessian $\\mathbf{H(x)}$ is\nan $n\\times n$ matrix, where the $(i,j)$th element is the second order\npartial derivative of $f(\\mathbf{x})$ with respect to $x_i$ and $x_j$:\n\n\n$$\\mathbf{H(x)}=\\begin{bmatrix} \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_1^2}&\\frac{\\partial^2f(\\mathbf{x})}{\\partial x_1 \\partial x_2}& \\cdots & \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_1 \\partial x_n}\\\\ \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_2 \\partial x_1}&\\frac{\\partial^2f(\\mathbf{x})}{\\partial x_2^2}& \\cdots & \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_2 \\partial x_n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_n \\partial x_1}&\\frac{\\partial^2f(\\mathbf{x})}{\\partial x_n \\partial x_2}& \\cdots & \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_n^2} \\end{bmatrix}$$\n\n\n:::\n\n\n\n\nNote that the hessian will be a symmetric matrix because $\\frac{\\partial f(\\mathbf{x})}{\\partial x_1\\partial x_2} = \\frac{\\partial f(\\mathbf{x})}{\\partial x_2\\partial x_1}$.\n\nAlso note that given that $f(\\mathbf{x})$ is of quadratic form, each element of the hessian will be a constant.\n\n\nThese definitions will be employed when we determine the __Second Order Conditions__ of a function:\n\nGiven a function $f(\\mathbf{x})$ and a point $\\mathbf{x}^*$ such that $\\nabla f(\\mathbf{x}^*)=0$,\n\n1. Hessian is Positive Definite around $\\mathbf{x}^*$ $\\quad \\Longrightarrow \\quad$ Local Min\n2. Hessian is Negative Definite around $\\mathbf{x}^*$ $\\quad \\Longrightarrow \\quad$ Local Max\n3. Hessian is Indefinite around $\\mathbf{x}^*$ $\\quad \\Longrightarrow \\quad$ Saddle Point\n\n\nFurthermore, there's an easier way to check whether a $2\\times 2$ matrix is positive or negative definite.\n\n**Definiteness of 2 $\\times$ 2 Matrix**:  For a $2 \\times 2$ matrix \n\n$$\\mathbf{A}=\\begin{bmatrix}A_{11} & A_{12}\\\\ A_{21} & A_{22}\\end{bmatrix}$$\n\n\n1. If $\\mathrm{det}(\\mathbf{A}) > 0$ and $A_{11}>0$, then $\\mathbf{A}$ is positive definite\n1. If $\\mathrm{det}(\\mathbf{A}) > 0$ and $A_{11}<0$, then $\\mathbf{A}$ is negative definite\n1. If $\\mathrm{det}(\\mathbf{A}) < 0$, then $\\mathbf{A}$ is indefinite\n\n\n\n:::{#exm-}\n\n### Max and min with two dimensions\n\nWe found that the only critical point of\n$f(\\mathbf{x})=(x_1-1)^2+x_2^2+1$ is at $\\mathbf{x}^*=(1,0)$.  Is it a min, max, or\nsaddle point?\n\n:::\n\n\n:::{.solution}\n\nThe Hessian is\n\\begin{align*}\n\\mathbf{H(x)} &= \\begin{bmatrix} 2&0\\\\0&2 \\end{bmatrix}\n\\end{align*}\nSince $\\mathrm{det}(\\mathbf{H}(x)) = 4 > 0$ and $H_{11}=2>0$, the  Hessian is positive definite.\n\nMaxima, Minima, or Saddle Point? Since the Hessian is positive definite and the gradient equals 0, $x^\\star = (1,0)$ is a local minimum.\n\nNote: Alternate check of definiteness. Is $\\mathbf{H(x^*)} \\geq \\leq 0 \\quad \\forall \\quad \\mathbf{x}\\ne 0$\n  \n\n\\begin{align*}\n\\mathbf{x}^\\top H(\\mathbf{x}^*) \\mathbf{x} &= \\begin{bmatrix} x_1 & x_2 \\end{bmatrix}\\\\\n&= \\begin{bmatrix} 2&0\\\\0&2 \\end{bmatrix}\\\\\n\\begin{bmatrix} x_1\\\\x_2\\end{bmatrix} &= 2x_1^2+2x_2^2\n\\end{align*}\n\nFor any $\\mathbf{x}\\ne 0$, $2(x_1^2+x_2^2)>0$, so the Hessian is \tpositive definite and $\\mathbf{x}^*$ is a strict local minimum.\n  \n:::\n\n\n\n### Definiteness and Concavity {-}\n\nAlthough definiteness helps us to understand the curvature of an n-dimensional function, it does not necessarily tell us whether the function is globally concave or convex.\n\nWe need to know whether a function is globally concave or convex to determine whether a critical point is a global min or max.  We can use the definiteness of the Hessian to determine whether a function is globally concave or convex:\n\n1. Hessian is Positive Semidefinite $\\forall \\mathbf{x}$ $\\quad \\Longrightarrow \\quad$ Globally Convex\n2. Hessian is Negative Semidefinite  $\\forall \\mathbf{x}$ $\\quad \\Longrightarrow \\quad$ Globally Concave\n\n\nNotice that the definiteness conditions must be satisfied over the entire domain.\n\n\n## Global Maxima and Minima\n\n__Global Max/Min Conditions__: Given a function $f(\\mathbf{x})$ and a point $\\mathbf{x}^*$ such that $\\nabla f(\\mathbf{x}^*)=0$,\n\n1. $f(\\mathbf{x})$ Globally Convex $\\quad\n\\Longrightarrow \\quad$ Global Min\n2. $f(\\mathbf{x})$ Globally Concave $\\quad\n\\Longrightarrow \\quad$ Global Max\n\nNote that showing that $\\mathbf{H(x^*)}$ is negative semidefinite is\nnot enough to guarantee $\\mathbf{x}^*$ is a local max.  However, showing that\n$\\mathbf{H(x)}$ is negative semidefinite for all $\\mathbf{x}$ guarantees that $x^*$\nis a global max.  (The same goes for positive semidefinite and minima.)\n\n:::{#exm-} \n\nTake $f_1(x)=x^4$ and $f_2(x)=-x^4$.  \n\n- Both have $x=0$ as a critical point.  \n- Unfortunately, $f''_1(0)=0$ and $f''_2(0)=0$, so we can't tell whether $x=0$ is a min or max for either.  However, $f''_1(x)=12x^2$ and $f''_2(x)=-12x^2$.  \n- For all $x$, $f''_1(x)\\ge 0$ and $f''_2(x)\\le 0$ --- i.e., $f_1(x)$ is globally convex and $f_2(x)$ is globally concave. \n- So $x=0$ is a global min of $f_1(x)$ and a global max of $f_2(x)$.\n:::\n\n:::{#exr-}\nGiven $f(\\mathbf{x})=x_1^3-x_2^3+9x_1x_2$, find any maxima or minima.\n:::\n\n*Solution.*\n\n1. First order conditions\n    (1) Gradient $\\nabla f(\\mathbf{x})$\n          $$\\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \n        \\frac{\\partial f}{\\partial x_2}\\end{bmatrix} =\n        \\begin{bmatrix} 3x_1^2+9x_2 \\\\ -3x_2^2+9x_1 \\end{bmatrix}$$\n    (2) Critical Points $\\mathbf{x^*}$\n        - Set the gradient equal to zero and solve for $x_1$ and $x_2$. We have two equations and two unknowns. Solving for $x_1$ and $x_2$, we get two critical points: $\\mathbf{x}_1^*=(0,0)$ and $\\mathbf{x}_2^*=(3,-3)$.\n        - $3x_1^2 + 9x_2 = 0 \\quad \\Rightarrow \\quad  9x_2 = -3x_1^2 \\quad \\Rightarrow \\quad  x_2 = -\\frac{1}{3}x_1^2$\n        - $-3x_2^2 + 9x_1 = 0 \\quad \\Rightarrow \\quad -3(-\\frac{1} {3}x_1^2)^2 + 9x_1 = 0$\n        - $\\Rightarrow \\quad -\\frac{1}{3}x_1^4 + 9x_1 = 0 \\quad \\Rightarrow \\quad x_1^3 = 27x_1 \\quad \\Rightarrow \\quad x_1 = 3$\n        - $3(3)^2 + 9x_2 = 0 \\quad \\Rightarrow \\quad x_2 = -3$\n \n2. Second order conditions.  \n    (1) Hessian $\\mathbf{H(x)}$\n\n        $$\\begin{bmatrix} 6x_1&9\\\\9&-6x_2 \\end{bmatrix}$$\n\n    (2) Hessian $\\mathbf{H(x_1^*)}$\n\n        $$\\begin{bmatrix} 0&9\\\\9&0\\end{bmatrix}$$\n\n    (3) Leading principal minors of $\\mathbf{H(x_1^*)}$\n\n        $$M_1=0; M_2=-81$$\n\n    (4) Definiteness of $\\mathbf{H(x_1^*)}$?\n        - $\\mathbf{H(x_1^*)}$ is indefinite\n    (5) Maxima, Minima, or Saddle Point for $\\mathbf{x_1^*}$?\n        - Since $\\mathbf{H(x_1^*)}$ is indefinite, $\\mathbf{x}_1^*=(0,0)$ is a saddle point.\n    (6) Hessian \n\n        $$\\mathbf{H(x_2^*)}=\\begin{bmatrix} 18&9\\\\9&18\\end{bmatrix}$$\n\n    (7) Leading principal minors of $\\mathbf{H(x_2^*)}$\n\n        $$M_1=18; M_2=243$$\n\n    (8) Definiteness of $\\mathbf{H(x_2^*)}$?\n        - $\\mathbf{H(x_2^*)}$ is positive definite\n    (9) Maxima, Minima, or Saddle Point for $\\mathbf{x}_2^*$?\n        - Since $\\mathbf{H(x_2^*)}$ is positive definite, $\\mathbf{x}_1^*=(3,-3)$ is a strict local minimum\n    \n3. Global concavity/convexity.  \n    (1) Is f(x) globally concave/convex?\n        - No. In evaluating the Hessians for $\\mathbf{x}_1^*$ and $\\mathbf{x}_2^*$ we saw that the Hessian is not positive semidefinite at x $=$ (0,0).\n    (2) Are any $\\mathbf{x^*}$ global minima or maxima?\n        - No. Since the function is not globally concave/convex, we can't infer that $\\mathbf{x}_2^*=(3,-3)$ is a global minimum. In fact, if we set $x_1=0$, the $f(\\mathbf{x})=-x_2^3$, which will go to $-\\infty$ as $x_2\\to \\infty$.\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}