{
  "hash": "b502e86fbd4d817af94eb540516a6e38",
  "result": {
    "markdown": "# Simulation {#simulation}\n\nModule originally written by Connor Jerzak and Shiro Kuriwaki\n\n\n\n\n\n### Motivation: Simulation as an Analytical Tool {-}\n\nAn increasing amount of political science contributions now include a simulation. \n\n* [Axelrod (1977)](http://www-personal.umich.edu/~axe/research/Dissemination.pdf) demonstrated via simulation how atomized individuals evolve to be grouped in similar clusters or countries, a model of culture.^[[Axelrod, Robert. 1997. \"The Dissemination of Culture.\" _Journal of Conflict Resolution_ 41(2): 203–26.](http://www-personal.umich.edu/~axe/research/Dissemination.pdf)]\n* [Chen and Rodden (2013)](http://www-personal.umich.edu/~jowei/florida.pdf) argued in a 2013 article that the vote-seat inequality in U.S. elections that is often attributed to intentional partisan gerrymandering can actually attributed to simply the reality of \"human geography\" -- Democratic voters tend to be concentrated in smaller area. Put another way, no feasible form of gerrymandering could spread out Democratic voters in such a way to equalize their vote-seat translation effectiveness. After demonstrating the empirical pattern of human geography, they advance their key claim by simulating thousands of redistricting plans and record the vote-seat ratio.^[[Chen, Jowei, and Jonathan Rodden.  \"Unintentional Gerrymandering: Political Geography and Electoral Bias in Legislatures. _Quarterly Journal of Political Science_, 8:239-269\"](http://www-personal.umich.edu/~jowei/florida.pdf)]\n* [Gary King, James Honaker, and multiple other authors](https://gking.harvard.edu/files/abs/evil-abs.shtml) propose a way to analyze missing data with a method of multiple imputation, which uses a lot of simulation from a researcher's observed dataset.^[[King, Gary, et al. \"Analyzing Incomplete Political Science Data: An Alternative Algorithm for Multiple Imputation\". _American Political Science Review_, 95: 49-69.](https://gking.harvard.edu/files/abs/evil-abs.shtml)]  (Software: Amelia^[[James Honaker, Gary King, Matthew Blackwell (2011). Amelia II: A Program for Missing Data. Journal of\n  Statistical Software, 45(7), 1-47.](http://www.jstatsoft.org/v45/i07/)])\n\nStatistical methods also incorporate simulation: \n  \n* The bootstrap: a statistical method for estimating uncertainty around some parameter by re-sampling observations. \n* Bagging: a method for improving machine learning predictions by re-sampling observations, storing the estimate across many re-samples, and averaging these estimates to form the final estimate. A variance reduction technique. \n* Statistical reasoning: if you are trying to understand a quantitative problem, a wonderful first-step to understand the problem better is to simulate it! The analytical solution is often very hard (or impossible), but the simulation is often much easier :-) \n\n\n\n### Where are we? Where are we headed? {-}\n\nUp till now, you should have covered:\n\n* `R` basics\n* Visualization\n* Matrices and vectors\n* Functions, objects, loops\n* Joining real data\n\n\nIn this module, we will start to work with generating data within R, from thin air, as it were. Doing simulation also strengthens your understanding of Probability.\n\n\n### Check your Understanding {-}\n\n* What does the `sample()` function do?\n* What does `runif()` stand for?\n* What is a `seed`?\n* What is a Monte Carlo?\n\n\nCheck if you have an idea of how you might code the following tasks:\n\n* Simulate 100 rolls of a die\n* Simulate one random ordering of 25 numbers\n* Simulate 100 values of white noise (uniform random variables)\n* Generate a \"bootstrap\" sample of an existing dataset\n\nWe're going to learn about this today!\n\n\n\n## Pick a sample, any sample\n\n## The `sample()` function\n\nThe core functions for coding up stochastic data revolves around several key functions, so we will simply review them here.  \n\nSuppose you have a vector of values `x` and from it you want to randomly sample a sample of length `size`. For this, use the `sample` function\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-2_99294aaae8ad09497574649c31ea1576'}\n\n```{.r .cell-code}\nsample(x = 1:10, size = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 9 8 6 5 1\n```\n:::\n:::\n\n\nThere are two subtypes of sampling -- with and without replacement.\n\n1. Sampling without replacement (`replace = FALSE`) means once an element of `x` is chosen, it will not be considered again:\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-3_bf625c3eca1e3f21a3657f2bf544d60f'}\n\n```{.r .cell-code}\nsample(x = 1:10, size = 10, replace = FALSE) ## no number appears more than once\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  3  9 10  2  1  6  8  4  7  5\n```\n:::\n:::\n\n\n2. Sampling with replacement (`replace = TRUE`) means that even if an element of `x` is chosen, it is put back in the pool and may be chosen again. \n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-4_b7960043c7f434b4ad029d5dced4c207'}\n\n```{.r .cell-code}\nsample(x = 1:10, size = 10, replace = TRUE) ## any number can appear more than once\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  5 10  6  4 10 10  3  2  5  1\n```\n:::\n:::\n\n\nIt follows then that you cannot sample without replacement a sample that is larger than the pool.\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-5_d8784d1df3f1815d55db3143a0f0ad4a'}\n\n```{.r .cell-code}\nsample(x = 1:10, size = 100, replace = FALSE)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in sample.int(length(x), size, replace, prob): cannot take a sample larger than the population when 'replace = FALSE'\n```\n:::\n:::\n\n\n\nSo far, every element in `x` has had an equal probability of being chosen. In some application, we want a sampling scheme where some elements are more likely to be chosen than others. The argument `prob` handles this.\n\nFor example, this simulates 20 fair coin tosses (each outcome is equally likely to happen)\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-6_0503286e7f8772c8ccfcd618a2b6f92d'}\n\n```{.r .cell-code}\nsample(c(\"Head\", \"Tail\"), size = 20, prob = c(0.5, 0.5), replace = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"Tail\" \"Head\" \"Head\" \"Head\" \"Head\" \"Head\" \"Head\" \"Tail\" \"Head\" \"Head\"\n[11] \"Tail\" \"Head\" \"Tail\" \"Tail\" \"Head\" \"Head\" \"Tail\" \"Tail\" \"Head\" \"Tail\"\n```\n:::\n:::\n\n\n\nBut this simulates 20 biased coin tosses, where say the probability of Tails is 4 times more likely than the number of Heads\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-7_ff478005bbaf2f182f9e7ebaf07a7d28'}\n\n```{.r .cell-code}\nsample(c(\"Head\", \"Tail\"), size = 20, prob = c(0.2, 0.8), replace = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"Tail\" \"Head\" \"Tail\" \"Tail\" \"Tail\" \"Head\" \"Tail\" \"Head\" \"Tail\" \"Tail\"\n[11] \"Head\" \"Tail\" \"Tail\" \"Tail\" \"Tail\" \"Tail\" \"Tail\" \"Tail\" \"Tail\" \"Tail\"\n```\n:::\n:::\n\n\n### Sampling rows from a dataframe\n\nIn tidyverse, there is a convenience function to sample rows randomly: `sample_n()` and `sample_frac()`. \n\nFor example, load the dataset on cars, `mtcars`, which has 32 observations.\n\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-8_40bc4974de6b7e11e20234d4e39f9926'}\n\n```{.r .cell-code}\nmtcars\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n```\n:::\n:::\n\nsample_n picks a user-specified number of rows from the dataset:\n\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-9_44c8e197ee4b35eaf9318cb7c6a0d842'}\n\n```{.r .cell-code}\nsample_n(mtcars, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                mpg cyl  disp  hp drat    wt qsec vs am gear carb\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.9  1  0    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.9  1  1    4    1\nFord Pantera L 15.8   8 351.0 264 4.22 3.170 14.5  0  1    5    4\n```\n:::\n:::\n\n\nSometimes you want a X percent sample of your dataset. In this case use `sample_frac()`\n\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-10_6bd1d5ade4d819e688e6823a8897ef6b'}\n\n```{.r .cell-code}\nsample_frac(mtcars, 0.10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corona     21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nFerrari Dino      19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n```\n:::\n:::\n\n\nAs a side-note, these functions have very practical uses for any type of data analysis:\n\n* Inspecting your dataset: using `head()` all the same time and looking over the first few rows might lead you to ignore any issues that end up in the bottom for whatever reason.\n* Testing your analysis with a small sample: If running analyses on a dataset takes more than a handful of seconds, change your dataset upstream to a fraction of the size so the rest of the code runs in less than a second. Once verifying your analysis code runs, then re-do it with your full dataset (by simply removing the `sample_n` / `sample_frac` line of code in the beginning). While three seconds may not sound like much, they accumulate and eat up time.\n\n\n## Random numbers from specific distributions\n\n### `rbinom()` {-}\n`rbinom` builds upon `sample` as a tool to help you answer the question -- what is the _total number of successes_ I would get if I  sampled a binary (Bernoulli) result from a test with `size` number of trials each, with a event-wise probability of `prob`. The first argument `n` asks me how many such numbers I want.\n\nFor example, I want to know how many Heads I would get if I flipped a fair coin 100 times. \n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-11_ee97f57bbe96a360afbdece3b00abdf4'}\n\n```{.r .cell-code}\nrbinom(n = 1, size = 100, prob = 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 48\n```\n:::\n:::\n\n\nNow imagine this I wanted to do this experiment 10 times, which would require I flip the coin 10 x 100 = 1000 times! Helpfully, we can do this in one line\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-12_3421ec2852e1b190d7936a09e622626d'}\n\n```{.r .cell-code}\nrbinom(n = 10, size = 100, prob = 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 52 60 43 52 52 53 49 51 54 54\n```\n:::\n:::\n\n\n### `runif()` {-}\n`runif` also simulates a stochastic scheme where each event has equal probability of getting chosen like `sample`, but is a continuous rather than discrete system.  We will cover this more in the next math module.\n\nThe intuition to emphasize here is that one can generate potentially infinite amounts (size `n`) of noise that is a essentially random\n\n\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-13_541fb46b6988226995510a08195cb4d2'}\n\n```{.r .cell-code}\nrunif(n = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4901431 0.1499494 0.6383251 0.1004919 0.5810503\n```\n:::\n:::\n\n\n\n### `rnorm()` {-}\n\n`rnorm` is also a continuous distribution, but draws from a Normal distribution -- perhaps the most important distribution in statistics. It runs the same way as `runif`\n\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-14_99c78311a91e8d7fe5eb2912d1e84218'}\n\n```{.r .cell-code}\nrnorm(n = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -2.51846262 -0.09770950  0.07450981 -0.16810119  1.46179707\n```\n:::\n:::\n\n\nTo better visualize the difference between the output of `runif` and `rnorm`, let's generate lots of each and plot a histogram.\n\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-15_06d7c8bc05e9be60b359e829d2f9c24d'}\n\n```{.r .cell-code}\nfrom_runif <- runif(n = 1000)\nfrom_rnorm <- rnorm(n = 1000)\n\npar(mfrow = c(1, 2)) ## base-R parameter for two plots at once\nhist(from_runif)\nhist(from_rnorm)\n```\n\n::: {.cell-output-display}\n![](14_simulation_files/figure-epub/unnamed-chunk-15-1.png)\n:::\n:::\n\n\n\n## r, p, and d\n\nEach distribution can do more than generate random numbers (the prefix `r`). We can compute the cumulative probability by the function `pbinom()`, `punif()`, and `pnorm()`. Also the density -- the value of the PDF -- by `dbinom()`, `dunif()` and `dnorm()`.  \n\n\n\n## `set.seed()`\n\n`R` doesn't have the ability to generate truly random numbers! Random numbers are actually very hard to generate. (Think: flipping a coin --> can be perfectly predicted if I know wind speed, the angle the coin is flipped, etc.). Some people use random noise in the atmosphere or random behavior in quantum systems to generate \"truly\" (?) random numbers. Conversely, R uses deterministic algorithms which take as an input a \"seed\" and which then perform a series of operations to generate a sequence of random-seeming numbers (that is, numbers whose sequence is sufficiently hard to predict).\n\nLet's think about this another way. Sampling is a stochastic process, so every time you run `sample()` or `runif()` you are bound to get a different output (because different random seeds are used). This is intentional in some cases but you might want to avoid it in others. For example, you might want to diagnose a coding discrepancy by setting the random number generator to give the same number each time. To do this, use the function `set.seed()`.\n\nIn the function goes any number. When you run a sample function in the same command as a preceding `set.seed()`, the sampling function will always give you the same sequence of numbers. In a sense, the sampler is no longer random (in the sense of unpredictable to use; remember: it never was \"truly\" random in the first place)\n\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-16_de2ad7a0cebfd4cd34e93cdfb9f5fe6b'}\n\n```{.r .cell-code}\nset.seed(02138)\nrunif(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.51236144 0.61530551 0.37451441 0.43541258 0.21166530 0.17812129\n [7] 0.04420775 0.45567854 0.88718264 0.06970056\n```\n:::\n:::\n\n\nThe random number generator should  give you the exact same sequence of numbers if you precede the function by the same seed, \n\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-17_30b26eafd6c6b2cb4af089172e196af6'}\n\n```{.r .cell-code}\nset.seed(02138)\nrunif(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.51236144 0.61530551 0.37451441 0.43541258 0.21166530 0.17812129\n [7] 0.04420775 0.45567854 0.88718264 0.06970056\n```\n:::\n:::\n\n\n\n\n## Exercises {-}\n\n### Census Sampling {-}\n\nWhat can we learn from surveys of populations, and how wrong do we get if our sampling is biased?^[This example is inspired from [Meng, Xiao-Li  (2018). Statistical paradises and paradoxes in big data (I): Law of large populations, big data paradox, and the 2016 US presidential election. _Annals of  Applied Statistics_ 12:2, 685–726. doi:10.1214/18-AOAS1161SF.](https://statistics.fas.harvard.edu/files/statistics-2/files/statistical_paradises_and_paradoxes.pdf)]  Suppose we want to estimate the proportion of U.S. residents who are non-white (`race != \"White\"`). In reality, we do not have any population dataset to utilize and so we _only see the sample survey_. Here, however, to understand how sampling works, let's conveniently use the Census extract in some cases and pretend we didn't in others.\n\n\n(a) First, load `usc2010_001percent.csv` into your R session. After loading the `library(tidyverse)`, browse it. Although this is only a 0.01 percent extract, treat this as your population for pedagogical purposes. What is the population proportion of non-White residents?\n\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-18_28c69f80eecfda6d45b7eb95805cfa0c'}\n\n:::\n\n\n\n\n(b) Setting a seed to `1669482`, sample 100 respondents from this sample. What is the proportion of non-White residents in this _particular_ sample? By how many percentage points are you off from (what we labelled as) the true proportion?\n\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-19_8e548dd22f51be8f732f0c97598d7fb5'}\n\n:::\n\n\n\n(c) Now imagine what you did above was one survey. What would we get if we did 20 surveys? \n\nTo simulate this, write a loop that does the same exercise 20 times, each time computing a sample proportion. Use the same seed at the top, but be careful to position the `set.seed` function such that it generates the same sequence of 20 samples, rather than 20 of the same sample. \n\nTry doing this with a `for` loop and storing your sample proportions in a new length-20 vector. (Suggestion: make an empty vector first as a container). After running the loop, show a histogram of the 20 values. Also what is the average of the 20 sample estimates?\n\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-20_8f10e25aff1b873d8a3115648dda2532'}\n\n:::\n\n\n\n(d) Now, to make things more real, let's introduce some response bias. The goal here is not to correct response bias but to induce it and see how it affects our estimates.  Suppose that non-White residents are 10 percent less likely to respond to enter your survey than White respondents. This is plausible if you think that the Census is from 2010 but you are polling in 2018, and racial minorities are more geographically mobile than Whites.  Repeat the same exercise in (c) by modeling this behavior. \n\nYou can do this by creating a variable, e.g. `propensity`, that is 0.9 for non-Whites and 1 otherwise. Then, you can refer to it in the propensity argument.\n\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-21_efd492acc4e4839cd297add25dc6b52b'}\n\n:::\n\n\n\n(e) Finally, we want to see if more data (\"Big Data\") will improve our estimates. Using the same unequal response rates framework as (d), repeat the same exercise but instead of each poll collecting 100 responses, we collect 10,000. \n\n\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-22_5e3102d74f5d3da0bb64db9e59ba5717'}\n\n:::\n\n\n\n(f) Optional - visualize your 2 pairs of 20 estimates, with a bar showing the \"correct\" population average. \n\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-23_d62ec4f78f83231842ebd6b668ca1358'}\n\n:::\n\n\n\n\n\n\n### Conditional Proportions {-}\n\nThis example is not on simulation, but is meant to reinforce some of the probability discussion from math lecture. \n\nRead in the Upshot Siena poll from Fall 2016, `data/input/upshot-siena-polls.csv`.\n\nIn addition to some standard demographic questions, we will focus on one called `vt_pres_2` in the csv. This is a two-way presidential vote question, asking respondents who they plan to vote for President if the election were held today -- Donald Trump, the Republican, or Hilary Clinton, the Democrat, with options for Other candidates as well. For this problem, use the two-way vote question rather than the 4-way vote question. \n\n(a)  Drop the the respondents who answered the November poll (i.e. those for which `poll == \"November\"`). We do this in order to ignore this November population in all subsequent parts of this question because they were not asked the Presidential vote question. \n\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-24_9bd39fa8cda777fefef700e23207bf3d'}\n\n:::\n\n\n\n(b) Using the dataset after the procedure in (a), find the proportion of _poll respondents_ (those who are in the sample) who support Donald Trump. \n\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-25_427708c71e07ad4cfa8423f9c7275753'}\n\n:::\n\n\n\n(c)  Among those who supported Donald Trump, what proportion of them has a Bachelor's degree or higher (i.e. have a Bachelor's, Graduate, or other Professional Degree)?\n\n\n(d)  Among those who did not support Donald Trump (i.e. including supporters of Hilary Clinton, another candidate, or those who refused to answer the question), what proportion of them has a Bachelor's degree or higher? \n\n\n(e)  Express the numbers in the previous parts as probabilities of specified events.  Define your own symbols: For example, we can let $T$ be the event that a randomly selected respondent in the poll supports Donald Trump, then the proportion in part (b) is the probability $P(T).$ \n\n\n\n(f) Suppose we randomly sampled a person who participated in the survey and found that he/she had a Bachelor's degree or higher. Given this evidence, what is the probability that the same person supports Donald Trump? Use Bayes Rule and show your work -- that is, do not use data or R to compute the quantity directly.  Then, verify this is the case via R.\n\n\n\n\n\n\n### The Birthday problem {-}\n\nWrite code that will answer the well-known birthday problem via simulation.^[This exercise draws from Imai (2017)]\n\nThe problem is fairly simple: Suppose $k$ people gather together in a room. What is the probability at least two people share the same birthday?\n\nTo simplify reality a bit, assume that (1) there are no leap years, and so there are always 365 days in a year, and (2) a given individual's birthday is randomly assigned and independent from each other. \n\n\n_Step 1_: Set `k` to a concrete number. Pick a number from 1 to 365 randomly, `k` times to simulate birthdays (would this be with replacement or without?).\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-26_491e4cfbf41c0f1d9478dcf916615813'}\n\n```{.r .cell-code}\n# Your code\n```\n:::\n\n\n\n_Step 2_: Write a line (or two) of code  that gives a `TRUE` or `FALSE` statement of whether or not at least two people share the same birth date.\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-27_95abe4fdb5765409d9e64ab706f8e4ed'}\n\n```{.r .cell-code}\n# Your code\n```\n:::\n\n\n\n_Step 3_: The above steps will generate a `TRUE` or `FALSE` answer for your event of interest, but only for one realization of an event in the sample space. In order to estimate the _probability_ of your event happening, we need a \"stochastic\", as opposed to \"deterministic\", method. To do this, write a loop that does Steps 1 and 2 repeatedly for many times, call that number of times `sims`. For each of `sims` iteration, your code should give you a `TRUE` or `FALSE` answer. Code up a way to store these estimates.\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-28_f5324852b6fdbee0b9167b7483d08fcc'}\n\n```{.r .cell-code}\n# Your code\n```\n:::\n\n\n\n_Step 4_: Finally, generalize the function further by letting `k` be a user-defined number. You have now created a _Monte Carlo simulation_! \n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-29_e45e4a0909cbc5bdfebf0e9f29708eec'}\n\n```{.r .cell-code}\n# Your code\n```\n:::\n\n\n\n_Step 5_: Generate a table or plot that shows how the probability of sharing a birthday changes by `k` (fixing `sims` at a large number like `1000`). Also generate a similar plot that shows how the probability of sharing a birthday changes by `sims` (fixing `k` at some arbitrary number like `10`). \n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-30_2bfce812d02c9f3a3307b0811b8a886f'}\n\n```{.r .cell-code}\n# Your code\n```\n:::\n\n\n\n_Extra credit_: Give an \"analytical\" answer to this problem, that is an answer through deriving the mathematical expressions of the probability.\n\n::: {.cell hash='14_simulation_cache/epub/unnamed-chunk-31_b53128ea7ad46125a04a8da11260a178'}\n\n```{.r .cell-code}\n# Your equations\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": {},
    "postProcess": true
  }
}