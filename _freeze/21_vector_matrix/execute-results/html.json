{
  "hash": "be1d21c0ca6b3a7cde20953ab4ae5f2c",
  "result": {
    "markdown": "\n# Matrix Operations\n\n## Vector {#vector-def}\n\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/fNk_zzaMoSs\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen style=\"border:1;display:block;margin:10px auto;\"></iframe>\n\n\n__Vector__: A vector in $n$-space is an ordered list of $n$ numbers.  These numbers can be represented as either a row vector or a column vector: \n\n$${\\bf v} = \\begin{bmatrix} v_1 & v_2 & \\dots & v_n\\end{bmatrix}$$\n\n$${\\bf v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}$$ \n\n        \nWe can also think of a vector as defining a point in $n$-dimensional space, usually $\\mathbb{R}^n$; each element of the vector defines the coordinate of the point in a particular direction.\n\n__Vector Addition and Subtraction__:  If two vectors, ${\\bf u}$ and ${\\bf v}$, have the same size (i.e. have the same number of elements), they can be added (subtracted) together:\n\n$${\\bf u} + {\\bf v} = \\begin{bmatrix} u_1 + v_1 \\\\ u_2 + v_2 \\\\ \\vdots \\\\ u_k + v_n \\end{bmatrix}$$\n\n$${\\bf u} - {\\bf v} = \\begin{bmatrix} u_1 - v_1 \\\\ u_2 - v_2 \\\\ \\vdots \\\\ u_k - v_n \\end{bmatrix}$$\n\n\nGeometrically, vector addition is witnessed by placing the two vectors, $\\mathbf{a}$ and $\\mathbf{b}$, _tail-to-head_. The result, $\\mathbf{a}+\\mathbf{b}$, is the vector from the open tail to the open head. This is demonstrated in @fig-vectoradd.\n\n\n::: {.cell layout-align=\"center\" fig='true' hash='21_vector_matrix_cache/html/fig-vectoradd_d8939c7c819625c97a96011fe4e54e82'}\n::: {.cell-output-display}\n![Geometry of Vector Addition [(Source)](https://shainarace.github.io/LinearAlgebra/mult.html)](images/vectoradd.png){#fig-vectoradd fig-align='center' width=40%}\n:::\n:::\n\n\nWhen subtracting vectors as $\\mathbf{a}-\\mathbf{b}$ we simply add $-\\mathbf{b}$ to $\\mathbf{a}$. The vector $-\\mathbf{b}$ has the same length as $\\mathbf{b}$ but points in the opposite direction. This vector has the same length as the one which connects the two heads of $\\mathbf{a}$ and $\\mathbf{b}$ as shown in @fig-vectorsub. \n\n\n::: {.cell layout-align=\"center\" fig='true' hash='21_vector_matrix_cache/html/fig-vectorsub_41b1d59172edb6aa9796e3f59fc2db5b'}\n::: {.cell-output-display}\n![Geometry of Vector Subtraction [(Source)](https://shainarace.github.io/LinearAlgebra/mult.html)](images/vectorsub.jpg){#fig-vectorsub fig-align='center' width=40%}\n:::\n:::\n\n\n\n__Scalar Multiplication__:  The product of a scalar $c$ (i.e. a constant) and vector ${\\bf v}$ is: \\\n\n$$ c{\\bf v} =  \\begin{bmatrix} cv_1 \\\\ cv_2 \\\\ \\dots \\\\ cv_n \\end{bmatrix} $$\n\n\nScalar multiplication changes the length of a vector but not the overall direction (although a negative scalar will scale the vector in the opposite direction through the origin). We can see this geometric interpretation of scalar multiplication in @fig-vectormult.\n\n\n\n::: {.cell layout-align=\"center\" fig='true' hash='21_vector_matrix_cache/html/fig-vectormult_277341a735744f1b97946339be17c8eb'}\n::: {.cell-output-display}\n![Geometric Effect of Scalar Multiplication [(Source)](https://shainarace.github.io/LinearAlgebra/mult.html)](images/vectormult.jpg){#fig-vectormult fig-align='center' width=40%}\n:::\n:::\n\n\n## Special Vectors {#special-vectors}\n\n__Zero Vector__:  The zero vector is a vector of all zeros.  It is the additive identity for vectors.  That is, if ${\\bf v}$ is any vector, then ${\\bf v} + {\\bf 0} = {\\bf v}$.  The zero vector is denoted by ${\\bf 0}$.\n\n__Unit Vectors__:  A unit vector is a vector of length 1.  There are many unit vectors, one for each dimension.  For example, in $\\mathbb{R}^3$, there are three unit vectors: \n\n\n$${\\bf e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, {\\bf e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\;\\;\\mathrm{ and }\\;\\; {\\bf e}_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}.$$\n\n\nThese are the unit vectors in the $x$, $y$, and $z$ directions, respectively.  Unit vectors are useful for representing directions in space.\n\n\n## Linear Combinations \n\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/k7RM-ot2NWY\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen style=\"border:1;display:block;margin:10px auto;\"></iframe>\n\n\n__Linear combinations__: The vector ${\\bf u}$ is a linear combination of the vectors ${\\bf v}_1, {\\bf v}_2,  \\cdots , {\\bf v}_k$ if\n\n\n$${\\bf u} = c_1{\\bf v}_1 + c_2{\\bf v}_2 +  \\cdots + c_k{\\bf v}_k$$\n\n\n\nFor example,  $\\begin{bmatrix}9 \\\\ 13 \\\\ 17 \\end{bmatrix}$ is a linear combination of the following three vectors: $\\begin{bmatrix}1 \\\\ 2 \\\\ 3 \\end{bmatrix}$, $\\begin{bmatrix} 2 \\\\ 3\\\\ 4\\end{bmatrix}$, and $\\begin{bmatrix} 3 \\\\ 4 \\\\ 5 \\end{bmatrix}$. This is because \n\n$$\\begin{bmatrix}9 \\\\ 13 \\\\ 17 \\end{bmatrix} = (2)\\begin{bmatrix}1 \\\\ 2 \\\\ 3 \\end{bmatrix} + (-1)\\begin{bmatrix} 2 \\\\ 3\\\\ 4\\end{bmatrix} + 3\\begin{bmatrix} 3 \\\\ 4 \\\\ 5 \\end{bmatrix}.$$\n\n\n\nNotice that you can always represent one vector into linear combinations of the unit vectors, where the coefficients are the elements in each coordinates. For example, $\\begin{bmatrix} 9 \\\\ 13 \\\\ 17 \\end{bmatrix}$ is a linear combination of the unit vectors $\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$, $\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$, and $\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}$, with coefficients $9$, $13$, and $17$, respectively.\n\n\n## Matrix {#matrixbasics}\n\n__Matrix__: A matrix of size $m\\times n$ is an array of real numbers arranged in $m$ rows by $n$ columns. The dimensionality of the matrix is defined as the number of rows by the number of columns, $m \\times n$. \n\n\n$${\\bf A}=\\begin{bmatrix}\n    A_{11} & A_{12} & \\cdots & A_{1n} \\\\\n    A_{21} & A_{22} & \\cdots & A_{2n} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    A_{m1} & A_{m2} & \\cdots & A_{mn}\n\\end{bmatrix}$$\n\nThe element of matrix $\\mathbf{A}$ corresponding to _row_ $i$ and _column_ $j$ is written $A_{ij}$.\n        \nNote that you can think of vectors as special cases of matrices; a column vector of length $k$ is a $k \\times 1$ matrix, while a row vector of the same length is a $1 \\times k$ matrix.\n\n:::{#exm-dimensions}\n### Matrix Dimensions\nConsider the following data matrix, containing observations on the two variables *Credit Score* and *Income*:\n\n\n$$\\mathbf{A}=\\begin{bmatrix} 780 & 95000\\\\ 600 & 60000\\\\ 550 & 65000\\\\ 400 & 35000\\\\ 450 & 40000\\\\ 750 & 80000\\end{bmatrix}$$\n\n\nThe dimension of the matrix $\\mathbf{A}$ is $6\\times 2$ because $\\mathbf{A}$ has 6 rows and 2 columns. Thus when referring to $\\mathbf{A}$ we might write $\\mathbf{A}_{6\\times 2}$ when the size is important. Note that the number of rows _always_ comes first when specifying the size of a matrix!\n:::\n\nIt's also useful to think of matrices as being made up of a collection of row or column vectors.  \n\nWhen we write \n\n$$\\mathbf{A}=[\\mathbf{a}_1 | \\mathbf{a}_2 | \\dots | \\mathbf{a}_n ]$$\n\nwe are viewing the matrix $\\mathbf{A}$ as collection of column vectors, $\\mathbf{a}_j$, in the following way:\n\n$$\\mathbf{A}=[ \\mathbf{a}_1 | \\mathbf{a}_2 | \\dots | \\mathbf{a}_n ]=\\left[\\begin{matrix} \\uparrow & \\uparrow &\\uparrow&\\dots & \\uparrow \\\\ \\mathbf{a}_1&\\mathbf{a}_2&\\mathbf{a}_3&\\dots&\\mathbf{a}_n \\\\ \\downarrow &\\downarrow &\\downarrow &\\dots&\\downarrow   \\end{matrix}\\right].$$\n\n\nSimilarly, we can write $\\mathbf{A}$ as a collection of row vectors:\n\n\n$$\\mathbf{A}=\\left[\\begin{matrix} \\mathbf{a}_1 \\\\ \\mathbf{a}_2 \\\\ \\vdots \\\\  \\mathbf{a}_m \\end{matrix}\\right] =  \\left[\\begin{matrix} \\longleftarrow & \\mathbf{a}_1 & \\longrightarrow \\\\ \\longleftarrow & \\mathbf{a}_2 & \\longrightarrow \\\\ \\vdots & \\vdots & \\vdots \\\\ \\longleftarrow & \\mathbf{a}_m & \\longrightarrow \\end{matrix}\\right].$$\n\n\nSometimes, we will want to refer to both rows and columns in the same context. In these situations, we may use $\\mathbf{A}_{i \\star}$ to reference the $i$-th row and $\\mathbf{A}_{\\star j}$ to reference the $j$-th column.\n\n__Matrix Addition__: Let $\\bf A$ and $\\bf B$ be two $m\\times n$ matrices.\n\n\n$$\\mathbf{A+B}=\\begin{bmatrix} a_{11}+b_{11} & a_{12}+b_{12} & \\cdots & a_{1n}+b_{1n} \\\\ a_{21}+b_{21} & a_{22}+b_{22} & \\cdots & a_{2n}+b_{2n} \\\\ \\vdots & \\vdots  & \\ddots & \\vdots \\\\ a_{m1}+b_{m1} & a_{m2}+b_{m2} & \\cdots & a_{mn}+b_{mn} \\end{bmatrix}$$\n\n\nNote that matrices ${\\bf A}$ and ${\\bf B}$ must have the same dimensionality, in which case they are __conformable for addition__.\n    \n:::{#exm-matrixaddition}\n\n$${\\bf A}=\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}, \\qquad {\\bf B}=\\begin{bmatrix} 1 & 2 & 1 \\\\ 2 & 1 & 2 \\end{bmatrix}$$\n\nFind $\\mathbf{A}+\\mathbf{B}$\n:::\n\n\n__Scalar Multiplication__:  Given the scalar $s$, the scalar multiplication of $s {\\bf A}$ is\n\n$$ s {\\bf A}=  s \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\end{bmatrix} = \\begin{bmatrix} s a_{11} & s a_{12} & \\cdots & s a_{1n} \\\\ s a_{21} & s a_{22} & \\cdots & s a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ s a_{m1} & s a_{m2} & \\cdots & s a_{mn} \\end{bmatrix}$$\n\n\n:::{#exm-scalarmulti}\n\n$s=2$, \n\n\n$${\\bf A}=\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}$$\n\n  \nFind $s {\\bf A}$\n\n:::\n\n\n## Transpose {#transpose}\n\nOne important transformation we will have to perform on a matrix is to switch the columns into rows. It is not necessary that you see the importance of this transformation right now, but trust that it is something we will need quite frequently.\n\n__Matrix Transpose__: The transpose of the $m\\times n$ matrix $\\bf A$ is the $n\\times m$ matrix ${\\bf A}^\\top$  (also written ${\\bf A}'$) obtained by interchanging the rows and columns of $\\bf A$.\n\nIn other words, the $(i,j)$-th element of $\\mathbf{A}^\\top$ is the $(j,i)$-th element of $\\mathbf{A}$.\n\n\n$$\\left(\\mathbf{A}^\\top\\right)_{ij} = \\mathbf{A}_{ji}$$\n\n\n_Note: If we transpose the transpose of a matrix, we will get back the original matrix. That is,_ $$(\\mathbf{A}^\\top)^\\top = \\mathbf{A}.$$\n\nFor example,\n\n${\\bf A}=\\begin{bmatrix} 4&-2&3\\\\0&5&-1\\end{bmatrix}, \\qquad {\\bf A}^\\top=\\begin{bmatrix} 4&0\\\\-2&5\\\\3&-1 \\end{bmatrix}$\n\n${\\bf B}=\\begin{bmatrix} 2\\\\-1\\\\3 \\end{bmatrix}, \\qquad {\\bf B}^\\top=\\begin{bmatrix} 2&-1&3\\end{bmatrix}$\n\n\nThus, if $\\mathbf{A}$ is a $3\\times 4$ matrix then $\\mathbf{A}^\\top$ is a $4\\times 3$ matrix as follows:\n\n\n$$\\mathbf{A} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} &A_{14}\\\\ A_{21} & A_{22} & A_{23} &A_{24}\\\\ A_{31} & A_{32} & A_{33} &A_{34}\\end{bmatrix} \\quad  \\mathbf{A}^\\top = \\begin{bmatrix} A_{11} & A_{21} & A_{31} \\\\ A_{12} & A_{22} & A_{32} \\\\ A_{13} & A_{23} & A_{33} \\\\ A_{14} & A_{24} & A_{34}\\end{bmatrix}$$\n\n\n:::{#exm-transpose}\n\nFor the following matrices and vectors, determine the transpose:\n\n$$\\mathbf{B}=\\begin{bmatrix} 2 & -3 & -4 \\\\5&-6&-7\\\\-8&9&0 \\end{bmatrix} \\qquad \\mathbf{M}=\\begin{bmatrix} -1&2\\\\-3&6\\\\7&-9\\\\5&-1 \\end{bmatrix} \\qquad \\mathbf{x}=\\begin{bmatrix}3\\\\-4\\\\5\\\\6\\end{bmatrix}$$\n\nTo find the transpose, we simply create new matrices whose rows are the corresponding columns of each matrix or vector:\n\n$$\\mathbf{B}^\\top=\\begin{bmatrix} 2 &5& -8\\\\-3&-6&9\\\\-4&-7&0\\end{bmatrix} \\qquad \\mathbf{M}^\\top = \\begin{bmatrix}-1&-3&7&5\\\\2&6&-9&-1 \\end{bmatrix} $$ $$\\mathbf{x}^\\top = \\begin{bmatrix} 3&-4&5&6 \\end{bmatrix}$$\n\n:::\n\nThe following properties of matrix transpose are useful to know:\n\n1. $({\\bf A+B})^\\top = {\\bf A}^\\top+{\\bf B}^\\top$\n2. $({\\bf A}^\\top)^\\top={\\bf A}$\n3. $(s{\\bf A})^\\top = s{\\bf A}^\\top$\n4. $({\\bf AB})^\\top = {\\bf B}^\\top{\\bf A}^\\top$; and by induction $({\\bf ABC})^\\top = {\\bf C}^\\top{\\bf B}^\\top{\\bf A}^\\top$\n\nExample of $({\\bf AB})^\\top = {\\bf B}^\\top{\\bf A}^\\top$:\n\n\n$${\\bf A}=\\begin{bmatrix} 1&3&2\\\\2&-1&3\\end{bmatrix}, \\qquad {\\bf B}=\\begin{bmatrix} 0&1\\\\2&2\\\\3&-1\\end{bmatrix}$$\n\n$$ ({\\bf AB})^\\top = \\left[ \\begin{bmatrix} 1&3&2\\\\2&-1&3\\end{bmatrix} \\begin{bmatrix} 0&1\\\\2&2\\\\3&-1\\end{bmatrix} \\right]^\\top = \\begin{bmatrix} 12&7\\\\5&-3 \\end{bmatrix}$$\n\n$$ {\\bf B}^\\top{\\bf A}^\\top= \\begin{bmatrix} 0&2&3\\\\1&2&-1 \\end{bmatrix}  \\begin{bmatrix} 1&2\\\\3&-1\\\\2&3 \\end{bmatrix} = \\begin{bmatrix} 12&7\\\\5&-3 \\end{bmatrix}$$\n\n\n\n\n\n## Special Matrices {#special-matrix}\n\n__Zero Matrix__: The zero matrix is a matrix of all zeros. For example, the $3\\times 4$ zero matrix is given by \n\n$${\\bf 0}_{3\\times 4}=\\begin{bmatrix} 0&0&0&0\\\\0&0&0&0\\\\0&0&0&0 \\end{bmatrix}$$\n\n\n__Identity Matrix__: The identity matrix is a square matrix with ones on the main diagonal and zeros elsewhere. For example, the $3\\times 3$ identity matrix is given by\n\n$${\\bf I}_3=\\begin{bmatrix} 1&0&0\\\\0&1&0\\\\0&0&1 \\end{bmatrix}$$\n\n\nNotice that we can write identity matrix as collections of unit vectors. For example, the $3\\times 3$ identity matrix can be written as\n\n$$\\mathbf{I}_3=\\left[ \\mathbf{e}_1 | \\mathbf{e}_2 | \\mathbf{e}_3 \\right],$$\n\nwhere\n\n$${\\bf e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, {\\bf e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\;\\;\\mathrm{ and }\\;\\; {\\bf e}_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}$$\n\nare unit vectors in $\\mathbb{R}^3$, the 3-dimensional vector space.\n\n__Symmetric Matrix__: A matrix is symmetric if it is equal to its transpose. That is, if $\\mathbf{A}$ is a symmetric matrix, then \n\n$$\\mathbf{A}=\\mathbf{A}^\\top.$$\n\n\n__Diagonal Matrix__: A matrix is diagonal if all the elements outside the main diagonal are zero. \n\nFor example:\n\n$$\\mathbf{D} = \\begin{bmatrix} \\sigma_1 & 0 & 0 & 0 \\\\ 0 & \\sigma_2 & 0 & 0\\\\ 0&0&\\sigma_3&0\\\\ 0&0&0&\\sigma_4 \\end{bmatrix}$$\n\nSince the off diagonal elements are 0, we need only define the diagonal elements for such a matrix. Thus, we will frequently write\n\n$$\\mathbf{D}=\\mathrm{diag}\\{\\sigma_1,\\sigma_2,\\sigma_3,\\sigma_4\\}.$$\n\n\n\n## Vector Inner Product\n\n\nWhen we multiply matrices, we do not perform the operation element-wise as we did with addition and scalar multiplication. Matrix multiplication is, in itself, a very powerful tool for summarizing information. In fact, many of the analytical tools, like linear regression, can all be understood more clearly with a firm grasp on matrix multiplication. Because this operation is so important, we will spend a considerable amount of energy breaking it down in many ways. \n\nWe often want to summarize the relationship between two vectors into a single number. If the two vectors are pointing in similar directions, the number should be large and positive. If the two vectors are pointing in opposite directions, the number should be small and negative. If the two vectors are perpendicular, the number should be zero. This idea leads to the definition of the inner product of two vectors.\n\n__Inner Product__: The inner product (also called dot product) of two vectors $\\mathbf{u}$ and $\\mathbf{v}$ is defined as\n\n$$\\mathbf{u} \\cdot \\mathbf{v}=\\mathbf{u}^\\top\\mathbf{v}=\\sum_{i=1}^n u_i v_i = u_1 v_1 + u_2 v_2+\\cdots u_n v_n.$$\n\nThe inner product is a scalar quantity.\n\nIf ${\\bf u} \\cdot {\\bf v} = 0$, the two vectors are orthogonal (or perpendicular).\n\n::: {.cell layout-align=\"center\" hash='21_vector_matrix_cache/html/fig-animinnerproduct_98ae697987fdec7cf47d1264522a3f6c'}\n::: {.cell-output-display}\n![Animation of Inner Product between two vectors [(Source)](https://shainarace.github.io/LinearAlgebra/mult.html)](images/animinnerprod.gif){#fig-animinnerproduct fig-align='center' width=50%}\n:::\n:::\n\nWhy are we doing this? To understand what's going on intuitively, let's think about the case of two-dimensions where we can break down the components of the inner product into $x$ and $y$ axis.\nDenote\n\n$$\\mathbf{a}=\\begin{bmatrix}a_x \\\\ a_y\\end{bmatrix}=a_x\\begin{bmatrix}1 \\\\ 0\\end{bmatrix} + a_y\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$$\n\n$$\\mathbf{b}=\\begin{bmatrix}b_x \\\\ b_y\\end{bmatrix}=b_x\\begin{bmatrix}1 \\\\ 0\\end{bmatrix} + b_y\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$$\n\nSince $x$ and $y$ axis are perpendicular to each other, the inner products of their unit vectors $\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$ and $\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$ are zero, thus we have that\n\\begin{align*}\\mathbf{a}\\cdot\\mathbf{b}\n&=\\left(a_x\\begin{bmatrix}1 \\\\ 0\\end{bmatrix} + a_y\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}\\right)\\cdot\\left(b_x\\begin{bmatrix}1 \\\\ 0\\end{bmatrix} + b_y\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}\\right) \\\\\n&=\na_x b_x \\underbrace{\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}^\\top \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}}_{=1\\cdot 1 + 0\\cdot0=1} + \na_x b_y \\underbrace{\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}^\\top \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}}_{=1\\cdot 0 + 0\\cdot1=0} + \na_y b_x \\underbrace{\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}^\\top \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}}_{=0\\cdot 1 + 1\\cdot0=0} + \na_y b_y \\underbrace{\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}^\\top \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}}_{=0\\cdot 0 + 1\\cdot1=1}\\\\\n&=a_x b_x + a_y b_y\n\\end{align*}\n\n\n::: {.cell layout-align=\"center\" hash='21_vector_matrix_cache/html/fig-dot-product-components_3d331908eccd52e305212803f308198f'}\n::: {.cell-output-display}\n![Components of Inner Product [(Source)](https://betterexplained.com/articles/vector-calculus-understanding-the-dot-product/)](images/dot_product_components.jpeg){#fig-dot-product-components fig-align='center' width=50%}\n:::\n:::\n\n\n\n:::{#exm-inner-prod}\n### Vector Inner Product\nLet $$\\mathbf{x}=\\begin{bmatrix} -1 \\\\2\\\\4\\\\0 \\end{bmatrix} \\quad \\mathbf{y}=\\begin{bmatrix} 3 \\\\5\\\\1\\\\7 \\end{bmatrix} \\quad \\mathbf{v}=\\begin{bmatrix} -3 \\\\-2\\\\5\\\\3\\\\-2 \\end{bmatrix} \\quad \\mathbf{u}= \\begin{bmatrix} 2\\\\-1\\\\3\\\\-3\\\\-2 \\end{bmatrix}$$\n\nIf possible, compute the following inner products:\n\na. $\\mathbf{x}^\\top\\mathbf{y}$\n\\begin{align*}\n\\mathbf{x}^\\top\\mathbf{y} &=\\begin{bmatrix} -1 &2&4&0 \\end{bmatrix} \\begin{bmatrix} 3 \\\\5\\\\1\\\\7 \\end{bmatrix} \\\\\n&= (-1)(3)+(2)(5)+(4)(1)+(0)(7) = -3+10+4=11\n\\end{align*}\nb. $\\mathbf{x}^\\top\\mathbf{v}$\nThis is not possible because $\\mathbf{x}$ and $\\mathbf{v}$ do not have the same number of elements\nc. $\\mathbf{v}^\\top\\mathbf{u}$\n\\begin{align*}\n\\mathbf{v}^\\top\\mathbf{u} &= \\begin{bmatrix} -3 &-2&5&3&-2 \\end{bmatrix} \\begin{bmatrix} 2\\\\-1\\\\3\\\\-3\\\\-2 \\end{bmatrix} \\\\\n&= (-3)(2)+(-2)(-1)+(5)(3)+(3)(-3)+(-2)(-2) = -6+2+15-9+4 = 6\n\\end{align*}\n:::\n\n\n\n\n\n:::{#exr-vectors1}\n\nLet $u = \\begin{bmatrix} 7\\\\1\\\\-5\\\\3\\end{bmatrix}$, $v = \\begin{bmatrix} 9\\\\-3\\\\2\\\\8 \\end{bmatrix}$, $w = \\begin{bmatrix} 1\\\\13\\\\ -7\\\\2 \\\\15 \\end{bmatrix}$, and $c = 2$. Calculate the following: \n  \n  1. $u-v$\n  \n  2. $cw$\n  \n  3. $u \\cdot v$\n  \n  4. $w \\cdot v$\n  \n:::\n\n\n## Matrix Multiplication\n\nMatrix multiplication is nothing more than a collection of inner products done simultaneously in one operation. We must be careful when multiplying matrices because, as with vectors, the operation is not always possible. Unlike the vector inner product, the order in which you multiply matrices makes a big difference!\n\n\n__Matrix Multiplication__: Let $\\mathbf{A}$ be a $m\\times n$ matrix and $\\mathbf{B}$ be a ${k\\times p}$ matrix. The matrix product $\\mathbf{A}\\mathbf{B}$ is possible if and only if $n=k$; that is, when the number of columns in $\\mathbf{A}$ is the same as the number of rows in $\\mathbf{B}$. If this condition holds, then the the product, $\\mathbf{A}\\mathbf{B}$, is a $m\\times p$ matrix and the $(i,j)$ entry of the product $\\mathbf{A}\\mathbf{B}$ is the inner product of the $i$th row of $\\mathbf{A}$ and the $j$th column of $\\mathbf{B}$:\n\n\n$$(\\mathbf{A}\\mathbf{B})_{ij} = \\mathbf{A}_{i\\star}\\cdot\\mathbf{B}_{\\star j}$$\n\n\nIn other words,\n\n\n\\begin{align*}\n\\mathbf{AB}\n&=\\left[\\begin{matrix} \\longleftarrow & \\mathbf{a}_1 & \\longrightarrow \\\\ \n\\longleftarrow & \\mathbf{a}_2 & \\longrightarrow \\\\ \n\\vdots & \\vdots & \\vdots \\\\ \n\\longleftarrow & \\mathbf{a}_m & \\longrightarrow \\end{matrix}\\right]\n\\left[\\begin{matrix} \\uparrow & \\uparrow &\\dots & \\uparrow \\\\ \n\\mathbf{b}_1&\\mathbf{b}_2&\\dots&\\mathbf{b}_p \\\\ \n\\downarrow &\\downarrow &\\dots&\\downarrow   \\end{matrix}\\right] \\\\\n&=\\begin{bmatrix} \\mathbf{a}_1\\cdot\\mathbf{b}_1 & \\mathbf{a}_1\\cdot\\mathbf{b}_2 & \\cdots & \\mathbf{a}_1\\cdot\\mathbf{b}_p \\\\ \\mathbf{a}_2\\cdot\\mathbf{b}_1 & \\mathbf{a}_2\\cdot\\mathbf{b}_2 & \\cdots & \\mathbf{a}_2\\cdot\\mathbf{b}_p \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\mathbf{a}_m\\cdot\\mathbf{b}_1 & \\mathbf{a}_m\\cdot\\mathbf{b}_2 & \\cdots & \\mathbf{a}_m\\cdot\\mathbf{b}_p \\end{bmatrix}\n\\end{align*}\nwhere $\\mathbf{a}_i$ is the $i$th row of $\\mathbf{A}$ and $\\mathbf{b}_j$ is the $j$th column of $\\mathbf{B}$.\n\n:::{#exm-matmult}\n\n### Steps to Compute Matrix Multiplication\n\nLet $$\\mathbf{A}=\\begin{bmatrix} 2 & 3 \\\\ -1 & 4 \\\\ 5 & 1 \\end{bmatrix} \\quad \\;\\;\\mathrm{ and }\\;\\; \\quad \\mathbf{B}=\\begin{bmatrix}  0 & -2 \\\\ 2 & -3 \\end{bmatrix}$$\n\nWhen we first get started with matrix multiplication, we often follow a few simple steps:\n\n1. Write down the matrices and their dimensions. Make sure the \"inside\" dimensions match - those corresponding to the columns of the first matrix and the rows of the second matrix: $$\\underset{(3\\times \\red{2})}{\\mathbf{A}} \\underset{(\\red{2} \\times 2)}{\\mathbf{B}}$$ If these dimensions match, then we can multiply the matrices. If they don't, we stop right there - multiplication is not possible.\n2. Now, look at the \"outer\" dimensions - this will tell you the size of the resulting matrix. $$\\underset{(\\blue{3}\\times 2)}{\\mathbf{A}} \\underset{(2\\times \\blue{2})}{\\mathbf{B}}$$ So the product $\\mathbf{A}\\mathbf{B}$ is a $3\\times 2$ matrix.\n3. Finally, we compute the product of the matrices by multiplying each row of $\\mathbf{A}$ by each column of $\\mathbf{B}$ using inner products. The element in the first row and first column of the product (written $(\\mathbf{A}\\mathbf{B})_{11}$) will be the inner product of the first row of $\\mathbf{A}$ and the first column of $\\mathbf{B}$. Then, $(\\mathbf{A}\\mathbf{B})_{12}$ will be the inner product of the first row of $\\mathbf{A}$ and the second column of $\\mathbf{B}$, etc. $$\\mathbf{A}\\mathbf{B} =\\begin{bmatrix} (2)(0)+(3)(2) & (2)(-2)+(3)(-3)\\\\ (-1)(0)+(4)(2) & (-1)(-2)+(4)(-3)\\\\ (5)(0)+(1)(2) & (5)(-2)+(1)(-3) \\end{bmatrix} = \\begin{bmatrix} 6&-13\\\\8 & -10\\\\2&-13\\end{bmatrix} $$\n:::\n\n\n:::{#exr-matmult}\n### Matrix Multiplication\n\nSuppose we have \n\n$$\\mathbf{A}_{4\\times 6} \\quad \\mathbf{B}_{5\\times 5} \\quad \\mathbf{M}_{5\\times 4} \\quad \\mathbf{P}_{6\\times 5}$$\n\nCircle the matrix products that are possible to compute and write the dimension of the result.\n\n$$\\mathbf{A}\\mathbf{M} \\qquad \\mathbf{M}\\mathbf{A} \\qquad \\mathbf{B}\\mathbf{M}  \\qquad \\mathbf{M}\\mathbf{B} \\qquad \\mathbf{P}\\mathbf{A} \\qquad \\mathbf{P}\\mathbf{M} \\qquad \\mathbf{A}\\mathbf{P} \\qquad \\mathbf{A}^\\top\\mathbf{P} \\qquad \\mathbf{M}^\\top\\mathbf{B}$$\n\nLet \n\n$$\\mathbf{A}=\\begin{bmatrix} 1&1&0&1\\\\0&1&1&1\\\\1&0&1&0\\end{bmatrix} \\quad \\mathbf{M} = \\begin{bmatrix} -2&1&-1&2&-2\\\\1&-2&0&-1&2\\\\2&1&-3&-2&3 \\\\ 1&3&2&-1&2\\end{bmatrix}$$\n\n$$\\mathbf{C}=\\begin{bmatrix} -1&0&1&0\\\\1&-1&0&0\\\\0&0&1&-1 \\end{bmatrix}$$\n\nDetermine the following matrix products, if possible:\n\n1. $\\mathbf{A}\\mathbf{C}$\n2. $\\mathbf{A}\\mathbf{M}$\n3. $\\mathbf{A}^\\top\\mathbf{C}$\n:::\n\nThe following properties of matrix multiplication and addition are useful to know:\n\n1. Associative: $\\bf (A+B)+C = A+(B+C)$\n2. $\\bf (AB)C = A(BC)$\n3. Commutative: $\\bf A+B=B+A$\n4. Distributive: $\\bf A(B+C)=AB+AC$\n5. $\\bf (A+B)C=AC+BC$\n\nCommutative law for multiplication does not hold -- the order of multiplication matters:\n\n$$\\bf AB \\ne BA$$\n\n\nFor example,\n\n$${\\bf A}=\\begin{bmatrix} 1&2\\\\-1&3\\end{bmatrix}, \\qquad {\\bf B}=\\begin{bmatrix} 2&1\\\\0&1\\end{bmatrix}$$\n\n$${\\bf AB}=\\begin{bmatrix} 2&3\\\\-2&2\\end{bmatrix}, \\qquad {\\bf BA}=\\begin{bmatrix} 1&7\\\\-1&3\\end{bmatrix}$$\n\n\n## Matrix-Vector Product\n\n\nWhat if we want to multiply matrix and vector? A matrix-vector product works exactly the same way as matrix multiplication; after all, a vector $\\mathbf{x}$ is nothing but an $n\\times 1$ matrix. In order to multiply a matrix by a vector, again we must match the dimensions to make sure they line up correctly. For example, if we have an $m\\times n$ matrix $\\mathbf{A}$, we can multiply by a $1\\times m$ row vector $\\mathbf{v}^\\top$ on the left:\n\n$$\\mathbf{v}^\\top\\mathbf{A} \\quad \\textrm{works because } \\underset{ (1\\times \\red{m})}{\\mathbf{v}^\\top} \\underset{(\\red{m}\\times n)}{\\mathbf{A}}$$\n\n$$\\Longrightarrow \\textrm{The result will be a   } 1 \\times n \\textrm{ row vector.}$$\n\nor we can multiply by an $n\\times 1$ column vector $\\mathbf{x}$ on the right:\n\n\n$$\\mathbf{A}\\mathbf{x} \\quad \\textrm{works because } \\underset{(m\\times \\red{n})}{\\mathbf{A}}\\underset{(\\red{n}\\times 1)}{\\mathbf{x}} $$\n\n$$\\Longrightarrow \\textrm{The result will be a   } m\\times 1 \\textrm{ column vector.}$$\n\n\nMatrix-vector multiplication works the same way as matrix multiplication: we simply multiply rows by columns until we've completed the answer. In the case of $\\mathbf{v}^\\top\\mathbf{A}$, we'd multiply the row $\\mathbf{v}$ by each of the $n$ columns of $\\mathbf{A}$, carving out our solution, one entry at a time :\n\n\n$$\\mathbf{v}^\\top\\mathbf{A} = \\begin{bmatrix} \\mathbf{v}^\\top\\mathbf{A}_{*1} & \\mathbf{v}^\\top\\mathbf{A}_{*2} & \\cdots & \\mathbf{v}^\\top\\mathbf{A}_{*n} \\end{bmatrix}.$$\n\n\nIn the case of $\\mathbf{A}\\mathbf{x}$, we'd multiply each of the $m$ rows of $\\mathbf{A}$ by the column $\\mathbf{x}$:\n\n\n$$\\mathbf{A}\\mathbf{x} = \\begin{bmatrix} \\mathbf{A}_{1*}\\mathbf{x} \\\\ \\mathbf{A}_{2*}\\mathbf{x} \\\\ \\vdots \\\\ \\mathbf{A}_{m*}\\mathbf{x} \\end{bmatrix}.$$\n\n\n:::{#exm-matrix-vector-product}\n### Matrix-Vector Product\nLet $$\\mathbf{A}=\\begin{bmatrix} 2 & 3 \\\\ -1 & 4 \\\\ 5 & 1 \\end{bmatrix}  \\quad \\mathbf{v}=\\begin{bmatrix} 3\\\\2 \\end{bmatrix} \\quad \\mathbf{q}=\\begin{bmatrix} 2\\\\-1\\\\3\\end{bmatrix}$$\n\nDetermine whether the following matrix-vector products are possible. When possible, compute the product.\n\na. $\\mathbf{A}\\mathbf{q}$ $$\\textrm{Not Possible: Inner dimensions do not match} \\quad \\underset{(3\\times \\red{2})}{\\mathbf{A}}\\underset{(\\red{3}\\times 1)}{\\mathbf{q}}$$\nb. $\\mathbf{A}\\mathbf{v}$ $$\\begin{bmatrix} 2 & 3 \\\\ -1 & 4 \\\\ 5 & 1 \\end{bmatrix} \\begin{bmatrix} 3\\\\2 \\end{bmatrix} = \\begin{bmatrix} 2(3)+3(2) \\\\  -1(3)+4(2)\\\\5(3)+1(2) \\end{bmatrix} = \\begin{bmatrix} 12\\\\5\\\\17\\end{bmatrix}$$\nc. $\\mathbf{q}^\\top\\mathbf{A}$ $$\\begin{bmatrix} \\blue{2} & \\blue{-1} & \\blue{3}\\end{bmatrix} \\begin{bmatrix} \\blue{2} & 3 \\\\ \\blue{-1} & 4 \\\\ \\blue{5} & 1 \\end{bmatrix}  =  \\begin{bmatrix} \\blue{20} & 5  \\end{bmatrix}$$\nd. $\\mathbf{v}^\\top\\mathbf{A}$ $$\\textrm{Not Possible: Inner dimensions do not match} \\quad \\underset{(1\\times \\red{2})}{\\mathbf{v}^\\top}\\underset{(\\red{3}\\times 2)}{\\mathbf{A}}$$\n:::\n\n\n## Matrix Product IS Linear Combination\n\nAll matrix products can be viewed as linear combinations. This vantage point is _extremely_ crucial to our understanding of linear algebra. Let's start with matrix-vector product and see how we can depict it as a linear combination of the columns of the matrix.\n\n__Matrix-Vector Product as Linear Combination__:\n\nLet $\\mathbf{A}$ be an $m\\times n$ matrix partitioned into columns, \n\n$$\\mathbf{A} = \\begin{bmatrix}\\mathbf{a}_1 | \\mathbf{a}_2 | \\dots | \\mathbf{a}_n\\end{bmatrix}$$\n\nand let $\\mathbf{x}$ be a $n$-dimensional vector\n\n$$\\mathbf{x}=\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$$\n\nThen, $\\mathbf{A}\\mathbf{x}$ is the linear combination of the columns of $\\mathbf{A}$ using coefficients in $\\mathbf{x}$:\n\n$$\\mathbf{A}\\mathbf{x} = x_1\\mathbf{a}_1 + x_2\\mathbf{a}_2 + \\dots + x_n\\mathbf{a}_n.$$\n\n\nThe animation below illustrates the relationship between matrix-vector product and linear combination. \n\n::: {.cell layout-align=\"center\" fig='true' hash='21_vector_matrix_cache/html/fig-matvecprodlincombanim_fb65a39362a4c37c7fca6dca71d5dac5'}\n::: {.cell-output-display}\n![Illustration of Matrix-Vector Product as Linear Combinations [(Source)](https://shainarace.github.io/LinearAlgebra/mult.html)](images/animmatveclincomb.gif){#fig-matvecprodlincombanim fig-align='center' width=50%}\n:::\n:::\n\n:::{#exm-matrix-vector-product-lc}\nCalculate \n\n$$\\begin{bmatrix} 3 & 1 \\\\ 1 & 2\\end{bmatrix}\\begin{bmatrix}-1 \\\\ 2\\end{bmatrix}.$$\n\n\nCalculate \n\n$$(-1)\\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix}+(2)\\begin{bmatrix}1 \\\\ 2\\end{bmatrix}.$$\n\n\nInterpret the geometric relationship between the two.\n\n:::\n\nConceptually, while doing matrix-vector product, the matrix $\\mathbf{A}$ is **transforming** the vector $\\mathbf{x}$ in a specific way (by taking linear cominations) to the vector $\\mathbf{A}\\mathbf{x}$. This is illustrated in the video below.\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/kYB8IZa5AuE\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen style=\"border:1;display:block;margin:10px auto;\"></iframe>\n\n\n__Matrix Multiplication as Collection of Linear Combinations__:\n\nWe can view matrix multiplication as applying matrix-vector product iteratively. \n\nIf \n\n$$\\mathbf{A}\\mathbf{B}=\\mathbf{C},$$ \n\nthen the columns of $\\mathbf{C}$ can be viewed as linear combinations of the columns of $\\mathbf{A}$ and the coefficients are given in the elements of each column of $\\mathbf{B}$. Denote\n\n\n$$\\mathbf{A}_{m\\times n} = \\begin{bmatrix}\\mathbf{a}_1 | \\mathbf{a}_2 | \\dots | \\mathbf{a}_n\\end{bmatrix}$$\n\n$$\\mathbf{B}_{n\\times p} = \\begin{bmatrix}\\mathbf{b}_1 | \\mathbf{b}_2 | \\dots | \\mathbf{b}_p\\end{bmatrix}$$\n\n$$\\mathbf{C}_{m\\times p} = \\begin{bmatrix}\\mathbf{c}_1 | \\mathbf{c}_2 | \\dots | \\mathbf{c}_p\\end{bmatrix}$$\n\nThen, for example, $\\mathbf{c}_1$ is the linear combination of the columns of $\\mathbf{A}$ using coefficients in the first column of $\\mathbf{B}$, i.e., $\\mathbf{b}_1$:\n\n$$\\mathbf{b}_1=\\begin{bmatrix}B_{11}\\\\ B_{21}\\\\ \\vdots \\\\ B_{n1} \\end{bmatrix},$$\n\n$$\\mathbf{C}_1= B_{11}\\mathbf{a}_1 + B_{21}\\mathbf{a}_2 + \\dots + B_{n1}\\mathbf{a}_n=\\mathbf{A}\\mathbf{b}_1,$$\n\n\nwhich is the matrix-vector product of $\\mathbf{A}$ and $\\mathbf{b}_1$. As a result, we can write the whole matrix multiplication as a collection of matrix-vector products by columns:\n\n\n$$\\mathbf{C} = \\begin{bmatrix}\\mathbf{c}_1 | \\mathbf{c}_2 | \\dots | \\mathbf{c}_p\\end{bmatrix} = \\begin{bmatrix}\\mathbf{A}\\mathbf{b}_1 | \\mathbf{A}\\mathbf{b}_2 | \\cdots | \\mathbf{A}\\mathbf{b}_p\\end{bmatrix}.$$\n\n\n\nThe animation below illustrates the relationship between matrix multiplication and linear combinations.\n\n::: {.cell layout-align=\"center\" fig='true' hash='21_vector_matrix_cache/html/multlincombanim_a60ff297c001aa3e20243a643c34c600'}\n::: {.cell-output-display}\n![Illustration of Matrix Multiplication as Linear Combinations [(Source)](https://shainarace.github.io/LinearAlgebra/mult.html)](images/animmultlincombanim.gif){fig-align='center' width=50%}\n:::\n:::\n\nIn fact, when doing matrix multiplication, we're actually applying a **series of transformations**, illustrated nicely in the video below.\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/XkY2DOUCWMU\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen style=\"border:1;display:block;margin:10px auto;\"></iframe>\n\n\n\n\n\n\n\n## Answers to Examples and Exercises {-}\n\n\nAnswer to @exr-vectors1:\n\n  1. $\\begin{bmatrix} -2 &4&-7&-5 \\end{bmatrix}$\n  2. $\\begin{bmatrix} 2 &26&-14&4&30 \\end{bmatrix}$\n  3. 63 -3 -10 + 24 = 74\n  4. undefined\n\nAnswer to @exm-matrixaddition:\n\n${\\bf A+B}=\\begin{bmatrix} 2 & 4 & 4 \\\\ 6 & 6 & 8 \\end{bmatrix}$\n    \nAnswer to @exm-scalarmulti:\n\n$s {\\bf A} = \\begin{bmatrix} 2 & 4 & 6 \\\\ 8 & 10 & 12 \\end{bmatrix}$\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}