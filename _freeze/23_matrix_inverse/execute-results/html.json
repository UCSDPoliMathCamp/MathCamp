{
  "hash": "097557dca15dbe5cbd319cf816fd5098",
  "result": {
    "markdown": "# Matrix Inverse and Linear Independence\n\n## The Inverse of a Matrix\n\n\n__Identity Matrix__:  The $n\\times n$ identity matrix ${\\bf I}_n$ is the matrix whose diagonal elements are 1 and all off-diagonal elements are 0. Examples:\n            $$ {\\bf I}_2=\\begin{bmatrix} 1&0\\\\0&1 \\end{bmatrix}, \\qquad {\\bf I}_3=\\begin{bmatrix} 1&0&0\\\\ 0&1&0\\\\ \n            0&0&1 \\end{bmatrix}$$\t\n            \n            \n__Inverse Matrix__:  An $n\\times n$ matrix ${\\bf A}$ is __nonsingular__ or __invertible__ if there exists an $n\\times n$ matrix ${\\bf A}^{-1}$ such that $${\\bf A} {\\bf A}^{-1} = {\\bf A}^{-1} {\\bf A} = {\\bf I}_n$$ where ${\\bf A}^{-1}$ is the inverse of ${\\bf A}$.  If there is no such ${\\bf A}^{-1}$, then ${\\bf A}$ is singular or not invertible.\n\nExample:  Let\n        $${\\bf A} = \\begin{bmatrix} 2&3\\\\2&2 \\end{bmatrix}, \\qquad {\\bf B}=\\begin{bmatrix} -1&\\frac{3}{2}\\\\ 1&-1\n        \\end{bmatrix}$$\n        Since $${\\bf A} {\\bf B} = {\\bf B} {\\bf A} = {\\bf I}_n$$ we conclude that ${\\bf B}$ is the inverse, ${\\bf A}^{-1}$, of ${\\bf A}$ and that ${\\bf A}$ is nonsingular.\n\n__Properties of the Inverse__:\n\n- If the inverse exists, it is unique.\n\n- If ${\\bf A}$ is nonsingular, then ${\\bf A}^{-1}$ is nonsingular.\n\n- $({\\bf A}^{-1})^{-1} = {\\bf A}$\n\n- If ${\\bf A}$ and ${\\bf B}$ are nonsingular, then ${\\bf A}{\\bf B}$ is nonsingular\n\n- $({\\bf A}{\\bf B})^{-1} = {\\bf B}^{-1}{\\bf A}^{-1}$\n\n- If ${\\bf A}$ is nonsingular, then $({\\bf A}^\\top)^{-1}=({\\bf A}^{-1})^\\top$\n\n \n__Procedure to Find__ ${\\bf A}^{-1}$:  We know that if ${\\bf B}$ is the inverse of ${\\bf A}$, then $${\\bf A} {\\bf B} = {\\bf B} {\\bf A} = {\\bf I}_n$$  Looking only at the first and last parts of this $${\\bf A} {\\bf B} = {\\bf I}_n$$  Solving for ${\\bf B}$ is equivalent to solving for $n$ linear systems, where each column of ${\\bf B}$ is solved for the corresponding column in ${\\bf I}_n$.  We can solve the systems simultaneously by augmenting ${\\bf A}$ with ${\\bf I}_n$ and performing Gauss-Jordan elimination on ${\\bf A}$.  If Gauss-Jordan elimination on $[{\\bf A} | {\\bf I}_n]$ results in $[{\\bf I}_n | {\\bf B} ]$, then ${\\bf B}$ is the inverse of ${\\bf A}$.  Otherwise, ${\\bf A}$ is singular.\n\nTo summarize:  To calculate the inverse of ${\\bf A}$ \n        \n1. Form the augmented matrix $[ {\\bf A} | {\\bf I}_n]$\n\n2. Using elementary row operations, transform the augmented matrix to reduced row echelon form.\n\n3. The result of step 2 is an augmented matrix $[ {\\bf C} | {\\bf B} ]$.\n            \n    a. If ${\\bf C}={\\bf I}_n$, then ${\\bf B}={\\bf A}^{-1}$.\n            \n    b. If ${\\bf C}\\ne{\\bf I}_n$, then $\\bf C$ has a row of zeros. This means ${\\bf A}$ is singular and ${\\bf A}^{-1}$ does not exist.\n\n\n:::{#exm-inverse}\n\nFind the inverse of the following matrix:\n\n  $${\\bf A}=\\begin{bmatrix} 1&1&1\\\\0&2&3\\\\5&5&1 \\end{bmatrix}$$\n\n\n:::\n\n\n\n\n\n:::{#exr-inverse1}\n\nFind the inverse of the following matrix:\n\n  $${\\bf A}=\\begin{bmatrix} 1&0&4\\\\0&2&0\\\\0&0&1 \\end{bmatrix}$$\n\n\n:::\n\n\n\n\n## Linear Systems and Inverse\n\nLet's return to the matrix representation of a linear system \n        \n\n  $$\\bf{Ax} = \\bf{b}$$\n\n\nIf $\\bf{A}$ is an $n\\times n$ matrix,then $\\bf{Ax}=\\bf{b}$ is a system of $n$ equations in $n$ unknowns.  Suppose $\\bf{A}$ is nonsingular. Then $\\bf{A}^{-1}$ exists.  To solve this system, we can multiply each side by $\\bf{A}^{-1}$ and reduce it as follows:\n\\begin{align*} \\bf{A}^{-1} (\\bf{A} \\bf{x}) & =  \\bf{A}^{-1} \\bf{b} \\\\ (\\bf{A}^{-1} \\bf{A})\\bf{x} & =  \\bf{A}^{-1} \\bf{b}\\\\ \\bf{I}_n \\bf{x}     & =  \\bf{A}^{-1} \\bf{b}\\\\ \\bf{x} & =  \\bf{A}^{-1} \\bf{b} \\end{align*}\n\nHence, given $\\bf{A}$ and $\\bf{b}$ and given that $\\bf{A}$ is nonsingular, then $\\bf{x} = \\bf{A}^{-1} \\bf{b}$ is a unique solution to this system.\n\n\n:::{#exr-invlinsys}\n\nUse the inverse matrix to solve the following linear system:\n\n\\begin{align*} \n-3x + 4y &= 5 \\\\\n2x - y &= -10\n\\end{align*}\n\n**Hint**: the linear system above can be written in the matrix form\n\n$\\textbf{A}\\textbf{z} = \\textbf{b}$ \n  \ngiven \n\n  $$\\textbf{A} = \\begin{bmatrix} -3&4\\\\2&-1 \\end{bmatrix},$$\n\n  $$\\textbf{z} = \\begin{bmatrix} x\\\\y \\end{bmatrix},$$ \n\nand \n\n  $$\\textbf{b} = \\begin{bmatrix} 5\\\\-10 \\end{bmatrix}$$\n\n\n:::\n\n## When is Matrix Invertible?\n\n\nThe following statements for an $n\\times n$ square matrix $\\mathbf{A}$ are **equivalent**:\n\n- $\\mathbf{A}$ is **invertible**:\n    $$\n    \\mathbf{A}^{-1} \\textrm{ exists and } \\mathbf{A}^{-1}\\mathbf{A}=\\mathbf{I}_n\n    $$\n- The system $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$ has a **unique solution** for all $\\mathbf{b}\\neq\\mathbf{0}$ (zero vector)\n- If $\\mathbf{A}\\mathbf{x} = \\mathbf{0}$ (zero vector), it implies that $\\mathbf{x} = \\mathbf{0}$ (zero vector)\n- The column vectors in $\\mathbf{A}$ are **linearly independent** and **spans** $\\mathbb{R}^n$\n- The **rank** of $\\mathbf{A}$ is $n$:\n    $$\n    \\mathrm{rank}(\\mathbf{A})=n\n    $$\n- The **determinant** of $\\mathbf{A}$ is not zero:\n    $$\n    \\det(\\mathbf{A}) \\neq 0\n    $$\n\nConceptually, we want columns of $\\mathbf{A}$ contains enough vectors to reach all coordinates (**span**) $\\mathbb{R}^n$.  \nHowever, when we include too many vectors, we will have **redundant** vectors when the new vectors does not provide new information (you can already express the new vectors using the existing vectors) and so the vectors become **linearly dependent**\n\n\n\n## Linear Independence {#linearindependence}\n\n\n__Linear Dependence__: A set of vectors $\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\}$ is **linearly dependent** if some vector $\\mathbf{v}_i$ is a *linear combination* of the *other vectors* $\\mathbf{v}_j$\n\nIf all vectors **cannot** be written as linear combinations of the other vectors, then the set of vectors are **linearly independent**\n\n$\\mathbf{v}_i$ is **not** a *linear combination* of the *other vectors* $\\mathbf{v}_j$, i.e.,\n\n\n$$\n{\\color{red}\\mathbf{v}_i}\n\\neq \n\\sum_{j\\neq i} c_j \\mathbf{v}_j \n= \\underbrace{c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_n \\mathbf{v}_n}_{\\textrm{linear combination {\\color{red}excluding} } {\\color{red}\\mathbf{v}_i}}\n\\;\\;\\;\\;\\;\\; \\textrm{ for all } i\n$$\n\n\nSubtracting $\\mathbf{v}_i$, this is equivalent to \n\n\n$$\n\\underbrace{\\mathbf{0}}_{\\textrm{zero vector!}}\n\\neq \n{\\color{red} - \\mathbf{v}_i} + \n\\sum_{j\\neq i} c_j \\mathbf{v}_j \n= \n\\underbrace{c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + {\\color{red} (-1) \\mathbf{v}_i} + \\cdots + c_n \\mathbf{v}_n}_{\\textrm{linear combination of } \\mathbf{v}_1, \\mathbf{v}_2, \\ldots, {\\color{red} \\mathbf{v}_i}, \\ldots, \\mathbf{v}_n}\n\\;\\; \\textrm{ for all } i.\n$$\n\n\n\nThis is equivalent to saying: The only solution to \n\n$$\n\\underbrace{\\mathbf{0}}_{\\textrm{zero vector!}}\n= \n\\underbrace{c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots +  \\mathbf{v}_i + \\cdots + c_n \\mathbf{v}_n}_{\\textrm{linear combination of } \\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_i, \\ldots, \\mathbf{v}_n}\n$$\n\nis $c_1 = c_2 = \\cdots = c_n = 0$.\n\nThis is equivalent to the statement that $\\mathbf{A}\\mathbf{x} = \\mathbf{0}$ implies $\\mathbf{x} = \\mathbf{0}$.\n\n\n__Linear independence__:  A set of vectors ${\\bf v}_1, {\\bf v}_2,  \\cdots , {\\bf v}_k$ is linearly independent if the only solution to the equation\n\n\n$$c_1{\\bf v}_1 + c_2{\\bf v}_2 +  \\cdots + c_k{\\bf v}_k = 0$$\n\n\nis $c_1 = c_2 = \\cdots = c_k = 0$.  If another solution exists, the set of vectors is linearly dependent.\n    \nA set $S$ of vectors is linearly dependent if and only if at least one of the vectors in $S$ can be written as a linear combination of the other vectors in $S$.\n\n Linear independence is only defined for sets of vectors with the same number of elements; any linearly independent set of vectors in $n$-space contains at most $n$ vectors.\n \n \n Since $\\begin{bmatrix}9 \\\\ 13 \\\\ 17 \\end{bmatrix}$ is a linear combination of $\\begin{bmatrix}1 \\\\ 2 \\\\ 3 \\end{bmatrix}$, $\\begin{bmatrix} 2 \\\\ 3\\\\ 4\\end{bmatrix}$, and $\\begin{bmatrix} 3 \\\\ 4 \\\\ 5 \\end{bmatrix}$, these 4 vectors constitute a linearly dependent set. \n\n:::{#exm-linearindep} \nAre the following sets of vectors linearly independent?\n  \n  1. $\\begin{bmatrix}2 \\\\ 3 \\\\ 1 \\end{bmatrix}$ and $\\begin{bmatrix}4 \\\\ 6 \\\\ 1 \\end{bmatrix}$\n  2. $\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\end{bmatrix}$, $\\begin{bmatrix}0 \\\\ 5 \\\\ 0 \\end{bmatrix}$, and $\\begin{bmatrix}10 \\\\ 10 \\\\ 0 \\end{bmatrix}$\n\n:::\n\n\n\n:::{#exr-linearindep1}\nAre the following sets of vectors linearly independent?\n  \n  1. $${\\bf v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} , {\\bf v}_2 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix} , {\\bf v}_3 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} $$ \n  \n  2. $${\\bf v}_1 = \\begin{bmatrix} 2 \\\\ 2 \\\\ 1 \\end{bmatrix} , {\\bf v}_2 = \\begin{bmatrix} -4 \\\\ 6 \\\\ 5 \\end{bmatrix} , {\\bf v}_3 = \\begin{bmatrix} -2 \\\\ 8 \\\\ 6 \\end{bmatrix} $$\n\n:::\n\n\n\n\n## Rank of a Matrix\n\nAnother way to think about linear independence is to consider the **rank** of a matrix.  \n    \n__Rank__:  The **rank** of a matrix is the number of linearly independent rows or columns in the matrix.  The rank of a matrix is denoted by $\\textrm{rank}(\\mathbf{A})$.\n\nFor example\n  $$\\begin{bmatrix} 1 & 2 & 3 \\\\\n              0 & 4 & 5 \\\\\n              0 & 0 & 6 \\end{bmatrix}$$\n            \nRank = 3\n            \n  $$\\begin{bmatrix} 1 & 2 & 3 \\\\ \n    0 & 4 & 5 \\\\\n    0 & 0 & 0 \\end{bmatrix}$$\n            \nRank = 2\n\n\n:::{#exr-rank}\n\nFind the rank of each matrix below:\n\n(Hint: transform the matrices into row echelon form. Remember that the number of nonzero rows of a matrix in row echelon form is the rank of that matrix)\n\n1. \n  $$\\begin{bmatrix} 1 & 1 & 2 \\\\ \n  2 & 1 & 3 \\\\\n  1 & 2 & 3 \\end{bmatrix}$$\n\n2.\n  $$\\begin{bmatrix} 1 & 3 & 3 & -3 & 3\\\\ \n  1 & 3 & 1 & 1 & 3 \\\\\n  1 & 3 & 2 & -1 & -2 \\\\\n  1 & 3 & 0 & 3 & -2 \\end{bmatrix}$$\n  \n:::\n\n\n\nAnswer to @exr-rank:\n\n1. rank is 2\n2. rank is 3\n\n\n\n\n## Determinants\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Ip3X9LOh2dk\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen style=\"border:1;display:block;margin:10px auto;\"></iframe>\n\n\n__Singularity__:  Determinants can be used to determine whether a square matrix is nonsingular.\n    \nA square matrix is nonsingular if and only if its determinant is not zero.\n\nDeterminant of a $1 \\times 1$ matrix, equals $|\\mathbf{A}|=|a_{11}|=a_{11}$\n\nDeterminant of a $2 \\times 2$ matrix,   \n\n$$\\mathbf{A}=\\begin{vmatrix} a_{11}&a_{12}\\\\ a_{21}&a_{22} \\end{vmatrix}$$\n\n\n\\begin{align*}\\det({\\bf A}) &= |{\\bf A}|\\\\\n        &= a_{11}|a_{22}| - a_{12}|a_{21}|\\\\\n        &= a_{11}a_{22} - a_{12}a_{21}\n\\end{align*}\n\nWe can extend the second to last equation above to get the definition of the determinant of a $3 \\times 3$ matrix:\n\\begin{align*}\n        \\begin{vmatrix} a_{11}&a_{12}&a_{13}\\\\  a_{21} & a_{22}&a_{23}\\\\ a_{31}&a_{32}&a_{33} \\end{vmatrix} \n            &= \n            a_{11} \\begin{vmatrix} a_{22}&a_{23}\\\\ a_{32}&a_{33} \\end{vmatrix}\n            - a_{12} \\begin{vmatrix} a_{21}&a_{23}\\\\ a_{31}&a_{33} \\end{vmatrix}\n            + a_{13} \\begin{vmatrix} a_{21}&a_{22}\\\\ a_{31}&a_{32} \n            \\end{vmatrix}\\\\\n            &= a_{11}(a_{22}a_{33} - a_{23}a_{32}) - a_{12}(a_{21}a_{33} - a_{23}a_{31}) + a_{13}(a_{21}a_{32} - a_{22}a_{31})\n\\end{align*}\n\nLet's extend this now to any $n\\times n$ matrix. Let's define $\\mathbf{A}_{ij}$ as the $(n-1)\\times (n-1)$ submatrix of $\\mathbf{A}$ obtained by deleting row $i$ and column $j$.  Let the $(i,j)$th __minor__ of $\\mathbf{A}$ be  the determinant of $\\mathbf{A}_{ij}$:\n\n$$M_{ij} = \\left|\\mathbf{A}_{ij}\\right|$$\n\n    \nThen for any $n\\times n$ matrix $\\mathbf{A}$\n\n$$|\\mathbf{A}| = a_{11}M_{11} - a_{12}M_{12} + \\cdots + (-1)^{n+1} a_{1n} M_{1n}$$\n\n\n\n    \nFor example, in figuring out whether the following matrix has an inverse? \n\n$$\\mathbf{A}=\\begin{bmatrix} 1&1&1\\\\0&2&3\\\\5&5&1 \\end{bmatrix}$$\n\n\n1. Calculate its determinant.\n\\begin{align*}\n|\\mathbf{A}| &= 1(2-15) - 1(0-15) + 1(0-10) \\nonumber\\\\\n&= -13+15-10 \\nonumber\\\\\n&= -8\\nonumber\n\\end{align*}\n\n2. Since $|{\\bf A}|\\ne 0$, we conclude that ${\\bf A}$ has an inverse.\n\n:::{#exr-determinants}\n\nDetermine whether the following matrices are nonsingular:\n  \n1. \n  $$\\begin{bmatrix}\n  1 & 0 & 1\\\\\n  2 & 1 & 2\\\\\n  1 & 0 & -1\n  \\end{bmatrix}$$\n              \n2. \n  $$\\begin{bmatrix}\n        2 & 1 & 2\\\\\n        1 & 0 & 1\\\\\n        4 & 1 & 4\n    \\end{bmatrix}$$\n\n:::\n\n\n## Getting Inverse of a Matrix using its Determinant\n\nThus far, we have a number of algorithms to\n\n1. Find the solution of a linear system,\n2. Find the inverse of a matrix\n\nbut these remain just that --- algorithms.  At this point, we have no way of telling how the solutions $x_j$ change as the parameters $a_{ij}$ and $b_i$ change, except by changing the values and \"rerunning\" the algorithms.\n\n\nWith determinants, we can provide an explicit formula for the inverse and\ntherefore provide an explicit formula for the solution of an $n\\times n$ linear system.\n\nHence, we can examine how changes in the parameters and $b_i$ affect the solutions $x_j$.\n\n__Determinant Formula for the Inverse of a $2 \\times 2$__: \n\nThe determinant of a $2 \\times 2$ matrix $\\mathbf{A}=\\begin{bmatrix} a & b\\\\ c & d\\\\ \\end{bmatrix}$ is defined as:\n\n\n$$\\det(\\mathbf{A}) = a d - b c$$\n\n\nThe inverse of $\\mathbf{A}$ is given by:\n\n\n$$\\frac{1}{\\det({\\bf A})} \\begin{bmatrix} d & -b \\\\ -c & a\\\\ \\end{bmatrix}$$\n\n        \n\nFor example, Let's calculate the inverse of matrix A from @exr-invlinsys using the determinant formula. \n\n\nRecall, \n\n  $$\\mathbf{A} = \\begin{bmatrix}\n              -3 & 4\\\\\n              2 & -1\\\\\n          \\end{bmatrix}$$\n          \n\n  $$\\det(\\mathbf{A}) = (-3)(-1) - (4)(2) = 3 - 8  = -5$$\n\n  \n  $$\\frac{1}{\\det(\\mathbf{A})} \\begin{bmatrix}\n              -1 & -4\\\\\n              -2 & -3\\\\\n          \\end{bmatrix}$$\n          \n  $$\\frac{1}{-5} \\begin{bmatrix}\n              -1 & -4\\\\\n              -2 & -3\\\\\n          \\end{bmatrix}$$\n          \n  $$\\begin{bmatrix}\n              \\frac{1}{5} & \\frac{4}{5}\\\\\n              \\frac{2}{5} & \\frac{3}{5}\\\\\n          \\end{bmatrix}$$\n\n\n:::{#exr-calcinverse}\n\nCaculate the inverse of $\\mathbf{A}$\n  $$\\mathbf{A} = \\begin{bmatrix}\n              3 & 5\\\\\n              -7 & 2\\\\\n          \\end{bmatrix}$$\n\n:::\n\n\n\n## Answers to Examples and Exercises {-}\n\n\n\nAnswer to @exm-linearindep:\n\n  1. yes\n  2. no\n\nAnswer to @exr-linearindep1:\n\n  1. yes\n  2. no ($-v_1 -v_2 + v_3  = 0$)\n  \n\n\nAnswer to @exr-rank:\n\n1. rank is 2\n\n2. rank is 3\n\nAnswer to @exm-inverse:\n\n$\\left(\\begin{array}{ccc|ccc}\n    1&1&1&1&0&0\\\\\n    0&2&3&0&1&0\\\\\n    5&5&1&0&0&1\n\\end{array} \\right)$\n        \n$\\left(\\begin{array}{ccc|ccc}\n    1&1&1 &1 &0&0\\\\\n    0&2&3 &0 &1&0\\\\\n    0&0&-4&-5&0&1\n\\end{array} \\right)$\n\n$\\left(\\begin{array}{ccc|ccc}\n    1&1&1&1  &0&0\\\\\n    0&2&3&0  &1&0\\\\\n    0&0&1&5/4&0&-1/4\n\\end{array} \\right)$\n\n$\\left(\\begin{array}{ccc|ccc}\n    1&1&0&-1/4 &0&1/4\\\\\n    0&2&0&-15/4&1&3/4\\\\\n    0&0&1&5/4  &0&-1/4\n\\end{array} \\right)$\n\n$\\left(\\begin{array}{ccc|ccc}\n    1&1&0&-1/4 &0  &1/4\\\\\n    0&1&0&-15/8&1/2&3/8\\\\\n    0&0&1&5/4  &0 &-1/4\n\\end{array} \\right)$\n    \n$\\left(\\begin{array}{ccc|ccc}\n    1&0&0&13/8 &-1/2&-1/8\\\\\n    0&1&0&-15/8&1/2 &3/8\\\\\n    0&0&1&5/4  &0   &-1/4\n\\end{array} \\right)$\n\n${\\bf A}^{-1} = \\left(\\begin{array}{ccc}\n    13/8 &-1/2&-1/8\\\\\n    -15/8&1/2 &3/8\\\\\n    5/4  &0   &-1/4\n\\end{array} \\right)$\n\n\nAnswer to @exr-inverse1:\n\n1. ${\\bf A}^{-1}=\\begin{bmatrix} 1&0&-4\\\\0&\\frac{1}{2}&0\\\\0&0&1 \\end{bmatrix}$\n\nAnswer to @exr-invlinsys:\n\n$\\textbf{z} = \\bf{A}^{-1} \\bf{b} = \\begin{bmatrix}\n    1/5 &4/5\\\\\n    2/5&3/5 \n\\end{bmatrix} \\begin{bmatrix}\n    5 \\\\\n    -10\n\\end{bmatrix}= \\begin{bmatrix}\n    -7 \\\\\n    -4\n\\end{bmatrix} = \\begin{bmatrix}\n    x \\\\\n    y\n\\end{bmatrix}$\n\nAnswer to @exr-determinants:\n\n1. nonsingular\n\n2. singular\n\nAnswer to @exr-calcinverse:\n\n$\\begin{bmatrix}\n            \\frac{2}{41} & \\frac{-5}{41}\\\\\n            \\frac{7}{41} & \\frac{3}{41}\\\\\n        \\end{bmatrix}$\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}