{
  "hash": "0595683079f75887a08d69c37ed64ef4",
  "result": {
    "markdown": "# Differential Calculus {#derivatives}\n\n\n\n\n\n\nCalculus is a fundamental part of any type of statistics exercise. Although you may not be taking derivatives and integral in your daily work as an analyst, calculus undergirds many concepts we use: maximization, expectation, and cumulative probability.\n\n\n## Derivatives {#sec-derivintro}\n\nThe derivative of $f$ at $x$ is its rate of change at $x$: how much $f(x)$ changes with a change in $x$. The rate of change is a fraction --- rise over run --- but because not all lines are straight and the rise over run formula will give us different values depending on the range we examine, we need to take a limit.\n\n\n:::{#def-derivative}\n### Derivative\nLet $f$ be a function whose domain includes an open interval containing the point $x$.  The derivative of $f$ at $x$ is given by\n\n\n$$\\frac{d}{dx}f(x) =\\lim\\limits_{h\\to 0} \\frac{f(x+h)-f(x)}{(x+h)-x} = \\lim\\limits_{h\\to 0} \\frac{f(x+h)-f(x)}{h}$$\n\n\nThere are a two main ways to denote a derivate:\n\n* Leibniz Notation: $\\frac{d}{dx}(f(x))$\n* Prime or Lagrange Notation: $f'(x)$\n\n:::\n\nIf $f(x)$ is a straight line, the derivative is the slope. For a curve, the slope changes by the values of $x$, so the derivative is the slope of the line tangent to the curve at $x$. See, For example, @fig-derivsimple.\n\n\n\n::: {.cell hash='32_derivatives_cache/epub/fig-derivsimple_d82d7d09ac8de664cdf1d97e7d4258d3'}\n::: {.cell-output-display}\n![The Derivative as a Slope](32_derivatives_files/figure-epub/fig-derivsimple-1.png){#fig-derivsimple}\n:::\n:::\n\n\n\nIf $f'(x)$ exists at a point $x_0$, then $f$ is said to be __differentiable__ at $x_0$. That also implies that $f(x)$ is continuous at $x_0$. \n\n\n\n### Properties of derivatives  {-}\n\nSuppose that $f$ and $g$ are differentiable at $x$ and that $\\alpha$ is a constant.  Then the functions $f\\pm g$, $\\alpha f$, $f g$, and $f/g$ (provided $g(x)\\ne 0$) are also differentiable at $x$.  Additionally,\n\n__Constant rule:__ $$\\left[k f(x)\\right]' = k f'(x)$$\n\n__Sum rule:__ $$\\left[f(x)\\pm g(x)\\right]' = f'(x)\\pm g'(x)$$\n\nWith a bit more algebra, we can apply the definition of derivatives to get a formula for of the derivative of a product and a derivative of a quotient.\n\n__Product rule:__ $$\\left[f(x)g(x)\\right]^\\prime = f^\\prime(x)g(x)+f(x)g^\\prime(x)$$\n\n__Quotient rule:__  $$\\left[f(x)/g(x)\\right]^\\prime = \\frac{f^\\prime(x)g(x) - f(x)g^\\prime(x)}{[g(x)]^2}, ~g(x)\\neq 0$$\n\n\nFinally,  one way to think of the power of derivatives is that it takes a function a notch down in complexity. The power rule applies to any higher-order function:\n\n__Power rule:__ $$\\left[x^k\\right]^\\prime = k x^{k-1}$$\n\nFor any real number $k$ (that is, both whole numbers and fractions). The power rule is proved __by induction__, a neat method of proof used in many fundamental applications to prove that a general statement holds for every possible case, even if there are countably infinite cases.  We'll show a simple case where $k$ is an integer here.\n\n:::{.proof}\nWe would like to prove that \n\n\n$$\\left[x^k\\right]^\\prime = k x^{k-1}$$\n\nfor any integer $k$.\n\nFirst, consider the first case (the base case) of $k = 1$. We can show by the definition of derivatives (setting $f(x) = x^1 = 1$) that \n\n\n$$[x^1]^\\prime = \\lim_{h \\rightarrow 0}\\frac{(x + h) - x}{(x + h) - x}= 1.$$\n\n\nBecause $1$ is also expressed as $1 x^{1- 1}$, the statement we want to prove holds for the case $k =1$.\n\nNow, \\emph{assume} that the statement holds for some integer $m$. That is, assume \n\n$$\\left[x^m\\right]^\\prime = m x^{m-1}$$\n\n\nThen, for the case $m + 1$, using the product rule above, we can simplify\n\n\\begin{align*}\n\\left[x^{m + 1}\\right]^\\prime &= [x^{m}\\cdot x]^\\prime\\\\\n&= (x^m)^\\prime\\cdot x + (x^m)\\cdot (x)^\\prime\\\\\n&= m x^{m - 1}\\cdot x + x^m ~~\\because \\text{by previous assumption}\\\\\n&= mx^m + x^m\\\\\n&= (m + 1)x^m\\\\\n&= (m + 1)x^{(m + 1) - 1}\n\\end{align*}\n\nTherefore, the rule holds for the case $k = m + 1$ once we have assumed it holds for $k = m$. Combined with the first case, this completes proof by induction -- we have now proved that the statement holds for all integers $k = 1, 2, 3, \\cdots$. \n\nTo show that it holds for real fractions as well, we can prove expressing that exponent by a fraction of two integers.\n\n:::\n\n\n\n\n\nThese \"rules\"\tbecome apparent by applying the definition of the derivative above to each of the things to be \"derived\", but these come up so frequently that it is best to repeat until it is muscle memory. \n\n\n:::{#exr-introderivatives} \n\nFor each of the following functions, find the first-order derivative $f^\\prime(x)$. \n\n1. $f(x)=c$\n2. $f(x)=x$\n3. $f(x)=x^2$\n4. $f(x)=x^3$\n5. $f(x)=\\frac{1}{x^2}$\n6. $f(x)=(x^3)(2x^4)$\n7. $f(x) = x^4 - x^3 + x^2 - x + 1$\n8. $f(x) = (x^2 + 1)(x^3 - 1)$\n9. $f(x) = 3x^2 + 2x^{1/3}$\n10. $f(x)=\\frac{x^2+1}{x^2-1}$\n\n:::\n\n\n## Higher-Order Derivatives {#derivpoly}\n\nThe first derivative is applying the definition of derivatives on the function, and it can be expressed as\n\n\n$$f'(x),  ~~ y',  ~~ \\frac{d}{dx}f(x), ~~ \\frac{dy}{dx}$$\n\n\nWe can keep applying the differentiation process to functions that are themselves derivatives.  The derivative of $f'(x)$ with respect to $x$, would then be $$f''(x)=\\lim\\limits_{h\\to 0}\\frac{f'(x+h)-f'(x)}{h}$$ and we can therefore call it the __Second derivative:__ \n\n\n\n$$f''(x), ~~ y'', ~~ \\frac{d^2}{dx^2}f(x), ~~ \\frac{d^2y}{dx^2}$$\n\n\n\nSimilarly, the derivative of $f''(x)$ would be called the third derivative and is denoted $f'''(x)$. And by extension, the __nth derivative__  is expressed as  $\\frac{d^n}{dx^n}f(x)$, $\\frac{d^ny}{dx^n}$.\n\n\n\n:::{#exm-succ-der}\n\n\\begin{align*}\nf(x) &=x^3\\\\\nf^{\\prime}(x) &=3x^2\\\\\nf^{\\prime\\prime}(x) &=6x \\\\\nf^{\\prime\\prime\\prime}(x) &=6\\\\\nf^{\\prime\\prime\\prime\\prime}(x) &=0\\\\\n\\end{align*}\n:::\n\nEarlier, in @sec-derivintro, we said that if a function differentiable at a given point, then it must be continuous. Further, if $f'(x)$ is itself continuous, then $f(x)$ is called continuously differentiable. All of this matters because many of our findings about optimization rely on differentiation, and so we want our function to be differentiable in as many layers.  A function that is continuously differentiable infinitly is called \"smooth\". Some examples: $f(x) = x^2$, $f(x) = e^x$. \n\n\n## The Chain Rule\n\nAs useful as the above rules are, many functions you'll see won't fit neatly in each case immediately. Instead, they will be functions of functions. For example, the difference between $x^2 + 1^2$ and $(x^2 + 1)^2$  may look trivial, but the sum rule can be easily applied to the former, while it's actually not obvious what do with the latter. \n\n__Composite functions__ are formed by substituting one function into another and are denoted by $$(f\\circ g)(x)=f[g(x)].$$  To form $f[g(x)]$, the range of $g$ must be contained (at least in part) within the domain of $f$. The domain of $f\\circ g$ consists of all the points in the domain of $g$ for which $g(x)$ is in the domain of $f$.\n\n:::{#exm-}\nLet $f(x)=\\ln x$ for  $0<x<\\infty$ and  $g(x)=x^2$ for $-\\infty<x<\\infty$.\n\nThen \n\n$$(f\\circ g)(x)=\\ln x^2, -\\infty<x<\\infty - \\{0\\}$$\n\n\nAlso \n\n$$(g\\circ f)(x)=[\\ln x]^2, 0<x<\\infty$$\n\n\nNotice that $f\\circ g$ and $g\\circ f$ are not the same functions.\n:::\n\nWith the notation of composite functions in place, now we can introduce a helpful additional rule that will deal with a derivative of composite functions as a chain of concentric derivatives. \n\n__Chain Rule__:  \n\nLet $y=(f\\circ g)(x)= f[g(x)]$. The derivative of $y$ with respect to $x$ is $$\\frac{d}{dx} \\{ f[g(x)] \\} = f'[g(x)] g'(x)$$ \n\nWe can read this as: \"the derivative of the composite function $y$ is the derivative of $f$ evaluated at $g(x)$, times the derivative of $g$. \"\n\nThe chain rule can be thought of as the derivative of the \"outside\" times the derivative of the \"inside\", remembering that the derivative of the outside function is evaluated at the value of the inside function.\n\n\n* The chain rule can also be written as $$\\frac{dy}{dx}=\\frac{dy}{dg(x)} \\frac{dg(x)}{dx}$$ This expression does not imply that the $dg(x)$'s cancel out, as in fractions.  They are part of the derivative notation and you can't separate them out or cancel them.)\t\n\t\n\n:::{#exm-tothesix}\nFind $f^\\prime(x)$ for $f(x) = (3x^2+5x-7)^6$.\n:::\n\n\nThe direct use of a chain rule is when the exponent of is itself a function, so the power rule could not have applied generaly:\n\n__Generalized Power Rule__:  \n\nIf $f(x)=[g(x)]^p$ for any rational number $p$, $$f^\\prime(x) =p[g(x)]^{p-1}g^\\prime(x)$$\n\n\n## Derivatives of logs and exponents\n\nNatural logs and exponents (they are inverses of each other; see Prerequisites) crop up everywhere in statistics. Their derivative is a special case from the above, but quite elegant. \n\n:::{#thm-derivexplog}\n\nThe functions $e^x$ and the natural logarithm $\\ln(x)$ are continuous and differentiable in their domains, and their first derivate is\n\n  $$(e^x)^\\prime = e^x$$\n\n  $$\\ln(x)^\\prime = \\frac{1}{x}$$\n\n\nAlso, when these are composite functions, it follows by the generalized power rule that\n\n\n$$\\left(e^{g(x)}\\right)^\\prime = e^{g(x)} \\cdot g^\\prime(x)$$\n\n$$\\left(\\ln g(x)\\right)^\\prime = \\frac{g^\\prime(x)}{g(x)}, ~~\\text{if}~~ g(x) > 0$$\n\n\n:::\n\n\n\n\nWe will relegate the proofs to small excerpts. \n\n\n\n\n### Derivatives of exponents {-}\n\nTo repeat the main rule in @thm-derivexplog, the intuition is that  \n\n1. Derivative of $e^x$ is itself: $\\frac{d}{dx}e^x = e^x$ (See @fig-derivexponent)\n2. Same thing if there were a constant in front: $\\frac{d}{dx}\\alpha e^x = \\alpha e^x$\n3. Same thing no matter how many derivatives there are in front: $\\frac{d^n}{dx^n} \\alpha e^x = \\alpha e^x$\n4. Chain Rule: When the exponent is a function of $x$, remember to take derivative of that function and add to product. $\\frac{d}{dx}e^{g(x)}= e^{g(x)} g^\\prime(x)$\n\n\n\n::: {.cell hash='32_derivatives_cache/epub/fig-derivexponent_6bbbd27c650b7a88add8f2723234964a'}\n::: {.cell-output-display}\n![Derivative of the Exponential Function](32_derivatives_files/figure-epub/fig-derivexponent-1.png){#fig-derivexponent}\n:::\n:::\n\n\n:::{#exm-exmderivexp}\n\nFind the derivative for the following.\n\n1. $f(x)=e^{-3x}$\n2. $f(x)=e^{x^2}$\n3. $f(x)=(x-1)e^x$\n\n:::\n\n\n\n### Derivatives of logs {-}\n\nThe natural log  is the mirror image of the natural exponent and has mirroring properties, again, to repeat the theorem, \n\n1. log prime x is one over x (@fig-derivlog): \n\n  $$\\frac{d}{dx} \\ln x = \\frac{1}{x}$$ \n\n2. Exponents become multiplicative constants: \n\n  $$\\frac{d}{dx} \\ln x^k = \\frac{d}{dx} k \\ln x = \\frac{k}{x}$$\n\n3. Chain rule again: \n\n  $$\\frac{d}{dx} \\ln u(x) = \\frac{u'(x)}{u(x)}\\quad$$\n\n4. For any positive base $b$, \n\n  $$\\frac{d}{dx} b^x = (\\ln b)\\left(b^x\\right)$$\n\n::: {.cell hash='32_derivatives_cache/epub/fig-derivlog_1cb72b1beda4c2c5afe78e3868b69c89'}\n::: {.cell-output-display}\n![Derivative of the Natural Log](32_derivatives_files/figure-epub/fig-derivlog-1.png){#fig-derivlog}\n:::\n:::\n\n\n:::{#exm-exmderivlog}\n\nFind $dy/dx$ for the following.\n\n1. $f(x)=\\ln(x^2+9)$\n2. $f(x)=\\ln(\\ln x)$\n3. $f(x)=(\\ln x)^2$\n4. $f(x)=\\ln e^x$ \n\n:::\n\n\n### Outline of Proof {-}\n\nWe actually show the derivative of the log first, and then the derivative of the exponential naturally follows. \n\nThe general derivative of the log at any base $a$ is solvable by the definition of derivatives.\n\n\\begin{align*}\n(\\ln_a x)^\\prime = \\lim\\limits_{h\\to 0} \\frac{1}{h}\\ln_{a}\\left(1 + \\frac{h}{x}\\right)\n\\end{align*}\n\nRe-express $g = \\frac{h}{x}$ and get \n\\begin{align*}\n(\\ln_a x)^\\prime &= \\frac{1}{x}\\lim_{g\\to 0}\\ln_{a} (1 + g)^{\\frac{1}{g}}\\\\\n&= \\frac{1}{x}\\ln_a e\n\\end{align*}\n\nBy definition of $e$. As a special case, when $a = e$, then $(\\ln x)^\\prime = \\frac{1}{x}$. \n\nNow let's think about the inverse, taking the derivative of $y = a^x$.\n\n\\begin{align*}\ny &= a^x \\\\\n\\Rightarrow \\ln y &= x \\ln a\\\\\n\\Rightarrow \\frac{y^\\prime}{y} &= \\ln a\\\\\n\\Rightarrow  y^\\prime = y \\ln a\\\\\n\\end{align*}\n\nThen  in the special case where $a = e$, \n\n\n$$(e^x)^\\prime = (e^x)$$\n\n\n\n\n\n## Partial Derivatives\n\nWhat happens when there's more than variable that is changing?\n\n> If you can do ordinary derivatives, you can do partial derivatives: just hold all the other input variables constant except for the one you're differentiating with respect to. (Joe Blitzstein's Math Notes)\n\nSuppose we have a function $f$ now of two (or more) variables and we want to determine the rate of change relative to one of the variables. To do so, we would find its partial derivative, which is defined similar to the derivative of a function of one variable. \n\n__Partial Derivative__:  Let $f$ be a function of the variables $(x_1,\\ldots,x_n)$.  The partial derivative of $f$ with respect to $x_i$ is \n\n\n$$\\frac{\\partial f}{\\partial x_i} (x_1,\\ldots,x_n) = \\lim\\limits_{h\\to 0} \\frac{f(x_1,\\ldots,x_i+h,\\ldots,x_n)-f(x_1,\\ldots,x_i,\\ldots,x_n)}{h}$$\n\n\nOnly the $i$th variable changes --- the others are treated as constants.\n\n\nWe can take higher-order partial derivatives, like we did with functions of a single variable, except now the higher-order partials can be with respect to multiple variables.\n\n:::{#exm-}\nNotice that you can take partials with regard to different variables. \n\nSuppose $f(x,y)=x^2+y^2$. Then\n\n\\begin{align*}\n\\frac{\\partial f}{\\partial x}(x,y) &=\\\\\n\\frac{\\partial f}{\\partial y}(x,y) &=\\\\\n\\frac{\\partial^2 f}{\\partial x^2}(x,y) &=\\\\\n\\frac{\\partial^2 f}{\\partial x \\partial y}(x,y) &=\n\\end{align*}\n:::\n\n\n:::{#exr-}\nLet $f(x,y)=x^3 y^4 +e^x -\\ln y$. What are the following partial derivaitves?\n\n\\begin{align*}\n\\frac{\\partial f}{\\partial x}(x,y) &=\\\\\n\\frac{\\partial f}{\\partial y}(x,y) &=\\\\\n\\frac{\\partial^2 f}{\\partial x^2}(x,y) &=\\\\\n\\frac{\\partial^2 f}{\\partial x \\partial y}(x,y) &= \n\\end{align*}\n  \n:::\n\n\n\n\n## Taylor Approximation {#taylorapprox}\n\nA common form of approximation used in statistics involves derivatives. A Taylor series is a way to represent common functions as infinite series (a sum of infinite elements) of the function's derivatives at some point $a$. \n\nFor example, Taylor series are very helpful in representing nonlinear (read: difficult) functions as linear (read: manageable) functions. One can thus __approximate__ functions by using lower-order, finite series known as __Taylor polynomials__. If $a=0$, the series is called a Maclaurin series.\n  \n\nSpecifically, a Taylor series of a real or complex function  $f(x)$ that is infinitely differentiable in the neighborhood of point $a$ is: \n\n\n\\begin{align*}\n\tf(x) &= f(a) + \\frac{f'(a)}{1!} (x-a) +  \\frac{f''(a)}{2!} (x-a)^2 + \\cdots\\\\\n\t &= \\sum_{n=0}^\\infty \\frac{f^{(n)} (a)}{n!} (x-a)^n\n\\end{align*}\n  \n__Taylor Approximation__: We can often approximate the curvature of a function $f(x)$ at point $a$ using a 2nd order Taylor polynomial around point $a$: \n\n\n$$f(x) = f(a) + \\frac{f'(a)}{1!} (x-a) +  \\frac{f''(a)}{2!} (x-a)^2 + R_2$$\n\n\n\n$R_2$ is the remainder (R for remainder, 2 for the fact that we took two derivatives) and often treated as negligible,\ngiving us:\n\n\n$$f(x) \\approx f(a) + f'(a)(x-a) +  \\dfrac{f''(a)}{2} (x-a)^2$$\n\n\nThe more derivatives that are added, the smaller the remainder $R$ and the more accurate the approximation. Proofs involving limits guarantee that the remainder converges to 0 as the order of derivation increases.  \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": {},
    "postProcess": true
  }
}