<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>UCSD Political Science Math Camp - 13&nbsp; Measuring Uncertainty</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./42_logic_probability.html" rel="next">
<link href="./34_intergrals.html" rel="prev">
<link href="./favicon.ico" rel="icon">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script><style>html{ scroll-behavior: smooth; }</style>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean"
}
</script><script async="" src="https://hypothes.is/embed.js"></script><script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script><script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script><script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">
<link rel="stylesheet" href="style.css">
</head>
<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"><div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Measuring Uncertainty</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">UCSD Political Science Math Camp</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/UCSDPoliMathCamp/MathCamp/" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle sidebar-tool" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">About this Booklet</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00_warmup.html" class="sidebar-item-text sidebar-link">Warmup Questions</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_prerequisites.html" class="sidebar-item-text sidebar-link">Prerequisites</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">I Introduction to R</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11_orientation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">RStudio and Reading in Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12_visualization.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Visualization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13_data_wrangling_cleaning.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Data Wrangling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14_loops_simulations.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Loops and Simulation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15_non-wysiwyg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">LaTeX and Markdown</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">II Linear Algebra</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21_vector_matrix.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Matrix Operations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22_linear_systems.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Systems of Linear Equations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23_matrix_inverse.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Matrix Inverse and Linear Independence</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">III Calculus</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./31_limits.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Limits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./32_derivatives.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Differential Calculus</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./33_optimization.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Optimization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./34_intergrals.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Integral Calculus</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">IV Probability</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./41_measuring_uncertainty.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Measuring Uncertainty</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./42_logic_probability.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">The Logic of Probability</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./43_conditional.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Conditional Probability</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./44_distributions.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Probability Distributions</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./51_solutions-warmup.html" class="sidebar-item-text sidebar-link">Warmup Questions Solutions</a>
  </div>
</li>
    </ul>
</div>
</nav><!-- margin-sidebar --><div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">On this page</h2>
   
  <ul>
<li>
<a href="#what-is-probability" id="toc-what-is-probability" class="nav-link active" data-scroll-target="#what-is-probability"><span class="toc-section-number">13.1</span>  What is Probability?</a>
  <ul>
<li><a href="#the-classical-definition" id="toc-the-classical-definition" class="nav-link" data-scroll-target="#the-classical-definition">The Classical Definition</a></li>
  <li><a href="#the-frequentist-definition" id="toc-the-frequentist-definition" class="nav-link" data-scroll-target="#the-frequentist-definition">The Frequentist Definition</a></li>
  <li><a href="#the-subjective-definition" id="toc-the-subjective-definition" class="nav-link" data-scroll-target="#the-subjective-definition">The Subjective Definition</a></li>
  <li><a href="#the-axiomatic-definition" id="toc-the-axiomatic-definition" class="nav-link" data-scroll-target="#the-axiomatic-definition">The Axiomatic Definition</a></li>
  <li><a href="#probability-as-logic" id="toc-probability-as-logic" class="nav-link" data-scroll-target="#probability-as-logic">Probability as Logic</a></li>
  </ul>
</li>
  </ul><div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/UCSDPoliMathCamp/MathCamp/edit/main/41_measuring_uncertainty.Rmd" class="toc-action">Edit this page</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="uncertainty" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Measuring Uncertainty</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header><p>Social science research is hard—in part because human behavior is so unpredictable. Unlike fields such as physics and chemistry, our theories are rarely deterministic. Rather, we express our arguments in terms of probabilities or likelihoods. “Democratic countries <em>tend</em> to have lower levels of corruption”. Or “the belief that their vote matters increases the <em>likelihood</em> that an individual will turn out on election day”. Theories like these highlight why statistics and probability theory are such useful tools for social scientists. The goal of statistics is to use data to measure our uncertainty about a proposition, or hypothesis. And probability is the language we use to describe this uncertainty.</p>
<p>In this section of the book we will cover the theory of probability that underlies commonly used statistical methods you will encounter in subsequent courses.</p>
<p>The material for this section is drawn from the following sources:</p>
<ul>
<li>Jaynes E. T and G. Larry Bretthorst. 2003. <em>Probability Theory: The Logic of Science</em>. Cambridge UK: Cambridge University Press.</li>
<li>Clayton Aubrey. 2021. <em>Bernoulli’s Fallacy : Statistical Illogic and the Crisis of Modern Science</em>. New York: Columbia University Press.</li>
<li>Kurt Will. 2019. <em>Bayesian Statistics the Fun Way</em>. San Francisco: No Starch Press.</li>
<li><a href="https://www.youtube.com/watch?v=rfKS69cIwHc&amp;list=PL9v9IXDsJkktefQzX39wC2YG07vw7DsQ_&amp;ab_channel=AubreyClayton">Aubrey Clayton. 2015. YouTube Lectures on Probability Theory: The Logic of Science</a></li>
</ul>
<section id="what-is-probability" class="level2" data-number="13.1"><h2 data-number="13.1" class="anchored" data-anchor-id="what-is-probability">
<span class="header-section-number">13.1</span> What is Probability?</h2>
<p>What does the phrase “the probability of a coin flip coming up heads is one half” mean? You may be surprised to learn that there are many different answers to this question. In this section we will discuss the various definitions of probability philosophers and statisticians have come up with. As we will see, many commonly used definitions have appealing qualities. But each ultimately falls short in important ways. The exception being viewing probability as an extension of logic. This idea, developed by Edwin T. Jaynes in his book <em>Probability Theory: the Logic of Science</em> (2003), is rarely found in introductions to probability, but as we will see, is the most coherent way to understand and reason about probabilistic statements. Before diving into probability-as-logic, let’s take a look at the strengths and weaknesses of definitions with which you may be more familiar.</p>
<section id="the-classical-definition" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="the-classical-definition">The Classical Definition</h4>
<p>The mathematics of probability originated in the study of gambling games. People wanted to know the answer to questions like “if I roll two six-sided dice, what is the probability that their values sum to seven”? This would have implications for how much to wager on a given bet. In the 15th and 16th centuries, mathematicians came up with the following solution.</p>
<p><span class="math display">
\text{P(event A)} = \frac{\text{\# of ways event A can occur}}{\text{\# of all possible outcomes}}
</span></p>
<p>This classical definition of probability is still used in many introductory textbooks today—probably because it can be intuitively applied to most common examples used in teaching (e.g.&nbsp;flipping coins, rolling dice, dealing cards from a deck, or drawing balls from an urn). What is the probability that a coin will come up heads when we flip it? Well, there are two possible outcomes when we flip a coin: either it lands heads or tails, and there is one way for it to land heads. So our calculation would be:</p>
<p><span class="math display">
\frac{\text{heads}}{\text{[heads, tails]}} = \frac{1}{1 + 1} = \frac{1}{2}
</span></p>
<p>The classical definition also makes it clear to us that events which can happen in more ways are more likely. When we roll two six-sided dice, there are <span class="math inline">6*6 = 36</span> possible outcomes. Getting a sum equal to <em>seven</em> can happen in five ways [(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)] whereas getting a sum equal to <em>two</em> can only happen in one way [(1, 1)]. Therefore, according to the classical definition of probability, rolling two dice and getting a sum of seven is five times more likely than getting a sum of two.</p>
<p>The problem with the classical definition of probability is that is does a poor job describing uncertainty outside of simple games of chance. One of the main challenges comes from having to enumerate the “number of all possible outcomes” in the denominator. We can figure this out easily enough for flipping coins and rolling dice because there is an apparent symmetry in the outcome-space, but in the social sciences we rarely study such clear-cut systems. Take the statement “after receiving the encouragement treatment, an individual will turn out to vote with 70% probability.” There are only two outcomes: either the individual votes or they do not vote. So should the probability of voting always be 50/50? There is no apparent way to cut up the outcome space such that the classical probability formula gives us an answer like <span class="math inline">7/10</span>.</p>
<p>Another problem with the classical definition is that it requires each event in the denominator to be equi-probable. This is easy to assume when the events have a symmetric quality to them: flipping a coin, drawing cards from a deck, rolling dice etc. But what if we wanted to know the probability of rolling a specific value from a mis-shapen die? Furthmore, the fact that the denominator term requires events to be “equi-probable” means that this definition of probability is circular!</p>
</section><section id="the-frequentist-definition" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="the-frequentist-definition">The Frequentist Definition</h4>
<p>Beginning in the 19th century, a new definition of probability known as frequentism began to be widely used. According to frequentism, the probability of an event is defined by its long-run numerical frequency. We know that the probability of a coin toss coming up heads is <span class="math inline">1/2</span> because, if we were to toss a coin enough times, in the limit 50% of the tosses would come up heads. The mathematical basis for frequentism comes from Swiss mathematician Jacob Bernoulli’s Law of Large Numbers.</p>
<blockquote class="blockquote">
<p>“Take a large enough sample that you can be morally certain, to whatever degree that means to you, the ratio in the sample is within your desired tolerance of the [true ratio].”<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
</blockquote>
<p>Frequentism has an appealing objective quality to it because probabilities can supposedly be arrived at via empirical frequencies. But how can we ever truly verify that these frequencies match reality? In social science research we only get one shot of our data if we run a survey or a field experiment. And what about data from one-time events such as elections or wars? “The probability that the Democrats retain control of Congress in 2022 is 0.4.” According to frequentists, we would have to imagine re-running the 2022 mid-term election an infinite number of times in order to make sense of this statement. But if we re-ran the election under all the same circumstances, wouldn’t the outcome remain the same? Like the classical definition of probability, frequentism “works” for simple examples but leads to paradoxes when applied to modern scientific problems. Despite this, most statistical methods used today are based on the frequentist paradigm.</p>
<!-- As an aside, it is also worth noting that frequentist methods were developed by a set of individuals---chiefly Francis Galton, Karl Pearson, and Ronald Fisher---whose scientific interests were directed towards supporting eugenicist projects around the turn of the 20th century. These men were attracted to the apparent objectivity of frequentism (as opposed to subjective probability which we will turn to next) because they hoped to prove that human groups had innate differences. These "objective" differences somehow always showed how white Europeans were superior to colonized indigenous populations. For more on the dark origins of frequentist methods see [this article](https://nautil.us/how-eugenics-shaped-statistics-9365/). -->
</section><section id="the-subjective-definition" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="the-subjective-definition">The Subjective Definition</h4>
<p>Standing in radical opposition to the frequentist and classical definitions is a view that probability statements are no more than someone’s subjective degree of belief about a proposition. The nice thing about the subjective definition is that it is often how we talk about probability in our day to day lives. The statement “the probability that the Democrats retain control of Congress in 2022 is 0.4” is a perfectly valid way to express how certain we are about some outcome. We can also apply this to things like coin flips: “I don’t have any reason to think either heads or tails is more likely, so the probability of heads is 50/50.”</p>
<p>Unfortunately, the subjective view of probability does not provide any way to compare competing probability statements. If I say the probability of flipping a coin and getting heads is <span class="math inline">1/2</span>, but you say it is <span class="math inline">1/8</span>, who can we say is more correct without some sort of empirical verification? This problem also exists for one-time events. Let’s say I claim that the Democrats have a 20% chance of winning the election, and you claim they have an 80% chance of winning the election. The election happens and the Democrats win—who made the better prediction?</p>
<p>Subjective probability is often associated with Bayesian theories of probability. Bayes’ theorem, however, is simply a well-established mathematical relation. And it is used in every type of probability framework. If you are not familiar with Bayes’ theorem, the basic version looks like</p>
<p><span class="math display">
\text{P}(A|B) = \frac{\text{P}(A) \ \text{P}(B|A)}{\text{P}(B)}
</span></p>
<p>The most controversial part of this formula is <span class="math inline">\text{P}(A)</span>, also known as the <em>prior</em>. Frequentist statistical methods ignore this part of Bayes’ theorem when conducting inference, in part because of its association with a “subjective” way of thinking. But by doing so they implicitly assume all values of <span class="math inline">\text{P}(A)</span> are equally likely—which is itself a subjective choice! We will cover Bayes’ theorem extensively in our chapter on conditional probability.</p>
<!-- Lastly, the subjective definition of probability does not give us any nice tools for working with probabilities mathematically. The following basic axiom of probability -->
<!-- $$ -->
<!-- \text{P(event A)} + \text{P(not event A)} = 1 -->
<!-- $$ -->
<!-- says that probabilities must be *complementary*. Either event A happens, or it doesn't happen, with certainty. And by "certainty" we mean probability equals $1$. But if probability is purely subjective, there is no reason we would have to force probabilities to sum to $1$ in this way.  -->
</section><section id="the-axiomatic-definition" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="the-axiomatic-definition">The Axiomatic Definition</h4>
<p>In 1933 Russian mathematician Andrei Kolmogorov worked out a set of axioms governing how probability worked. Kolmogorov used a new field of mathematics called measure theory to construct a set of rules that probabilities must follow. Some of these rules might be familiar to you:</p>
<ol type="1">
<li>The probability of some event is a number greater than or equal to zero, and less than or equal to one.</li>
<li>The probability of an event, and the probability of not that event, must sum to one.</li>
<li>The probability of event A happening or event B happening is the sum of each of their independent probabilities minus their joint probability.</li>
</ol>
<p>Along with a few others, these rules formed the basis upon which to calculate any complex probability problem. Thus, the axiomatic definition of probability is something along the lines of “probability is a function that maps events to a real number, obeying the axioms of probability.”<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>That is all well and good, but the problem with the axiomatic definition is that it does not tell us anything about where probabilities come from, or what they mean in the real world. The axioms apply equally well to frequentist probability as well as subjective probability—which, as we saw, are philosophical antagonists.</p>
</section><section id="probability-as-logic" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="probability-as-logic">Probability as Logic</h3>
<p>There are many ways to answer the question “what is probability?” But each of the definitions we’ve encountered so far have come with big drawbacks which limit their usefulness in social science research. Luckily there is one last way to conceptualize probability coming to our rescue. What if I told you that this final definition A) has the empirical rigor of frequentist and classical approaches, B) easily incorporates subjective background knowledge in a principled manner, and C) uses all the convenient mathematical rules from Kolmogorov’s axioms!</p>
<p>The trick is to think about probability as an extension of logic. Traditional Aristotelian logic deals with propositions that are only true or false such as “if P then Q; P; therefore Q”. Or the contrapositive “if P then Q; not Q; therefore not P”. But we never work with 100% true or 100% false propositions in social science. Rather, we would like to make inductive statements like “if P then Q <em>is more plausible</em>; Q; therefore P <em>is more plausible</em>”. Here is a more tangible example. Say we have a theory that being a democracy makes countries less likely to start wars. We then go out and collect data on countries and find a negative association between democracy and wars started. Therefore we conclude that our theory has been made more plausible.</p>
<p>Viewing probability as an extension of logic comes to us primarily from Edwin T. Jaynes and his 2003 book <em>Probability Theory: the Logic of Science</em>. For Jaynes, probability is the language we use to reason about propositions when we are operating in a state of incomplete information. Contrary to what proponents of frequentism might claim, probabilities do not exist as tangible properties about some set of events. Jaynes calls this the “mind-projection fallacy”—confusing the nature of things as they are with the practice of gathering knowledge about something. In other words, mistaking ontology for epistemology.</p>
<p>One of Jaynes’s major contributions was the insistence that all probability statements are provisional on background information or assumptions. So we should really write all probabilities as</p>
<p><span class="math display">
\text{P}(A|X)
</span></p>
<p>Where the “|” symbol means “conditional on”, and <span class="math inline">X</span> stands for all the background knowledge we have relevant to the plausibility of proposition <span class="math inline">A</span>. The beauty of this approach is that it allows us to combine the subjective and the objective when trying to solve a scientific problem. Reasonable people might disagree about what assumptions go into any particular <span class="math inline">X</span>. But given some fixed <span class="math inline">X</span>, everyone’s conclusions will necessarily follow the logic of probability and end up in the same place.</p>
<p>The Jaynesian view of probability is also Bayesian in the sense that we will be using all parts of Bayes’ theorem to conduct scientific inference. The <em>prior probability of the hypothesis</em> is a crucial ingredient in coming up with an accurate <em>posterior probability of the hypothesis</em>—the probability we assign to the hypothesis after collecting data on it.</p>


<!-- -->

</section></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr>
<ol>
<li id="fn1"><p>Clayton (2021), pg. 8<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>https://iqss.github.io/prefresher/<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./34_intergrals.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Integral Calculus</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./42_logic_probability.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">The Logic of Probability</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Measuring Uncertainty {#uncertainty}</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>Social science research is hard---in part because human behavior is so unpredictable. Unlike fields such as physics and chemistry, our theories are rarely deterministic. Rather, we express our arguments in terms of probabilities or likelihoods. "Democratic countries *tend* to have lower levels of corruption". Or "the belief that their vote matters increases the *likelihood* that an individual will turn out on election day". Theories like these highlight why statistics and probability theory are such useful tools for social scientists. The goal of statistics is to use data to measure our uncertainty about a proposition, or hypothesis. And probability is the language we use to describe this uncertainty.</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>In this section of the book we will cover the theory of probability that underlies commonly used statistical methods you will encounter in subsequent courses. </span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>The material for this section is drawn from the following sources:</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Jaynes E. T and G. Larry Bretthorst. 2003. *Probability Theory: The Logic of Science*. Cambridge UK: Cambridge University Press.</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Clayton Aubrey. 2021. *Bernoulli's Fallacy : Statistical Illogic and the Crisis of Modern Science*. New York: Columbia University Press.</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Kurt Will. 2019. *Bayesian Statistics the Fun Way*. San Francisco: No Starch Press.</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span><span class="co">[</span><span class="ot">Aubrey Clayton. 2015. YouTube Lectures on Probability Theory: The Logic of Science</span><span class="co">](https://www.youtube.com/watch?v=rfKS69cIwHc&amp;list=PL9v9IXDsJkktefQzX39wC2YG07vw7DsQ_&amp;ab_channel=AubreyClayton)</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="fu">## What is Probability?</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>What does the phrase "the probability of a coin flip coming up heads is one half" mean? You may be surprised to learn that there are many different answers to this question. In this section we will discuss the various definitions of probability philosophers and statisticians have come up with. As we will see, many commonly used definitions have appealing qualities. But each ultimately falls short in important ways. The exception being viewing probability as an extension of logic. This idea, developed by Edwin T. Jaynes in his book *Probability Theory: the Logic of Science* (2003), is rarely found in introductions to probability, but as we will see, is the most coherent way to understand and reason about probabilistic statements. Before diving into probability-as-logic, let's take a look at the strengths and weaknesses of definitions with which you may be more familiar. </span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The Classical Definition {.unnumbered}</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>The mathematics of probability originated in the study of gambling games. People wanted to know the answer to questions like "if I roll two six-sided dice, what is the probability that their values sum to seven"? This would have implications for how much to wager on a given bet. In the 15th and 16th centuries, mathematicians came up with the following solution. </span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>\text{P(event A)} = \frac{\text{<span class="sc">\#</span> of ways event A can occur}}{\text{<span class="sc">\#</span> of all possible outcomes}}</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>This classical definition of probability is still used in many introductory textbooks today---probably because it can be intuitively applied to most common examples used in teaching (e.g. flipping coins, rolling dice, dealing cards from a deck, or drawing balls from an urn). What is the probability that a coin will come up heads when we flip it? Well, there are two possible outcomes when we flip a coin: either it lands heads or tails, and there is one way for it to land heads. So our calculation would be:</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>\frac{\text{heads}}{\text{<span class="co">[</span><span class="ot">heads, tails</span><span class="co">]</span>}} = \frac{1}{1 + 1} = \frac{1}{2}</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>The classical definition also makes it clear to us that events which can happen in more ways are more likely. When we roll two six-sided dice, there are $6*6 = 36$ possible outcomes. Getting a sum equal to *seven* can happen in five ways [(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)] whereas getting a sum equal to *two* can only happen in one way <span class="co">[</span><span class="ot">(1, 1)</span><span class="co">]</span>. Therefore, according to the classical definition of probability, rolling two dice and getting a sum of seven is five times more likely than getting a sum of two.</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>The problem with the classical definition of probability is that is does a poor job describing uncertainty outside of simple games of chance. One of the main challenges comes from having to enumerate the "number of all possible outcomes" in the denominator. We can figure this out easily enough for flipping coins and rolling dice because there is an apparent symmetry in the outcome-space, but in the social sciences we rarely study such clear-cut systems. Take the statement "after receiving the encouragement treatment, an individual will turn out to vote with 70% probability." There are only two outcomes: either the individual votes or they do not vote. So should the probability of voting always be 50/50? There is no apparent way to cut up the outcome space such that the classical probability formula gives us an answer like $7/10$. </span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>Another problem with the classical definition is that it requires each event in the denominator to be equi-probable. This is easy to assume when the events have a symmetric quality to them: flipping a coin, drawing cards from a deck, rolling dice etc. But what if we wanted to know the probability of rolling a specific value from a mis-shapen die? Furthmore, the fact that the denominator term requires events to be "equi-probable" means that this definition of probability is circular!</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The Frequentist Definition {.unnumbered}</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>Beginning in the 19th century, a new definition of probability known as frequentism began to be widely used. According to frequentism, the probability of an event is defined by its long-run numerical frequency. We know that the probability of a coin toss coming up heads is $1/2$ because, if we were to toss a coin enough times, in the limit 50% of the tosses would come up heads. The mathematical basis for frequentism comes from Swiss mathematician Jacob Bernoulli's Law of Large Numbers. </span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; "Take a large enough sample that you can be morally certain, to whatever degree that means to you, the ratio in the sample is within your desired tolerance of the </span><span class="co">[</span><span class="ot">true ratio</span><span class="co">]</span><span class="at">."^</span><span class="co">[</span><span class="ot">Clayton (2021), pg. 8</span><span class="co">]</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>Frequentism has an appealing objective quality to it because probabilities can supposedly be arrived at via empirical frequencies. But how can we ever truly verify that these frequencies match reality? In social science research we only get one shot of our data if we run a survey or a field experiment. And what about data from one-time events such as elections or wars? "The probability that the Democrats retain control of Congress in 2022 is 0.4." According to frequentists, we would have to imagine re-running the 2022 mid-term election an infinite number of times in order to make sense of this statement. But if we re-ran the election under all the same circumstances, wouldn't the outcome remain the same? Like the classical definition of probability, frequentism "works" for simple examples but leads to paradoxes when applied to modern scientific problems. Despite this, most statistical methods used today are based on the frequentist paradigm.</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- As an aside, it is also worth noting that frequentist methods were developed by a set of individuals---chiefly Francis Galton, Karl Pearson, and Ronald Fisher---whose scientific interests were directed towards supporting eugenicist projects around the turn of the 20th century. These men were attracted to the apparent objectivity of frequentism (as opposed to subjective probability which we will turn to next) because they hoped to prove that human groups had innate differences. These "objective" differences somehow always showed how white Europeans were superior to colonized indigenous populations. For more on the dark origins of frequentist methods see [this article](https://nautil.us/how-eugenics-shaped-statistics-9365/). --&gt;</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The Subjective Definition {.unnumbered}</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>Standing in radical opposition to the frequentist and classical definitions is a view that probability statements are no more than someone's subjective degree of belief about a proposition. The nice thing about the subjective definition is that it is often how we talk about probability in our day to day lives. The statement "the probability that the Democrats retain control of Congress in 2022 is 0.4" is a perfectly valid way to express how certain we are about some outcome. We can also apply this to things like coin flips: "I don't have any reason to think either heads or tails is more likely, so the probability of heads is 50/50."</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>Unfortunately, the subjective view of probability does not provide any way to compare competing probability statements. If I say the probability of flipping a coin and getting heads is $1/2$, but you say it is $1/8$, who can we say is more correct without some sort of empirical verification? This problem also exists for one-time events. Let's say I claim that the Democrats have a 20% chance of winning the election, and you claim they have an 80% chance of winning the election. The election happens and the Democrats win---who made the better prediction? </span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>Subjective probability is often associated with Bayesian theories of probability. Bayes' theorem, however, is simply a well-established mathematical relation. And it is used in every type of probability framework. If you are not familiar with Bayes' theorem, the basic version looks like</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>\text{P}(A|B) = \frac{\text{P}(A) \ \text{P}(B|A)}{\text{P}(B)}</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>The most controversial part of this formula is $\text{P}(A)$, also known as the *prior*. Frequentist statistical methods ignore this part of Bayes' theorem when conducting inference, in part because of its association with a "subjective" way of thinking. But by doing so they implicitly assume all values of $\text{P}(A)$ are equally likely---which is itself a subjective choice! We will cover Bayes' theorem extensively in our chapter on conditional probability.</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Lastly, the subjective definition of probability does not give us any nice tools for working with probabilities mathematically. The following basic axiom of probability --&gt;</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- $$ --&gt;</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \text{P(event A)} + \text{P(not event A)} = 1 --&gt;</span></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- $$ --&gt;</span></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- says that probabilities must be *complementary*. Either event A happens, or it doesn't happen, with certainty. And by "certainty" we mean probability equals $1$. But if probability is purely subjective, there is no reason we would have to force probabilities to sum to $1$ in this way.  --&gt;</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The Axiomatic Definition {.unnumbered}</span></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>In 1933 Russian mathematician Andrei Kolmogorov worked out a set of axioms governing how probability worked. Kolmogorov used a new field of mathematics called measure theory to construct a set of rules that probabilities must follow. Some of these rules might be familiar to you:</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a><span class="ss">  1. </span>The probability of some event is a number greater than or equal to zero, and less than or equal to one.</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a><span class="ss">  2. </span>The probability of an event, and the probability of not that event, must sum to one.</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a><span class="ss">  3. </span>The probability of event A happening or event B happening is the sum of each of their independent probabilities minus their joint probability.</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>Along with a few others, these rules formed the basis upon which to calculate any complex probability problem. Thus, the axiomatic definition of probability is something along the lines of "probability is a function that maps events to a real number, obeying the axioms of probability."^<span class="co">[</span><span class="ot">https://iqss.github.io/prefresher/</span><span class="co">]</span></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>That is all well and good, but the problem with the axiomatic definition is that it does not tell us anything about where probabilities come from, or what they mean in the real world. The axioms apply equally well to frequentist probability as well as subjective probability---which, as we saw, are philosophical antagonists.</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a><span class="fu">### Probability as Logic {.unnumbered}</span></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>There are many ways to answer the question "what is probability?" But each of the definitions we've encountered so far have come with big drawbacks which limit their usefulness in social science research. Luckily there is one last way to conceptualize probability coming to our rescue. What if I told you that this final definition A) has the empirical rigor of frequentist and classical approaches, B) easily incorporates subjective background knowledge in a principled manner, and C) uses all the convenient mathematical rules from Kolmogorov's axioms!</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>The trick is to think about probability as an extension of logic. Traditional Aristotelian logic deals with propositions that are only true or false such as "if P then Q; P; therefore Q". Or the contrapositive "if P then Q; not Q; therefore not P". But we never work with 100% true or 100% false propositions in social science. Rather, we would like to make inductive statements like "if P then Q *is more plausible*; Q; therefore P *is more plausible*". Here is a more tangible example. Say we have a theory that being a democracy makes countries less likely to start wars. We then go out and collect data on countries and find a negative association between democracy and wars started. Therefore we conclude that our theory has been made more plausible. </span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>Viewing probability as an extension of logic comes to us primarily from Edwin T. Jaynes and his 2003 book *Probability Theory: the Logic of Science*. For Jaynes, probability is the language we use to reason about propositions when we are operating in a state of incomplete information. Contrary to what proponents of frequentism might claim, probabilities do not exist as tangible properties about some set of events. Jaynes calls this the "mind-projection fallacy"---confusing the nature of things as they are with the practice of gathering knowledge about something. In other words, mistaking ontology for epistemology.  </span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>One of Jaynes's major contributions was the insistence that all probability statements are provisional on background information or assumptions. So we should really write all probabilities as</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>\text{P}(A|X)</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>Where the "|" symbol means "conditional on", and $X$ stands for all the background knowledge we have relevant to the plausibility of proposition $A$. The beauty of this approach is that it allows us to combine the subjective and the objective when trying to solve a scientific problem. Reasonable people might disagree about what assumptions go into any particular $X$. But given some fixed $X$, everyone's conclusions will necessarily follow the logic of probability and end up in the same place.</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>The Jaynesian view of probability is also Bayesian in the sense that we will be using all parts of Bayes' theorem to conduct scientific inference. The *prior probability of the hypothesis* is a crucial ingredient in coming up with an accurate *posterior probability of the hypothesis*---the probability we assign to the hypothesis after collecting data on it. </span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>