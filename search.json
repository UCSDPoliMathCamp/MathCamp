[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UCSD Political Science Math Camp",
    "section": "",
    "text": "This booklet is adapted from the Harvard Gov Prefresher booklet, maintained by Shiro Kuriwaki. It will serve as as the text for the UCSD Math Camp, taught by Bertrand Wilden and Keng-Chi Chang, and we have reordered and adapted it to fit the structure of our course. For information about the role of this math camp as an introduction to graduate school, you may be interested in “The Math Prefresher and The Collective Future of Political Science Graduate Training”, in PS: Political Science & Politics, by Gary King, Shiro Kuriwaki, and Yon Soo Park.\n\nFor information about the authors of the Harvard Gov Prefresher booklet, see here.\nWe have also updated it to include some material from previous instructors of the class at UCSD, including Rachel Schoner (2021), Luke Sanford (2019-2020), Kathryn Baragwanath (2019-2020), Brandon Merrell (2018), Inbok Rhee (2018).\n\nWe transitioned the booklet into a bookdown github repository in 2021. As we update this version, we appreciate any bug reports or fixes appreciated.\nAll changes should be made in the .Rmd files in the project root. To contribute a change, please make a pull request and set the repository maintainer as the reviewer."
  },
  {
    "objectID": "00_warmup.html",
    "href": "00_warmup.html",
    "title": "Warmup Questions",
    "section": "",
    "text": "Before our first meeting, please try solving these questions. They are a sample of the very beginning of each math section. We have provided links to the parts of the book you can read if the concepts are new to you.\nThe goal of this “pre”-math camp assignment is not to intimidate you but to set common expectations so you can make the most out of the actual Math Camp. Even if you do not understand some or all of these questions after skimming through the linked sections, your effort will pay off, and you will be better prepared for the math camp. We are also open to adjusting these expectations based on feedback (this class is for you), so please do not hesitate to write to the instructors for feedback."
  },
  {
    "objectID": "00_warmup.html#operations",
    "href": "00_warmup.html#operations",
    "title": "Warmup Questions",
    "section": "Operations",
    "text": "Operations\nSummation\nSimplify the following\n\n\\sum\\limits_{i = 1}^3 i\n\\sum\\limits_{k = 1}^3(3k + 2)\n\\sum\\limits_{i= 1}^4 (3k + i + 2)\nProducts\n\n\\prod\\limits_{i= 1}^3 i\n\\prod\\limits_{k=1}^3(3k + 2)\nLogs and exponents\nSimplify the following:\n\n4^2\n4^2 2^3\n\\log_{10}100\n\\log_{2}4\n\n\\log e, where \\log is the natural log (also written as \\ln) – a log with basee, and e is Euler’s constant\n\ne^a, e^b, e^c, where a, b, c are each constants\n\\log 0\ne^0\ne^1\n\\log e^2"
  },
  {
    "objectID": "00_warmup.html#limits",
    "href": "00_warmup.html#limits",
    "title": "Warmup Questions",
    "section": "Limits",
    "text": "Limits\nFind the limit of the following.\n\n\\lim\\limits_{x \\to 2} (x - 1)\n\\lim\\limits_{x \\to 2} \\frac{(x - 2) (x - 1)}{(x - 2)}\n\\lim\\limits_{x \\to 2}\\frac{x^2 - 3x + 2}{x- 2}"
  },
  {
    "objectID": "00_warmup.html#linear-algebra",
    "href": "00_warmup.html#linear-algebra",
    "title": "Warmup Questions",
    "section": "Linear Algebra",
    "text": "Linear Algebra\nVectors\nDefine the vectors\nu = \\begin{pmatrix} 1 \\\\2 \\\\3 \\end{pmatrix},\nv = \\begin{pmatrix} 4\\\\5\\\\6 \\end{pmatrix},\nand the scalar c = 2.\nCalculate the following:\n\nu + v\ncv\nu \\cdot v\n\nAre the following sets of vectors linearly independent?\n\nu = \\begin{pmatrix} 1\\\\ 2\\end{pmatrix}, v = \\begin{pmatrix} 2\\\\4\\end{pmatrix}\nu = \\begin{pmatrix} 1\\\\ 2\\\\ 5 \\end{pmatrix}, v = \\begin{pmatrix} 3\\\\ 7\\\\ 9 \\end{pmatrix}\na = \\begin{pmatrix} 2\\\\ -1\\\\ 1 \\end{pmatrix}, b = \\begin{pmatrix} 3\\\\ -4\\\\ -2 \\end{pmatrix}, c = \\begin{pmatrix} 5\\\\ -10\\\\ -8 \\end{pmatrix} (this requires some guesswork)\nMatrices\nGiven that\n\n\\mathbf{A}=\\begin{bmatrix}\n            7 & 5 & 1 \\\\\n            11 & 9 & 3 \\\\\n            2 & 14 & 21 \\\\\n            4 & 1 & 5\n        \\end{bmatrix}\n\nWhat is the dimensionality of matrix \\mathbf{A}?\nWhat is the element a_{23} of \\mathbf{A}?\nGiven that\n\n\\mathbf{B}\n=\n\\begin{bmatrix}\n    1 & 2 & 8 \\\\\n    3 & 9 & 11 \\\\\n    4 & 7 & 5 \\\\\n    5 & 1 & 9\n\\end{bmatrix}\n\nWhat is \\mathbf{A} + \\mathbf{B}?\nGiven that\n\n\\mathbf{C}=\\begin{bmatrix}\n            1 & 2 & 8 \\\\\n            3 & 9 & 11 \\\\\n            4 & 7 & 5\n        \\end{bmatrix}\n\nWhat is \\mathbf{A} + \\mathbf{C}?\nGiven that\n\nc = 2\n\nWhat is c {\\bf A}?"
  },
  {
    "objectID": "00_warmup.html#calculus",
    "href": "00_warmup.html#calculus",
    "title": "Warmup Questions",
    "section": "Calculus",
    "text": "Calculus\nFor each of the following functions f(x), find the derivative f'(x) or \\frac{d}{dx}f(x)\n\nf(x)=c\nf(x)=x\nf(x)=x^2\nf(x)=x^3\nf(x)=3x^2+2x^{1/3}\nf(x)=(x^3)(2x^4)"
  },
  {
    "objectID": "00_warmup.html#optimization",
    "href": "00_warmup.html#optimization",
    "title": "Warmup Questions",
    "section": "Optimization",
    "text": "Optimization\nFor each of the followng functions f(x), does a maximum and minimum exist in the domain x \\in \\mathbf{R}? If so, for what are those values and for which values of x?\n\nf(x) = x\nf(x) = x^2\nf(x) = -(x - 2)^2\n\nIf you are stuck, please try sketching out a picture of each of the functions."
  },
  {
    "objectID": "00_warmup.html#probability",
    "href": "00_warmup.html#probability",
    "title": "Warmup Questions",
    "section": "Probability",
    "text": "Probability\n\nIf there are 12 cards, numbered 1 to 12, and 4 cards are chosen, how many distinct possible choices are there? (unordered, without replacement)\nLet A = \\{1,3,5,7,8\\} and B = \\{2,4,7,8,12,13\\}. What is A \\cup B? What is A \\cap B? If A is a subset of the Sample Space S = \\{1,2,3,4,5,6,7,8,9,10\\}, what is the complement A^C?\nIf we roll two fair dice, what is the probability that their sum would be 11?\nIf we roll two fair dice, what is the probability that their sum would be 12?"
  },
  {
    "objectID": "01_prerequisites.html",
    "href": "01_prerequisites.html",
    "title": "Prerequisites",
    "section": "",
    "text": "The syllabus lists the assumed knowledge for Math Camp (Chapters 1-4 of Moore and Siegel). Below, you will find additional review material, examples, and sample exercises."
  },
  {
    "objectID": "01_prerequisites.html#operators",
    "href": "01_prerequisites.html#operators",
    "title": "Prerequisites",
    "section": "Operators",
    "text": "Operators\nAddition (+), Subtraction (-), multiplication and division are basic operations of arithmetic – combining numbers. In statistics and calculus, we want to add a sequence of numbers that can be expressed as a pattern without needing to write down all its components. For example, how would we express the sum of all numbers from 1 to 100 without writing a hundred numbers?\nFor this we use the summation operator \\sum and the product operator \\prod.\nSummation\n\n\\sum\\limits_{i=1}^{100} x_i = x_1+x_2+x_3+\\cdots+x_{100}\n\nThe bottom of the \\sum symbol indicates an index (here, i), and its start value 1. At the top is where the index ends. The notion of “addition” is part of the \\sum symbol. The content to the right of the summation is the meat of what we add. While you can pick your favorite index, start, and end values, the content must also have the index.\n\n\nCorollary 1 \n\\sum\\limits_{i=1}^n c x_i = c \\sum\\limits_{i=1}^n x_i\n\\sum\\limits_{i=1}^n (x_i + y_i) = \\sum\\limits_{i=1}^n x_i + \\sum\\limits_{i=1}^n y_i\n\\sum\\limits_{i=1}^n c = n c\n\n\nProduct\n\n\\prod\\limits_{i=1}^n x_i = x_1 x_2 x_3 \\cdots x_n\n\n\n\nCorollary 2 \n\\prod\\limits_{i=1}^n c x_i = c^n \\prod\\limits_{i=1}^n x_i\n\\prod\\limits_{i=k}^n c x_i = c^{n-k+1} \\prod\\limits_{i=k}^n x_i\n\n\\prod\\limits_{i=1}^n (x_i + y_i) = a total mess\n\\prod\\limits_{i=1}^n c = c^n\n\n\nFactorials\n\nDefinition 1 (Factorials) \nx! = x\\cdot (x-1) \\cdot (x-2) \\cdots (1)\n\n\nModulo\nTells you the remainder when you divide the first number by the second.\n\n17 \\mod 3 = 2\n100 \\ \\% \\ 30 = 10\n\n\nExercise 1 (Operators) Let x_1 = 4, x_2 = 3, x_3 = 7, x_4 = 11, x_5 = 2. Find the following:\n\n\\sum\\limits_{i=1}^{5} i\n\\prod\\limits_{i=1}^{5} i\n14 \\mod 4\n4!\n\\sum\\limits_{i=1}^{3} (7)x_i\n\\sum\\limits_{i=1}^{5} 2\n\\prod\\limits_{i=3}^{5} (2)x_i"
  },
  {
    "objectID": "01_prerequisites.html#functions",
    "href": "01_prerequisites.html#functions",
    "title": "Prerequisites",
    "section": "Functions",
    "text": "Functions\nA function is a mapping that relates members of one set to members of another set.\nFor instance, if you have two sets: set A and set B, a function f from A to B maps every value a in set A such that f(a) \\in B.\n\nThe set A is called the domain of function f\n\nThe set B is called the range of function f\n\n\nFunctions can be many-to-one, where many values from set A produce a single output in set B, or they can be one-to-one, where each value in set A corresponds to a single value in set B.\nA function by definition has a single function value for each element of its domain. This means, there cannot be “one-to-many” mapping.\nDimensionality\n\\mathbb{R}^1 is the set of all real numbers extending from -\\infty to +\\infty — i.e., the real number line. \\mathbb{R}^n is an n-dimensional space, where each of the n axes extends from -\\infty to +\\infty.\n\n\n\\mathbb{R}^1 is a one dimensional line.\n\n\\mathbb{R}^2 is a two dimensional plane.\n\n\\mathbb{R}^3 is a three dimensional space.\n\nPoints in \\mathbb{R}^n are ordered n-tuples (just means an combination of n elements where order matters), where each element of the n-tuple represents the coordinate along that dimension.\nFor example:\n\n\n\\mathbb{R}^1: (3)\n\n\n\\mathbb{R}^2: (-15, 5)\n\n\n\\mathbb{R}^3: (86, 4, 0)\n\n\n\n\n\n\n\n\nNotation (Functions)\n\n\n\nFunction of one variable:\n\nf:\\mathbb{R}^1\\to\\mathbb{R}^1\n\n\nExample: f(x)=x+1\n\nFor each x in \\mathbb{R}^1, f(x) assigns the number x+1\n\n\n\n\nFunction of two variables:\n\nf: \\mathbb{R}^2\\to\\mathbb{R}^1\n\n\nExample: f(x,y)=x^2+y^2\n\nFor each ordered pair (x,y) in \\mathbb{R}^2, f(x,y) assigns the number x^2+y^2\n\n\n\n\n\n\nWe often use variable x as input and another y as output, e.g. y=x+1\n\nExercise 2 (Functions) For each of the following, state whether they are one-to-one or many-to-one functions.\n\nFor x \\in [0,\\infty], f : x \\rightarrow x^2 (this could also be written as f(x) = x^2).\nFor x \\in [-\\infty, \\infty], f: x \\rightarrow x^2.\nFor x \\in [-3, \\infty], f: x \\rightarrow x^2.\nFor x \\in [0, \\infty], f: x \\rightarrow \\sqrt{x}\n\n\n\nSome functions are defined only on proper subsets of \\mathbb{R}^n.\n\n\nDomain: the set of numbers in X at which f(x) is defined.\n\nRange: elements of Y assigned by f(x) to elements of X, or we can use the notation f(X) to denote the range, where f(X)=\\{ y : y=f(x), x\\in X\\}\n\nTypes of Functions\nMonomials: f(x)=a x^k\na is the coefficient. k is the degree.\nExamples: y=x^2, y=-\\frac{1}{2}x^3\nPolynomials: sum of monomials.\nExamples: y=-\\frac{1}{2}x^3+x^2, y=3x+5\nThe degree of a polynomial is the highest degree of its monomial terms. Also, it’s often a good idea to write polynomials with terms in decreasing degree.\nExponential Functions: Example: y=2^x"
  },
  {
    "objectID": "01_prerequisites.html#logexponents",
    "href": "01_prerequisites.html#logexponents",
    "title": "Prerequisites",
    "section": "Logs and Exponents",
    "text": "Logs and Exponents\nRelationship of logarithmic and exponential functions:\n\ny=\\log_a(x) \\iff a^y=x\n\nThe log function can be thought of as an inverse for exponential functions. a is referred to as the “base” of the logarithm.\nCommon Bases: The two most common logarithms are base 10 and base e.\n\nBase 10: \\quad y=\\log_{10}(x) \\iff 10^y=x. The base 10 logarithm is often simply written as “\\log(x)” with no base denoted.\nBase e: \\quad y=\\log_e(x) \\iff e^y=x. The base e logarithm is referred to as the “natural” logarithm and is written as ``\\ln(x)“.\n\nProperties of exponential functions:\n\na^x a^y = a^{x+y}\na^{-x} = 1/a^x\na^x/a^y = a^{x-y}\n(a^x)^y = a^{x y}\na^0 = 1\n\nProperties of logarithmic functions (any base):\nGenerally, when statisticians or social scientists write \\log(x) they mean \\log_e(x). In other words: \\log_e(x) \\equiv \\ln(x) \\equiv \\log(x)\n\n\\log_a(a^x)=x\n\nand\n\na^{\\log_a(x)}=x\n\n\n\\log(x y)=\\log(x)+\\log(y)\n\\log(x^y)=y\\log(x)\n\\log(1/x)=\\log(x^{-1})=-\\log(x)\n\\log(x/y)=\\log(x\\cdot y^{-1})=\\log(x)+\\log(y^{-1})=\\log(x)-\\log(y)\n\\log(1)=\\log(e^0)=0\n\nChange of Base Formula: Use the change of base formula to switch bases as necessary:\n\n\\log_b(x) = \\frac{\\log_a(x)}{\\log_a(b)}\n\nExample:\n\n\\log_{10}(x) = \\frac{\\ln(x)}{\\ln(10)}\n\nYou can use logs to go between sum and product notation. This will be particularly important when you’re learning maximum likelihood estimation.\n\n\\begin{aligned}\n            \\log \\bigg(\\prod\\limits_{i=1}^n x_i \\bigg)\n            &= \\log(x_1 \\cdot x_2 \\cdot x_3 \\cdots \\cdot x_n) \\\\\n            &= \\log(x_1) + \\log(x_2) + \\log(x_3) + \\cdots + \\log(x_n) \\\\\n            &= \\sum\\limits_{i=1}^n \\log (x_i)\n\\end{aligned}\n\nTherefore, you can see that the log of a product is equal to the sum of the logs. We can write this more generally by adding in a constant, c:\n      \n\\begin{aligned}\n            \\log \\bigg(\\prod\\limits_{i=1}^n c x_i\\bigg)\n            &= \\log(cx_1 \\cdot cx_2 \\cdots cx_n) \\\\\n            &= \\log(c^n \\cdot x_1 \\cdot x_2 \\cdots x_n) \\\\\n            &= \\log(c^n) + \\log(x_1) + \\log(x_2) + \\cdots + \\log(x_n) \\\\\n            &= n \\log(c) +  \\sum\\limits_{i=1}^n \\log (x_i)\n\\end{aligned}   \n\n\nExercise 3 (Log) Evaluate each of the following logarithms\n\n\\log_4(16)\n\\log_2(16)\n\\log_\\frac{3}{2}(\\frac{27}{8})\n\nSimplify the following logarithm. By “simplify”, we actually really mean - use as many of the logarithmic properties as you can.\n\n\\log_4(x^3y^5)\n\\log(\\frac{x^9y^5}{z^3})\n\\ln{\\sqrt{xy}}"
  },
  {
    "objectID": "01_prerequisites.html#graphing-functions",
    "href": "01_prerequisites.html#graphing-functions",
    "title": "Prerequisites",
    "section": "Graphing Functions",
    "text": "Graphing Functions\nWhat can a graph tell you about a function?\n\nIs the function increasing or decreasing? Over what part of the domain?\nHow ``fast” does it increase or decrease?\nAre there global or local maxima and minima? Where?\nAre there inflection points?\nIs the function continuous?\nIs the function differentiable?\nDoes the function tend to some limit?\nOther questions related to the substance of the problem at hand."
  },
  {
    "objectID": "01_prerequisites.html#solving-for-variables",
    "href": "01_prerequisites.html#solving-for-variables",
    "title": "Prerequisites",
    "section": "Solving for Variables",
    "text": "Solving for Variables\nSometimes we’re given a function y=f(x) and we want to find how x varies as a function of y. Use algebra to move x to the left hand side (LHS) of the equation and so that the right hand side (RHS) is only a function of y.\n\nExample 1 (Solving for Variables) Solve for x:\n\ny=3x+2\ny=e^x\n\n\nSolving for variables is especially important when we want to find the roots of an equation: those values of variables that cause an equation to equal zero. Especially important in finding equilibria and in doing maximum likelihood estimation.\nProcedure: Given y=f(x), set f(x)=0. Solve for x.\nMultiple Roots:\n\nf(x)=x^2 - 9 \\quad\\Longrightarrow\\quad 0=x^2 - 9 \\quad\\Longrightarrow\\quad 9=x^2 \\quad\\Longrightarrow\\quad \\pm \\sqrt{9}=\\sqrt{x^2} \\quad\\Longrightarrow\\quad \\pm 3=x\n\nQuadratic Formula: For quadratic equations ax^2+bx+c=0, use the quadratic formula:\n\nx=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}\n\n\nExercise 4 (Solving for Variables) Solve for x:\n\nf(x)=3x+2 = 0\nf(x)=x^2+3x-4=0\nf(x)=e^{-x}-10 = 0"
  },
  {
    "objectID": "01_prerequisites.html#answers-to-examples-and-exercises",
    "href": "01_prerequisites.html#answers-to-examples-and-exercises",
    "title": "Prerequisites",
    "section": "Answers to Examples and Exercises",
    "text": "Answers to Examples and Exercises\nAnswer to Exercise 1:\n\n1 + 2 + 3 + 4 + 5 = 15\n1 * 2 * 3 * 4 * 5 = 120\n2\n4 * 3 * 2 * 1 = 24\n7(4 + 3 + 7) = 98\n2 + 2 + 2 + 2 + 2 = 10\n2^3(7)(11)(2) = 1232\n\nAnswer to Exercise 2:\n\none-to-one\nmany-to-one\nmany-to-one\none-to-one\n\nAnswer to Exercise 3:\n\n2\n4\n3\n3\\log_4(x) + 5\\log_4(y)\n9\\log(x) + 5\\log(y) - 3\\log(z)\n\\frac{1}{2}(\\ln{x} + \\ln{y})\n\nAnswer to Example 1:\n\ny=3x+2 \\quad\\Longrightarrow\\quad -3x=2-y \\quad\\Longrightarrow\\quad 3x=y-2 \\quad\\Longrightarrow\\quad x=\\frac{1}{3}(y-2)\nx = \\ln{y}\n\nAnswer to Exercise 4:\n\n\\frac{-2}{3}\nx = {1, -4}\nx = - \\ln10"
  },
  {
    "objectID": "11_orientation.html",
    "href": "11_orientation.html",
    "title": "\n1  RStudio and Reading in Data\n",
    "section": "",
    "text": "Welcome to Math Camp! The first day we will get to know each other and get acquainted with R. We will be working with R throughout this course, and today is the beginning, where you will learn about this program. It can be overwhelming to learn any programming language, but we will spend a lot of time and practice with it. Please work through the following tutorials and exercises. There is a lot of information here, and it will take time to understand everything. In fact, programming is a continual learning experience. Please come to class with areas of confusion and questions; we will walk through more examples together.\nYou should download the latest versions of R and R Studio.\nR Studio Cloud has great resources we will be using throughout this course. Please complete the following primer tutorial for the first session: The Basics: Programming Basics"
  },
  {
    "objectID": "11_orientation.html#orienting",
    "href": "11_orientation.html#orienting",
    "title": "\n1  RStudio and Reading in Data\n",
    "section": "\n1.1 Orienting",
    "text": "1.1 Orienting\nGetting to know RStudio\nRStudio is an Integrated Development Environment (IDE) for the programming language R. An IDE is a piece of software which allows you to more easily interface with programming languages. You can program in other languages (such as Python or SQL) using RStudio, and you can use other IDEs to program with R, but typically R users use RStudio—and vice versa. This is because RStudio was specifically created to facilitate R programming and comes with a ton of helpful features.\nThere are four main sections, or “panes”, in RStudio.\n\n\nBottom Left. This is the Console. It is the place that executes your R code. You can type directly in your Console and it will display the output immediately below. If your code generates errors or warning messages they will be displayed here too. You should rarely type code directly into your Console, however, because it can be difficult to keep track of things line by line. Also, every time you restart RStudio your Console will refresh itself which means you will lose your previous work. This is not only a problem for keeping track of your own analyses, but it also makes it impossible for anyone else to replicate the steps you took while coding. The main reasons to write directly in the Console are testing small code snippets or for quick data exploration (for example using the summary() function on a variable).\nUseful tip: use the Up-Arrow on your keyboard to quickly re-run pieces of code which were previously sent to the Console.\n\n\nTop Left. If you go to “File > New File” you can open up a script file which will then appear in your Source window. Script files are text documents containing R code which then gets sent down to the Console when executed. Script files are great because we can save them onto our computer to rerun our work at any time in the future. Plus we can send these files to anyone else with R installed on their computer and they will be able to rerun our analysis too. This is a core concept in ensuring your research is reproducible.\nThere are two types of script files people typically use when programming in R: .R and .Rmd files. Most R users do the bulk of their programming in .R files. These are plain text files containing commands for R to execute. If you want to write something else in .R files (such as English sentences), you can do so by starting a line with a # character. These are called comments and are a great way to explain what your R code is supposed to be doing. Comments make it helpful for other people to understand your code, as well as for yourself if you revisit a project months or years later!\nThe other common type of file used to write R code is an .Rmd, or R Markdown, file. We will cover R Markdown in much greater detail in a later section, but these are the basics for now: R Markdown allows you to seamlessly combine R code with written text to create a wide variety of presentation-ready documents. You can write all your academic papers using RMarkdown, as well as any presentation slides or even your website. The book you’re reading right now was written in R Markdown! The strength of combining R code and written text comes from how easy it is to update your document when something in your analysis changes. New data? Simply plug it in to the top of your R Markdown document and every graph and table will be automatically updated once you compile a new document. No more copy and pasting figures into Word—a practice which is tedious and prone to create errors.\n\nTop Right. This is your Environment tab and it keeps track of which object you’ve created in R. Objects in R are things like data frames, vectors, and functions. Many people refer to R as an “object-oriented language”, by which they mean that most of your code either creates new objects or modifies existing objects. This aspect is probably the largest difference between R and statistical programs like Stata. Each object in R has a specific type which defines what you can and cannot do with it. For example, you cannot add objects that are character types together \"UC\" + \"SD\", but you can add two objects with numeric types 3 + 5. Getting to know the rules surrounding object types is one of the trickier aspects of learning R. But overall, this method of programming is generally intuitive.\n\nBottom Right. Three important tabs live here. When you execute the code which makes a plot in a .R file it will, appropriately enough, show up down in your Plot tab.\nThe Help tab is where you can read the documentation for a specific function in R. To find the right help file you can either enter the function name into the search bar, or you can type ?function_name in the Console. Use the Help tab often! This should be the first place you go when you encounter a problem with your R code.\nLastly, the Files tab is very handy for keeping track of the various R scripts and data you may be working with on a particular project. It acts like a replacement File Explorer (if using a PC) or Finder (if using a Mac) without you having to have multiple windows open on your computer.\n\n\nRStudio Projects\nSpeaking of files and file paths…\nThroughout your time in the Political Science PhD program you will likely load hundreds of data sets into R. The first step to loading data into R is locating the file holding on the data on your computer. This is done using a file path—a string of characters which points to the file.\n\n\"/Users/bertrandwilden/Documents/UCSD/amazing_paper/data/cool_data.csv\"\n\nFor example the string above tells us the location of the file “cool_data.csv” located in the folder “data” which is a sub-directory of “amazing_paper” and so on. Getting a hang of using file paths to locate files can be one of the most frustrating parts of using a programming language like R. Modern computer systems, such as your phone, have made file paths invisible to most users. So don’t worry if any of this is confusing to you in the beginning!\nThere are two aspects of file paths which make them particularly annoying/difficult to work with. First, a file path that correctly locates a file on one computer will not locate the same file in another computer. This is because everyone has their own unique folder structures on their computer. Computer-specific file paths make it difficult to share code with others or to collaborate on the same project. They also lead to headaches when revisiting old code on a new computer.\nThe second, but related, issue with file paths is that they differ between Mac and PC computers. Mac file paths use the forward slash “/” between folders whereas PCs use the back slash “\\”. Back slashes in R strings are not processed literally–instead they are considered “escape” characters and serve a different purpose. This means you have to manually change all your backslashes to forward slashes to locate files when using a PC. Or you can manually add an extra back slash in front of each folder.\n\n\"C:\\\\Users\\\\bertrandwilden\\\\Documents\\\\UCSD\\\\amazing_paper\\\\data\\\\cool_data.csv\"\n\nWhat a hassle!\nLuckily tools have been developed to solve all these file path annoyances. The first solution is to use RStudio Projects. When you create a new RStudio Project it adds a .Rproj file with your project name to a folder on your computer. The .Rproj file now serves as the top level directory for any R code or data files in folders below it. So instead of using the full path\n\n\"/Users/bertrandwilden/Documents/UCSD/amazing_paper/data/cool_data.csv\"\n\nto locate your data, you now only need to type\n\n\"data/cool_data.csv\"\n\nThis is sometimes called a relative path. Each RStudio Project is self-contained and easily portable to other machines. RStudio Projects also have the benefit of making it easy to switch between various projects–giving you a clean slate with which to work from every time.1\nAwesome—-so RStudio Projects helped us solve one of our file path issues by using relative paths, but what about the Mac vs PC problem with different slashes? That’s where the R package “here” shines. The here package allows you to simply put each folder name in quotes and stitches the full path together behind the scenes. The file path\n\n\"data/cool_data.csv\"\n\nbecomes\n\nhere(\"data\", \"cool_data.csv\")\n\nThis code will now point to the correct file on any computer.\n\nInstalling R Packages\nWhile it is technically feasible to use only the functions that come pre-installed with R (e.g. “base R”), thousands of open source packages have been written to provide extra functionality. The term “open source” means that the underlying code for these packages lives on online repositories, such as GitHub, and can be viewed publicly. While open source packages can be written by anyone (including you someday!), there is a special process packages must undergo in order to be hosted officially by CRAN. Packages that have passed this systematic review by CRAN can be installed on your computer using the following command:\n\ninstall.packages(\"package_name_here\")\n\nIt is good practice to only use the install.packages command in your Console, rather than in a .R or .Rmd script file. This is because you only need to install a particular R package once and then you can then use it forever. Putting install.packages in your script file will make R attempt to download the package each time your code is run.\nAfter using the install.packages command, you then need to use the following command to access the package’s functions:\n\nlibrary(package_name_here)\n\nUnlike the install.packages command, the library command should be included at the top of any script files which then make use of the package’s functions. Another thing to note: the install.packages command requires the package name to be in quotes, whereas the library command does not require the package name to be in quotes. Don’t worry—mixing up when to use quotes and when not to is a common error you might encounter when starting out!\nInstalling the Tidyverse and Here Packages\nIn this course we will be making extensive use of the packages included in the Tidyverse. The Tidyverse is a set of packages designed to make data analysis in R easier and more streamlined. Each time you run library(tidyverse) in R, all of the following packages will get loaded in for you to use:\n\n\nggplot2 graphing\n\ndplyr data manipulation\n\ntidyr data cleaning\n\nreadr reading external data into R\n\npurrr functional programming\n\ntibble a nice alternative to base R data.frame objects\n\nstringr string manipulation\n\nforcats working with factor variables\n\nThe Tidyverse approach is usually contrasted against using “base R” functions, which do not require external packages. While everything Tidyverse can do, base R can do too, many find the Tidyverse approach much more intuitive. In fact, it is very common to see library(tidyverse) at the top of most R script files. There are also a multitude of packages that, although not technically part of the Tidyverse, share the same coding conventions as the core Tidyverse. So learning the Tidyverse will help when you when using more advanced packages.\nSome Tidyverse-adjacent packages:\n\n\nhaven reading in SPSS, Stata, and SAS data\n\nreadxl reading in Excel sheets\n\nrvest webscraping\n\nlubridate working with dates and times\n\ntidymodels machine learning workflows\n\nbroom statistical model object manipulation\n\nInstall the Tidyverse using:\n\n# This might take a couple minutes to download all packages\ninstall.packages(\"tidyverse\")\n\nI also highly recommend using the “here” package for file path management as explained earlier.\n\ninstall.packages(\"here\")\n\nOnce both packages are done installing, run the following lines of code to load them into R and make them available for use.\n\nlibrary(tidyverse)\nlibrary(here)"
  },
  {
    "objectID": "11_orientation.html#reading-in-data",
    "href": "11_orientation.html#reading-in-data",
    "title": "\n1  RStudio and Reading in Data\n",
    "section": "\n1.2 Reading in Data",
    "text": "1.2 Reading in Data\nFor our data visualization exercises we will be using the data set “county_elections.csv” which you can download at the Math Bootcamp GitHub repository here. After you download “county_elections.csv”, either copy or move it into your “data” folder in your Math Camp R Project directory. The source for this data comes from the MIT Election Data Science Lab and from the US Census accessed via IPUMS NHGIS.\nNow read the “county_elections.csv” data set into R using the following command:\n\ncounty_elections <- read_csv(here(\"data\", \"county_elections.csv\"))\n\nLet’s break down this line of code.\n\nThe function read_csv is from the package readr which is part of the Tidyverse. It loads a .csv data set file into R. The suffix .csv stands for “comma separated value” and is a very common format for storing tabular data. Inside read_csv(...) we put the file path pointing to the file we want to read into R.\nWe saw the here function earlier. Remember that this is just a convenient way of dealing with file paths. Try only running the command here(\"data\", \"county_elections.csv\") in your console to see how it creates an automatic file path for you.\nIn R, <- is the operator we use when we want to assign a value to an object. So the expression county_elections <- ... can be read as “take the thing on the right side of the arrow and assign it to an object named county_elections”. Check your Environment tab and verify that there is now an object called county_elections there. We can now take the county_elections object and do a bunch of stuff with it!\n\nWhen you read a new data set into R it’s often a good idea to do a quick visual inspection. Does the data look like what we’d expect? To do this, either click on the county_elections object in your Environment tab or type View(county_elections) in your Console. This will make the raw data pop up in a spreadsheet that you can scroll through and check out."
  },
  {
    "objectID": "12_visualization.html",
    "href": "12_visualization.html",
    "title": "2  Visualization",
    "section": "",
    "text": "This lesson is about creating effective data visualizations using the ggplot2 package (part of the Tidyverse). Becoming good at graphing your is a key skill you will want to develop while in the PhD program. Each graph you make should clearly communicate an insight without overloading your audience with too much information. Today we will practice the nuts and bolts of the coding necessary to accomplish this.\nLet’s start by loading in our external packages: Tidyverse, and here.\nWe will load in the same “county_elections.csv” data set from the previous chapter. Note: we will also remove each row in the data set containing missing values so that we avoid being spammed with warning messages from R. In a real data analysis project, you will want to investigate the source of missing data rather than blanket-removing everything."
  },
  {
    "objectID": "12_visualization.html#univariate-graphs",
    "href": "12_visualization.html#univariate-graphs",
    "title": "2  Visualization",
    "section": "\n2.1 Univariate Graphs",
    "text": "2.1 Univariate Graphs\nThe first graph we will make is a histogram. Histograms are the most common type of graph for displaying the distributions of continuous variables and make it easy to see the spread and central tendency of the data. Let’s plot the distribution of county median household income using the variable median_hh_inc in county_elections.\n\nggplot(county_elections) +\n  aes(x = median_hh_inc) +\n  geom_histogram()\n\n\n\n\nEach graph you create using ggplot will contain the following three elements:\n\n\nData. You need to tell ggplot which data set the variables that you want to graph come from. This section is ggplot(county_elections) in the code above.\n\nAesthetics. Now that we know which data set we’re working with, which variables do you want to use and in what way do we want them to be used? This information goes in the aes() section. Because histograms typically view the distribution of a single variable along the x-axis of a graph, we specify our aesthetic aes(x = median_hh_inc) in the code above.\n\nGeoms. The “geom” we choose defines the type of graph we’re ultimately creating (histogram, scatter plot, bar graph, etc). As you might expect, geom_histogram() creates a histogram for us!\n\nIn ggplot we combine these elements together using the + symbol. You could put the data, aesthetics, and geom sections all in the same line of code. But it is good practice to put each on its own line to make your code more readable.\nEach geom in ggplot has tons of extra options (also called arguments), which you can specify to make your graphs more pretty. Let’s begin to customizing our histogram.\n\nggplot(county_elections) +\n  aes(x = median_hh_inc) +\n  geom_histogram(bins = 50,\n                 color = \"white\",\n                 fill = \"steelblue\")\n\n\n\n\nWow look at that! Now let’s fix the ugly default names on the x and y axes, and add an informative title for our graph. We add custom labels to a ggplot graph by adding another + followed by a labs() section.\n\nggplot(county_elections) +\n  aes(x = median_hh_inc) +\n  geom_histogram(bins = 50,\n                 color = \"white\",\n                 fill = \"steelblue\") +\n  labs(title = \"Distribution of Median County Incomes\",\n       x = \"Median Household Income\",\n       y = \"Count\")\n\n\n\n\nThemes in ggplot control the overall look and background style of our graphs. For a complete list of themes: see here. There is also a package with a bunch of additional cool themes you can check out here. Personally I’m a big fan of theme_minimal().\n\nggplot(county_elections) +\n  aes(x = median_hh_inc) +\n  geom_histogram(bins = 50,\n                 color = \"white\",\n                 fill = \"steelblue\") +\n  labs(title = \"Distribution of Median County Incomes\",\n       x = \"Median Household Income\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\nOur county median household income variable looks like it’s a bit right-skewed—with a few extremely high income counties shown on the right hand side of the graph. Depending on your research question, it might make more sense to view this distribution on the log scale. It’s very easy to do this in ggplot using scale_x_log10().\n\nggplot(county_elections) +\n  aes(x = median_hh_inc) +\n  geom_histogram(bins = 50,\n                 color = \"white\",\n                 fill = \"steelblue\") +\n  labs(title = \"Distribution of Median County Incomes\",\n       x = \"Median Household Income\",\n       y = \"\",\n       caption = \"(log10 scale)\") +\n  theme_minimal() +\n  scale_x_log10(labels = scales::dollar)\n\n\n\n\nNow the data looks almost normally distributed. Also note the use of scales::dollar to make our x-axis a little easier to read. The scales package provides a ton of handy functions to deal with ugly default scales in ggplot. The :: operator is a way of accessing a single function from a package without loading all its other functions into R. It’s also a way of being explicit about which package’s function you are using. Sometimes you will run across situations where multiple packages have functions with the same name, but which do different things! Speaking from personal experience, this can lead to some really frustrating debugging sessions."
  },
  {
    "objectID": "12_visualization.html#bivariate-graphs",
    "href": "12_visualization.html#bivariate-graphs",
    "title": "2  Visualization",
    "section": "\n2.2 Bivariate Graphs",
    "text": "2.2 Bivariate Graphs\nIf histograms are the most common way to plot the distribution of a single continuous variable, scatter plots are the most common way to show the relationship between two continuous variables. Translating our histogram ggplot code to scatter plot code is straightforward: simply add a y-axis variable to aes(), and change the geom to geom_point(). The graph below displays the relationship between median household income and the population percentage in a county who did not complete high school.\n\nggplot(county_elections) +\n  aes(x = median_hh_inc, y = lesshs_pct) +\n  geom_point()\n\n\n\n\nIt looks like richer counties have lower rates of their population having less than a high school education. We can use the same customization options from histograms on our scatter plot to make things prettier.\n\nggplot(county_elections) +\n  aes(x = median_hh_inc, y = lesshs_pct) +\n  geom_point() +\n  labs(title = \"US Counties by Education and Income\",\n       x = \"Median Household Income\",\n       y = \"Less than High School %\") +\n  scale_x_continuous(label = scales::dollar) +\n  theme_classic()\n\n\n\n\nA lot of our data seems to be clustered up together. The solid points in geom_point() obscure this density so let’s fix this using the alpha argument. A geom’s alpha level specifies its transparency and ranges from 1 (solid) to 0 (invisible).\n\nggplot(county_elections) +\n  aes(x = median_hh_inc, y = lesshs_pct) +\n  geom_point(alpha = 0.2, color = \"darkcyan\") +\n  labs(title = \"US Counties by Education and Income\",\n       x = \"Median Household Income\",\n       y = \"Less than High School %\") +\n  scale_x_continuous(label = scales::dollar) +\n  theme_classic()\n\n\n\n\nThe negative relationship between our two variables is clear just by eyeballing it, but if we want to be real scientists we need to add the magic regression line. If you are unfamiliar with regression lines, don’t worry—we will be covering them extensively in your introductory quantitative methods course. A linear regression line is essentially just the “best fitting” straight line to the data.\nAdding a regression line to the graph gives us our first opportunity to combine multiple geoms. In the code chunk below, notice how we simply use + to add geom_smooth() to our ggplot object. This overlays a fitted line on top of the dots from geom_point(). The argument method = \"lm\" tells ggplot to use a linear regression line (lm = “linear model”) as opposed to some other type of fitted line.\n\nggplot(county_elections) +\n  aes(x = median_hh_inc, y = lesshs_pct) +\n  geom_point(alpha = 0.2, color = \"darkcyan\") +\n  geom_smooth(method = \"lm\", color = \"black\") +\n  labs(title = \"US Counties by Education and Income\",\n       x = \"Median Household Income\",\n       y = \"Less than High School %\") +\n  scale_x_continuous(label = scales::dollar) +\n  theme_classic()\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nThis graph would look way more professional if it didn’t mistakenly predict negative high school percentage values for high income counties. Linear regression is clearly not flexible enough to reflect the true relationship between our two variables.\nWhat if we re-scaled median household income to the log10 scale again?\n\nggplot(county_elections) +\n  aes(x = median_hh_inc, y = lesshs_pct) +\n  geom_point(alpha = 0.2, color = \"darkcyan\") +\n  geom_smooth(method = \"lm\", color = \"black\",\n              formula = \"y ~ log(x)\") +\n  labs(title = \"US Counties by Education and Income\",\n       x = \"Median Household Income\",\n       y = \"Less than High School %\") +\n  scale_x_continuous(label = scales::dollar) +\n  theme_classic()\n\n\n\n\nNot perfect, but now our fitted line is looking better!"
  },
  {
    "objectID": "12_visualization.html#trivariate-graphs",
    "href": "12_visualization.html#trivariate-graphs",
    "title": "2  Visualization",
    "section": "\n2.3 Trivariate(!) Graphs",
    "text": "2.3 Trivariate(!) Graphs\nWe are now experts at graphing one variable at a time—or two variables together—but what if we want to graph three or more variables at once? There are a number of ways to do this in ggplot as we will see below. However, first a word of caution: beware of cluttering your plots with too much information! It can be tempting to throw everything into a graph, but doing so can obscure the main point you’re trying to make. Always keep this in mind when going beyond graphing two variables at once.\n\n2.3.1 Using Colors and Shapes\nThe county_elections data set does not have a lot of categorical variables for us to work with. So let’s create one!\n\ncounty_elections <- county_elections |> \n  mutate(rural = ifelse(rural_pct > 50, \"Rural\", \"Not Rural\"))\n\nThis code chunk uses the mutate function to create a new variable in the county_elections data set called rural. The variable rural takes the value “Rural” if rural_pct is greater than 50 and takes the value “Not Rural” if rural_pct is less than or equal to 50. It is usually not a good idea to dichotomize a continuous variable in this way (using a binary Rural/Not Rural as opposed to the county’s rural percentage). Doing so throws away valuable information that is almost always relevant to the final analysis. In this case we can justify our choice to create a categorical variable because it will make plotting multiple variables much easier.\nLet’s now take our scatter plot showing the relationship between county median household income and education level, and color the points based on whether the county is rural or not. Doing so is as easy as adding color = rural to the aes() section in ggplot.\n\nggplot(county_elections) +\n  aes(x = median_hh_inc, y = lesshs_pct,\n      color = rural) + # Coloring points based on rural variable\n  geom_point(alpha = 0.5) + # Removed color from geom\n  labs(title = \"US Counties by Education and Income\",\n       x = \"Median Household Income\",\n       y = \"Less than High School %\") +\n  scale_x_continuous(label = scales::dollar) +\n  theme_classic()\n\n\n\n\nBy default, ggplot even gives us a handy legend to tell us which color points correspond to which value of rural.\nNow let’s try adding a continuous third variable to our scatter plot. One way to do this is with the size option in aes(). In the example below, each point’s size in our scatter plot is proportional to the total population in that county.\n\nggplot(county_elections) +\n  aes(x = median_hh_inc, y = lesshs_pct,\n      size = total_population) + # Changing size of points\n  geom_point(alpha = 0.2) + \n  labs(title = \"US Counties by Education and Income\",\n       x = \"Median Household Income\",\n       y = \"Less than High School %\") +\n  scale_x_continuous(label = scales::dollar) +\n  theme_classic()\n\n\n\n\nAre we overdoing things with adding too much information to our graph? Possibly!\nUsing Facets to Graph Comparisons\nOne of ggplot’s most powerful features is “faceting”. Facets allow you to easily graph comparisons between different levels of a categorical variable in a clear manner by creating side by side subgraphs. To apply a facet to our ggplot graph we can simply add + facet_wrap(~ facet_variable).\n\nggplot(county_elections) +\n  aes(x = median_hh_inc, y = lesshs_pct) +\n  geom_point(alpha = 0.2, color = \"darkcyan\") +\n  labs(title = \"US Counties by Education and Income\",\n       x = \"Median Household Income\",\n       y = \"Less than High School %\") +\n  scale_x_continuous(label = scales::dollar) +\n  theme_classic() +\n  facet_wrap(~ rural) # Adding faceting\n\n\n\n\nAs you can see, faceting is so powerful for showing comparisons because it preserves the scale in each subplot. This might be a better choice rather than coloring each point and overlapping everything. We can control whether we want the subgraphs side-by-side or on top of each other with the nrow argument.\n\nggplot(county_elections) +\n  aes(x = median_hh_inc, y = lesshs_pct) +\n  geom_point(alpha = 0.2, color = \"darkcyan\") +\n  labs(title = \"US Counties by Education and Income\",\n       x = \"Median Household Income\",\n       y = \"Less than High School %\") +\n  scale_x_continuous(label = scales::dollar) +\n  theme_classic() +\n  facet_wrap(~ rural, nrow = 2)"
  },
  {
    "objectID": "12_visualization.html#choropleth-maps",
    "href": "12_visualization.html#choropleth-maps",
    "title": "2  Visualization",
    "section": "\n2.4 Choropleth Maps",
    "text": "2.4 Choropleth Maps\nMaking maps in ggplot is relatively straightforward—and a much better idea than copying and pasting your data back and forth between R and a specialized program like ArcGIS. Choropleth maps show data broken down by geographic unit (in this case US counties). We will need to install an additional package urbnmapr to help ggplot make this type of graph. To install urbnmapr, run the following command in your Console.\n\ndevtools::install_github(\"UrbanInstitute/urbnmapr\")\n\nWe use the command devtools::install_github() because the developers of urbnmapr have not submitted their package to the official CRAN repository. So rather than using install.packages like we’re used to, we instead need to install the package directly from GitHub. A lot of excellent packages are not available on CRAN, but be aware that they might not have all the quality-control checks CRAN packages have.\nOnce you have the urbnmapr package installed, you can load it into R using:\n\nlibrary(urbnmapr)\n\nWe need to perform a couple data cleaning steps before the data is ready to map in ggplot. The first step is making our countyCode variable match the format of the corresponding US county code in the urbnmapr data. US counties are each given a unique 5-digit number called a FIPS code. However, at some point the “county_elections.csv” file was opened in Excel, which read the FIPS codes as numeric values thereby removing any 0’s from the start of each code. Never open your data in Excel! Now a bunch of the FIPS codes in our data are only 4-digits long instead of 5, which means they will not match the FIPS codes in the urbnmapr data. Luckily we can fix this using the Tidyverse. The function str_pad from the stringr package can be used to “pad” out a variable with a specific character until it becomes a specific size.\n\ncounty_elections <- county_elections |> \n  mutate(county_fips = str_pad(countyCode, width = 5, pad = \"0\"))\n\nNext we need to join our county_elections data with the mapping data from urbnmapr. We will do this using a left_join command, which, if you are not familiar with, we will cover in much greater detail in a future lesson. The big idea here is that we have one data set with county-level variables, such as median household income, that we need to merge with a data set containing the geographic coordinate information for each US county.\n\nmap_data <- left_join(county_elections, counties,\n                      by = \"county_fips\")\n\nAwesome! Now we are ready to make a map! Let’s check out the geographic distribution of population percentage without a high school diploma.\n\nggplot(map_data) +\n  aes(x = long, y = lat, \n      group = group, fill = lesshs_pct) +\n  geom_polygon(color = NA) +\n  # This second geom_polygon shows the state borders\n  geom_polygon(data = states, mapping = aes(long, lat, group = group),\n               fill = NA, size = 0.1, color = \"white\") +\n  # Making maps requires you to choose a geographic projection\n  coord_map(projection = \"albers\", lat0 = 39, lat1 = 45) +\n  # theme_void gives us a blank canvas\n  theme_void()\n\n\n\n\nDon’t worry if you are not yet able to understand every aspect of the ggplot code that produced this map. Try playing around with some of the arguments and see what happens to the map!\n\nggplot(map_data) +\n  aes(x = long, y = lat, \n      group = group, fill = lesshs_pct) +\n  geom_polygon(color = NA) +\n  geom_polygon(data = states, mapping = aes(long, lat, group = group),\n               fill = NA, size = 0.1, color = \"white\") +\n  coord_map(projection = \"albers\", lat0 = 39, lat1 = 45) +\n  # This creates a diverging color scale\n  # that is also colorblind friendly\n  scale_fill_viridis_c() +\n  labs(fill = \"Less than High School %\") +\n  theme_void() +\n  theme(legend.position = \"bottom\") \n\n\n\n\nSometimes a diverging color scale is better for contrasting high and low value areas."
  },
  {
    "objectID": "13_data_wrangling_cleaning.html",
    "href": "13_data_wrangling_cleaning.html",
    "title": "3  Data Wrangling",
    "section": "",
    "text": "The raw data used in much of political science research rarely comes to us in a format that is immediately accessible for analysis. Instead, we frequently need to write an R script to “clean” the data first. Only after cleaning the data will it be usable for visualization or statistical modeling. This process (also known as “data wrangling”) can be extremely arduous and time-consuming—depending on how messy the raw data is in the first place. According to the common aphorism, the time you spend writing R code to clean your data sets will far exceed the time you spend subsequently running any sort of statistical analysis on the cleaned data.\nAlthough data cleaning has a reputation as a dull, menial task (compared to the “fun” of statistical modeling), try to avoid treating the process like it is merely a roadblock—something to be overcome before you can begin the real analysis. In fact, each decision you make when wrangling your raw data into its final state is a crucial part of the final research product. Whether or not to drop certain observations, or using different levels of data aggregation, are some examples of choices which can have massive downstream effects. You should be thoughtful and transparent about your decision-making process the whole time you spend cleaning data."
  },
  {
    "objectID": "13_data_wrangling_cleaning.html#cleaning-data-using-dplyr",
    "href": "13_data_wrangling_cleaning.html#cleaning-data-using-dplyr",
    "title": "3  Data Wrangling",
    "section": "\n3.2 Cleaning Data Using dplyr",
    "text": "3.2 Cleaning Data Using dplyr\nAs in the other sections of this book, we will be using the Tidyverse approach to data cleaning here. The primary Tidyverse package for data cleaning is dplyr and it contains most of the functions we will be using in this chapter.\nRather than downloading a .csv file and using the read_csv() function to load in our data for this chapter, we will be using a package which contains multiple data sets. The data come from the United Nations General Assembly. First install the unvotes package by running the following command in your Console.\n\ninstall.packages(\"unvotes\")\n\nThen run\n\nlibrary(unvotes)\nlibrary(tidyverse)\n\nWhen you use the library() function, many packages automatically load small data sets which immediately become available for use. However, accessing these data can be a bit confusing because they do not automatically show up as objects in your Environment tab. Let’s add the unvotes data sets to our Environment with the following chunk of code:\n\nun_votes <- un_votes\nun_roll_calls <- un_roll_calls\nun_roll_call_issues <- un_roll_call_issues\n\nAbout the data\n\n\nun_votes, country-vote level data. Each row is a country’s vote on a particular UN Assembly resolution.\n\nun_roll_calls, resolution level data. Contains information about each resolution.\n\nun_roll_call_issues, resolution level data. Contains the issue-area for each resolution.\n\nThe Pipe Operator\nBefore we get started using dplyr we need to first introduce the “pipe” operator |>. Pipes are an extremely convenient tool for linking several functions together. They work by passing the object on the left hand side of the pipe into the function following the pipe. Here is an example.\n\n# The pipe way\nme |> \n  wake_up(time = \"8:00\") |> \n  get_out_of_bed(side = \"correct\") |> \n  get_dressed(pants = TRUE, shirt = TRUE) |> \n  leave_house(car = FALSE, bike = TRUE)\n\n# The non-pipe way\nleave_house(get_dressed(get_out_of_bed(wake_up(me, time = \"8:00\"), side = \"correct\"), pants = TRUE, shirt = TRUE), car = FALSE, bike = TRUE)\n\nBoth code chunks above will end up doing the same thing. But the pipe method is much easier to write, and much easier for others to read and understand.\nBefore R version 4.1, the pipe operator was only available in the magrittr package (which remains part of the Tidyverse). The magrittr pipe is written %>% and many R users continue to use this over the native R |>. There are a few very minor differences between the two pipes, which you should feel free to ignore. We use the native R |> pipe in this book because it is generally a good idea, all else equal, to reduce your dependency on outside packages when writing code."
  },
  {
    "objectID": "13_data_wrangling_cleaning.html#working-with-columns",
    "href": "13_data_wrangling_cleaning.html#working-with-columns",
    "title": "3  Data Wrangling",
    "section": "\n3.3 Working with Columns",
    "text": "3.3 Working with Columns\nEach column in a data set typically represents a single variable, or attribute, relating to the observation in a particular row. In this section we will look at some of dplyr’s functions for working with columns.\nSelect\nThe select() function is primarily used to remove unwanted columns from the data. Here is an example.\n\nun_roll_calls |> \n  select(rcid, date) # Keeping only two variables\n\n# A tibble: 6,202 × 2\n    rcid date      \n   <int> <date>    \n 1     3 1946-01-01\n 2     4 1946-01-02\n 3     5 1946-01-04\n 4     6 1946-01-04\n 5     7 1946-01-02\n 6     8 1946-01-05\n 7     9 1946-02-05\n 8    10 1946-02-05\n 9    11 1946-02-05\n10    12 1946-02-06\n# … with 6,192 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nNote that the code chunk above did not alter the data set object un_roll_calls. If we wanted to take this smaller data set and use it for something else, we will need to use the assignment operator <- to save our work.\n\n# This creates a new object called \"small_un_roll_calls\"\n# containing only rcid and date columns\nsmall_un_roll_calls <- un_roll_calls |> \n  select(rcid, date)\n\nThe select() function can be very versatile. Rather than typing every column name we want to keep, it is often faster to specify the columns we want to drop.\n\nun_roll_calls |> \n  select(-session) # Keep everything except session column\n\n# A tibble: 6,202 × 8\n    rcid importantvote date       unres   amend  para short                descr\n   <int>         <int> <date>     <chr>   <int> <int> <chr>                <chr>\n 1     3             0 1946-01-01 R/1/66      1     0 AMENDMENTS, RULES O… \"TO …\n 2     4             0 1946-01-02 R/1/79      0     0 SECURITY COUNCIL EL… \"TO …\n 3     5             0 1946-01-04 R/1/98      0     0 VOTING PROCEDURE     \"TO …\n 4     6             0 1946-01-04 R/1/107     0     0 DECLARATION OF HUMA… \"TO …\n 5     7             0 1946-01-02 R/1/295     1     0 GENERAL ASSEMBLY EL… \"TO …\n 6     8             0 1946-01-05 R/1/297     1     0 ECOSOC POWERS        \"TO …\n 7     9             0 1946-02-05 R/1/329     0     0 POST-WAR RECONSTRUC… \"TO …\n 8    10             0 1946-02-05 R/1/361     1     1 U.N. MEMBERS, RELAT… \"TO …\n 9    11             0 1946-02-05 R/1/376     0     0 TRUSTEESHIP AMENDME… \"TO …\n10    12             0 1946-02-06 R/1/394     1     1 COUNCIL MEMBER TERM… \"TO …\n# … with 6,192 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThere are many other convenient ways to selection columns (e.g. by common names, by type of data, or by position in the data set). For a full list of ways to use select() see this link. Here is one example of selecting every column which is a “character” type.\n\nun_roll_calls |> \n  select(where(is.character))\n\n# A tibble: 6,202 × 3\n   unres   short                              descr                             \n   <chr>   <chr>                              <chr>                             \n 1 R/1/66  AMENDMENTS, RULES OF PROCEDURE     \"TO ADOPT A CUBAN AMENDMENT TO TH…\n 2 R/1/79  SECURITY COUNCIL ELECTIONS         \"TO ADOPT A USSR PROPOSAL ADJOURN…\n 3 R/1/98  VOTING PROCEDURE                   \"TO ADOPT THE KOREAN PROPOSAL THA…\n 4 R/1/107 DECLARATION OF HUMAN RIGHTS        \"TO ADOPT A CUBAN PROPOSAL (A/3-C…\n 5 R/1/295 GENERAL ASSEMBLY ELECTIONS         \"TO ADOPT A 6TH COMMITTEE AMENDME…\n 6 R/1/297 ECOSOC POWERS                      \"TO ADOPT A SECOND 6TH COMM. AMEN…\n 7 R/1/329 POST-WAR RECONSTRUCTION            \"TO OPEN THE DISCUSSION ON THE PO…\n 8 R/1/361 U.N. MEMBERS, RELATIONS WITH SPAIN \"TO ADOPT GENERAL COMM. DRAFT RES…\n 9 R/1/376 TRUSTEESHIP AMENDMENTS             \"TO ADOPT DRAFT RESOLUTIONS I AND…\n10 R/1/394 COUNCIL MEMBER TERM LENGTH         \"TO ADOPT PARAGRAPH (A) OF THE 6T…\n# … with 6,192 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nLastly, we can use select() to rename columns in our data. The chunk of code below selects the “unres” and date columns, and renames “unres” to “un_resolution” at the same time.\n\nun_roll_calls |> \n  select(un_resolution = unres, date)\n\n# A tibble: 6,202 × 2\n   un_resolution date      \n   <chr>         <date>    \n 1 R/1/66        1946-01-01\n 2 R/1/79        1946-01-02\n 3 R/1/98        1946-01-04\n 4 R/1/107       1946-01-04\n 5 R/1/295       1946-01-02\n 6 R/1/297       1946-01-05\n 7 R/1/329       1946-02-05\n 8 R/1/361       1946-02-05\n 9 R/1/376       1946-02-05\n10 R/1/394       1946-02-06\n# … with 6,192 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nIf you only want to rename columns—without specifying which to select—dplyr has a function rename() for this purpose. The syntax is similar to the code chunk above new_variable_name = original_variable_name. This code will keep all columns and change the names of the specified variables.\n\nun_roll_calls |> \n  rename(un_resolution = unres,\n         amendment = amend,\n         paragraph = para)\n\n# A tibble: 6,202 × 9\n    rcid session importantvote date       un_resol…¹ amend…² parag…³ short descr\n   <int>   <dbl>         <int> <date>     <chr>        <int>   <int> <chr> <chr>\n 1     3       1             0 1946-01-01 R/1/66           1       0 AMEN… \"TO …\n 2     4       1             0 1946-01-02 R/1/79           0       0 SECU… \"TO …\n 3     5       1             0 1946-01-04 R/1/98           0       0 VOTI… \"TO …\n 4     6       1             0 1946-01-04 R/1/107          0       0 DECL… \"TO …\n 5     7       1             0 1946-01-02 R/1/295          1       0 GENE… \"TO …\n 6     8       1             0 1946-01-05 R/1/297          1       0 ECOS… \"TO …\n 7     9       1             0 1946-02-05 R/1/329          0       0 POST… \"TO …\n 8    10       1             0 1946-02-05 R/1/361          1       1 U.N.… \"TO …\n 9    11       1             0 1946-02-05 R/1/376          0       0 TRUS… \"TO …\n10    12       1             0 1946-02-06 R/1/394          1       1 COUN… \"TO …\n# … with 6,192 more rows, and abbreviated variable names ¹​un_resolution,\n#   ²​amendment, ³​paragraph\n# ℹ Use `print(n = ...)` to see more rows\n\n\nMutate\nThe select() and rename() functions are great for tidying up your data sets, but they do not change the underlying variables. To change existing variables or to create new ones we use mutate().\nLet’s say we discovered that all the roll call IDs in the column “rcid” were supposed to be in multiples of 10. The code chunk below creates a new variable called “rcid_10” which is simply the value of the original “rcid” variable multiplied by 10.\n\nun_votes |> \n  mutate(rcid_10 = rcid * 10) # Creates the new rcid_10 variable\n\n# A tibble: 869,937 × 5\n    rcid country            country_code vote  rcid_10\n   <dbl> <chr>              <chr>        <fct>   <dbl>\n 1     3 United States      US           yes        30\n 2     3 Canada             CA           no         30\n 3     3 Cuba               CU           yes        30\n 4     3 Haiti              HT           yes        30\n 5     3 Dominican Republic DO           yes        30\n 6     3 Mexico             MX           yes        30\n 7     3 Guatemala          GT           yes        30\n 8     3 Honduras           HN           yes        30\n 9     3 El Salvador        SV           yes        30\n10     3 Nicaragua          NI           yes        30\n# … with 869,927 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nIf you didn’t want to create a brand new variable, but instead wanted to overwrite the original variable, you just need to put the original variable to the left of the = in mutate().\n\nun_votes |> \n  mutate(rcid = rcid * 10) # Overwrites the existing rcid variable\n\n# A tibble: 869,937 × 4\n    rcid country            country_code vote \n   <dbl> <chr>              <chr>        <fct>\n 1    30 United States      US           yes  \n 2    30 Canada             CA           no   \n 3    30 Cuba               CU           yes  \n 4    30 Haiti              HT           yes  \n 5    30 Dominican Republic DO           yes  \n 6    30 Mexico             MX           yes  \n 7    30 Guatemala          GT           yes  \n 8    30 Honduras           HN           yes  \n 9    30 El Salvador        SV           yes  \n10    30 Nicaragua          NI           yes  \n# … with 869,927 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nWe commonly need to create a new variable whose values depend on the values of one of the original variables. The function case_when() helps us do this inside mutate().\n\nun_votes |> \n  mutate(vote_dummy = case_when(vote == \"yes\" ~ 1,\n                                vote == \"no\" ~ 0))\n\n# A tibble: 869,937 × 5\n    rcid country            country_code vote  vote_dummy\n   <dbl> <chr>              <chr>        <fct>      <dbl>\n 1     3 United States      US           yes            1\n 2     3 Canada             CA           no             0\n 3     3 Cuba               CU           yes            1\n 4     3 Haiti              HT           yes            1\n 5     3 Dominican Republic DO           yes            1\n 6     3 Mexico             MX           yes            1\n 7     3 Guatemala          GT           yes            1\n 8     3 Honduras           HN           yes            1\n 9     3 El Salvador        SV           yes            1\n10     3 Nicaragua          NI           yes            1\n# … with 869,927 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAs you can see, the code chunk above creates a new variable called “vote_dummy” which takes the value 1 if “vote” equals \"yes\" and takes the values 0 if “vote” equals \"no\". Like other programming languages, R uses the == logical operator to check whether a value equals, or is equivalent, to some other value. This is different from the single = which is used to create entire new variables inside mutate().\nHere is one more example using case_when().\n\nun_votes |> \n  mutate(rcid_era = case_when(rcid < 2000 ~ \"old\",\n                              rcid >= 2000 & rcid < 6000 ~ \"middle\",\n                              rcid >= 6000 ~ \"recent\"))\n\n# A tibble: 869,937 × 5\n    rcid country            country_code vote  rcid_era\n   <dbl> <chr>              <chr>        <fct> <chr>   \n 1     3 United States      US           yes   old     \n 2     3 Canada             CA           no    old     \n 3     3 Cuba               CU           yes   old     \n 4     3 Haiti              HT           yes   old     \n 5     3 Dominican Republic DO           yes   old     \n 6     3 Mexico             MX           yes   old     \n 7     3 Guatemala          GT           yes   old     \n 8     3 Honduras           HN           yes   old     \n 9     3 El Salvador        SV           yes   old     \n10     3 Nicaragua          NI           yes   old     \n# … with 869,927 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThe new variable “rcid_era” takes three values, \"old\", \"middle\", and \"recent\" based on what range the “rcid” variable falls within.\nTo recap, the most commonly used functions for cleaning columns/variables in your data are:\n\n\nselect() removes and/or renames columns.\n\nrename() renames existing columns without removing any.\n\nmutate() changes the values of existing columns and creates new columns."
  },
  {
    "objectID": "13_data_wrangling_cleaning.html#working-with-rows",
    "href": "13_data_wrangling_cleaning.html#working-with-rows",
    "title": "3  Data Wrangling",
    "section": "\n3.4 Working with Rows",
    "text": "3.4 Working with Rows\nFilter\nDplyr’s primary function for removing unwanted rows is filter(). Like we saw when using case_when() inside mutate(), using filter() requires some practice with logical operators. The function filter() works by specifying some variable and only keeping rows in the data for which the logical operation evaluates to TRUE. Here is an example.\n\nun_roll_calls |> \n  filter(importantvote == 1)\n\n# A tibble: 411 × 9\n    rcid session importantvote date       unres     amend  para short      descr\n   <int>   <dbl>         <int> <date>     <chr>     <int> <int> <chr>      <chr>\n 1  2491      38             1 1983-10-04 R/38/3        0     0 KAMPUCHEA  TO R…\n 2  2492      38             1 1983-11-06 R/38/7        0     0 GRENADA, … TO D…\n 3  2497      38             1 1983-11-06 R/38/29       0     0 AFGHANIST… TO D…\n 4  2504      38             1 1983-12-06 R/38/39A      0     0 APARTHEID… TO A…\n 5  2510      38             1 1983-12-06 R/38/39G      0     0 SOUTH AFR… TO R…\n 6  2526      38             1 1983-12-06 R/38/180E     0     0 ISRAEL, I… TO O…\n 7  2563      38             1 1983-12-07 R/38/187C     0     0 CHEMICAL,… TO N…\n 8  2610      38             1 1983-12-03 R/38/101      0     0 HUMAN RIG… TO E…\n 9  2641      39             1 1984-10-04 R/39/5        0     0 KAMPUCHEA  TO R…\n10  2645      39             1 1984-11-02 R/39/13       0     0 AFGHANIST… TO R…\n# … with 401 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThe original “un_roll_calls” data set object has 6202 rows, whereas this new data set only has 411 rows. This is because we filtered out any row in which the variable “importantvote” was not equal to 1. Like select(), we can extend the use of filter() in many ways.\n\n# & for AND\nun_roll_calls |> \n  filter(importantvote == 1 & session > 40)\n\n# A tibble: 386 × 9\n    rcid session importantvote date       unres     amend  para short      descr\n   <int>   <dbl>         <int> <date>     <chr>     <int> <int> <chr>      <chr>\n 1  2948      41             1 1986-10-03 R/41/6       NA    NA KAMPUCHEA  The …\n 2  2957      41             1 1986-11-02 R/41/31      NA    NA NICARAGUA… Urge…\n 3  2958      41             1 1986-11-04 R/41/33      NA    NA AFGHANIST… The …\n 4  2967      41             1 1986-11-05 R/41/38      NA    NA LIBYA, U.… Decl…\n 5  2968      41             1 1986-11-05 R/41/39A     NA    NA NAMIBIA, … Situ…\n 6  2996      41             1 1986-12-04 R/41/58C     NA    NA CHEMICAL,… Proh…\n 7  3075      41             1 1986-12-05 R/41/158     NA    NA HUMAN RIG… Ques…\n 8  3078      41             1 1986-12-05 R/41/162A    NA    NA MIDDLE EA… Reaf…\n 9  3098      41             1 1986-12-05 R/41/211A    NA    NA BUDGET, 1… Revi…\n10  3099      41             1 1986-12-05 R/41/211B    NA    NA BUDGET, 1… Revi…\n# … with 376 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n# | for OR\nun_roll_calls |> \n  filter(importantvote == 1 | session > 40)\n\n# A tibble: 3,228 × 9\n    rcid session importantvote date       unres     amend  para short      descr\n   <int>   <dbl>         <int> <date>     <chr>     <int> <int> <chr>      <chr>\n 1  2491      38             1 1983-10-04 R/38/3        0     0 KAMPUCHEA  TO R…\n 2  2492      38             1 1983-11-06 R/38/7        0     0 GRENADA, … TO D…\n 3  2497      38             1 1983-11-06 R/38/29       0     0 AFGHANIST… TO D…\n 4  2504      38             1 1983-12-06 R/38/39A      0     0 APARTHEID… TO A…\n 5  2510      38             1 1983-12-06 R/38/39G      0     0 SOUTH AFR… TO R…\n 6  2526      38             1 1983-12-06 R/38/180E     0     0 ISRAEL, I… TO O…\n 7  2563      38             1 1983-12-07 R/38/187C     0     0 CHEMICAL,… TO N…\n 8  2610      38             1 1983-12-03 R/38/101      0     0 HUMAN RIG… TO E…\n 9  2641      39             1 1984-10-04 R/39/5        0     0 KAMPUCHEA  TO R…\n10  2645      39             1 1984-11-02 R/39/13       0     0 AFGHANIST… TO R…\n# … with 3,218 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThe first chunk above filters out all rows in which “importantvote” did not equal 1 AND “session” was less than 40. The second chunk above filters out all rows in which “importantvote” did not equal 1 OR “session” was less than 40.\nRemoving missing, or NA, values is another common job for filter(). To do this you can use the function is.na() inside filter(). The function is.na() evaluates to TRUE if the variable’s value is NA and evaluates to FALSE otherwise. Because filter() only keeps rows where the condition evaluates to TRUE, in order to remove NA values we need to negate is.na() with the ! operator. The ! operator flips the truthiness of any logical statement it precedes.\n\nun_roll_calls |> \n  filter(!is.na(amend)) # Removing rows with NA for the amend variable\n\n# A tibble: 2,868 × 9\n    rcid session importantvote date       unres   amend  para short        descr\n   <int>   <dbl>         <int> <date>     <chr>   <int> <int> <chr>        <chr>\n 1     3       1             0 1946-01-01 R/1/66      1     0 AMENDMENTS,… \"TO …\n 2     4       1             0 1946-01-02 R/1/79      0     0 SECURITY CO… \"TO …\n 3     5       1             0 1946-01-04 R/1/98      0     0 VOTING PROC… \"TO …\n 4     6       1             0 1946-01-04 R/1/107     0     0 DECLARATION… \"TO …\n 5     7       1             0 1946-01-02 R/1/295     1     0 GENERAL ASS… \"TO …\n 6     8       1             0 1946-01-05 R/1/297     1     0 ECOSOC POWE… \"TO …\n 7     9       1             0 1946-02-05 R/1/329     0     0 POST-WAR RE… \"TO …\n 8    10       1             0 1946-02-05 R/1/361     1     1 U.N. MEMBER… \"TO …\n 9    11       1             0 1946-02-05 R/1/376     0     0 TRUSTEESHIP… \"TO …\n10    12       1             0 1946-02-06 R/1/394     1     1 COUNCIL MEM… \"TO …\n# … with 2,858 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nIt can be confusing to think in terms of logical statements, especially when negation is involved! But you will become more comfortable with filter() the more you practice.\nAnother handy logical operator is %in%. This lets you specify several values at once when checking whether to keep rows in filter().\n\n# Cumbersome way\nun_votes |> \n  filter(country == \"Kenya\" |\n         country == \"Grenada\" |\n         country == \"Canada\" |\n         country == \"Latvia\" |\n         country == \"Yemen\" |\n         country == \"Angola\")\n\n# A tibble: 23,324 × 4\n    rcid country country_code vote \n   <dbl> <chr>   <chr>        <fct>\n 1     3 Canada  CA           no   \n 2     4 Canada  CA           no   \n 3     5 Canada  CA           no   \n 4     6 Canada  CA           no   \n 5     7 Canada  CA           no   \n 6     8 Canada  CA           yes  \n 7     9 Canada  CA           yes  \n 8    10 Canada  CA           yes  \n 9    11 Canada  CA           yes  \n10    12 Canada  CA           yes  \n# … with 23,314 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n# Easier way\ncountry_list <- c(\"Kenya\", \"Grenada\", \"Canada\",\n                  \"Latvia\", \"Yemen\", \"Angola\")\nun_votes |> \n  filter(country %in% country_list)\n\n# A tibble: 23,324 × 4\n    rcid country country_code vote \n   <dbl> <chr>   <chr>        <fct>\n 1     3 Canada  CA           no   \n 2     4 Canada  CA           no   \n 3     5 Canada  CA           no   \n 4     6 Canada  CA           no   \n 5     7 Canada  CA           no   \n 6     8 Canada  CA           yes  \n 7     9 Canada  CA           yes  \n 8    10 Canada  CA           yes  \n 9    11 Canada  CA           yes  \n10    12 Canada  CA           yes  \n# … with 23,314 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThe two chunks of code above will produce the same filtered data set, but the second chunk requires fewer lines of code and is more flexible if we want to add or remove countries.\nAggregation\nFiltering is great for removing unwanted rows in your data. However, after using filter() the data set’s unit of analysis typically remains the same. In the code chunk above, for example, we removed most of the countries from our data, but each row in the resulting data set is still a unique country-vote observation.\nLet’s say we want to reduce the data set down to the country level, rather than country-vote level, and examine the proportion of “no” votes taken by each country. We can do this in dplyr using the pair of functions group_by() and summarize(). Here is an example.\n\nun_votes |> \n  # Creating a new numeric dummy vote variable\n  mutate(vote_dummy = case_when(vote == \"yes\" ~ 1,\n                                vote == \"no\" ~ 0)) |> \n  # Specify level of aggregation\n  group_by(country) |> \n  # Perform the aggregation function by group\n  summarize(proportion_yes_vote = mean(vote_dummy, na.rm = TRUE))\n\n# A tibble: 200 × 2\n   country           proportion_yes_vote\n   <chr>                           <dbl>\n 1 Afghanistan                     0.938\n 2 Albania                         0.816\n 3 Algeria                         0.965\n 4 Andorra                         0.833\n 5 Angola                          0.985\n 6 Antigua & Barbuda               0.988\n 7 Argentina                       0.935\n 8 Armenia                         0.950\n 9 Australia                       0.745\n10 Austria                         0.880\n# … with 190 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThe code above uses the function mean() to calculate the average value of “vote_dummy” within each country. Because “vote_dummy” takes the values 1 and 0, this average can be interpreted as a proportion of 1’s, or “yes” votes. We need to add na.rm = TRUE inside mean() to tell R to ignore NA values when calculating this average.\nIf we want to sort our data by the new aggregated column, we can do so using arrange() after summarize().\n\nun_votes |> \n  mutate(vote_dummy = case_when(vote == \"yes\" ~ 1,\n                                vote == \"no\" ~ 0)) |> \n  group_by(country) |> \n  summarize(proportion_yes_vote = mean(vote_dummy, na.rm = TRUE)) |> \n  arrange(proportion_yes_vote)\n\n# A tibble: 200 × 2\n   country                          proportion_yes_vote\n   <chr>                                          <dbl>\n 1 United States                                  0.369\n 2 Israel                                         0.464\n 3 Palau                                          0.541\n 4 Micronesia (Federated States of)               0.604\n 5 United Kingdom                                 0.616\n 6 Marshall Islands                               0.659\n 7 France                                         0.662\n 8 Federal Republic of Germany                    0.682\n 9 Canada                                         0.701\n10 Belgium                                        0.714\n# … with 190 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n# Use arrange(desc(proportion_yes_vote))\n# to sort in descending order\n\nThe United States votes “No” at a much higher rate than other countries!\nLastly, the dplyr function count() is very handy for quickly tallying the number of observations within each category of a variable. Simply put the name of the variable you want to count inside count() and voila.\n\nun_roll_call_issues |> \n  count(issue)\n\n# A tibble: 6 × 2\n  issue                                    n\n  <fct>                                <int>\n1 Colonialism                            957\n2 Arms control and disarmament          1092\n3 Economic development                   765\n4 Human rights                          1015\n5 Palestinian conflict                  1061\n6 Nuclear weapons and nuclear material   855"
  },
  {
    "objectID": "13_data_wrangling_cleaning.html#merging-data",
    "href": "13_data_wrangling_cleaning.html#merging-data",
    "title": "3  Data Wrangling",
    "section": "\n3.5 Merging Data",
    "text": "3.5 Merging Data\nSo far we have only been working with one data set at a time. But it is rare to find all the data you need for a particular project in one single data set. Frequently we will have to merge data sets together in order to have all our variables in the same place. The most common way to merge data using dplyr is with the left_join() function.\nTo illustrate how left_join() works, let’s say we have two data sets: our main data set, x and the data set we want to merge, y. Here is an example:\n\nxy_data <- left_join(x, y, by = \"key_variable\")\n\nThe new data set xy_data is produced by merging the y data set into the x data set based on the shared value of the key_variable variable. Left joins will always keep all rows in the x data set, but will only merge in rows from y if they match up with the key_variable. Your key variable will typically be something like country ID code or a unit’s name in the data. Because we like to use pipes in this course, the chunk of code above can be rewritten as:\n\nxy_data <- x |> \n  left_join(y, by = \"key_variable\")\n\nNow let’s take a look at left_join() using real data from the unvotes data set.\n\nun_votes |> \n  left_join(un_roll_calls, by = \"rcid\")\n\n# A tibble: 869,937 × 12\n    rcid country      count…¹ vote  session impor…² date       unres amend  para\n   <dbl> <chr>        <chr>   <fct>   <dbl>   <int> <date>     <chr> <int> <int>\n 1     3 United Stat… US      yes         1       0 1946-01-01 R/1/…     1     0\n 2     3 Canada       CA      no          1       0 1946-01-01 R/1/…     1     0\n 3     3 Cuba         CU      yes         1       0 1946-01-01 R/1/…     1     0\n 4     3 Haiti        HT      yes         1       0 1946-01-01 R/1/…     1     0\n 5     3 Dominican R… DO      yes         1       0 1946-01-01 R/1/…     1     0\n 6     3 Mexico       MX      yes         1       0 1946-01-01 R/1/…     1     0\n 7     3 Guatemala    GT      yes         1       0 1946-01-01 R/1/…     1     0\n 8     3 Honduras     HN      yes         1       0 1946-01-01 R/1/…     1     0\n 9     3 El Salvador  SV      yes         1       0 1946-01-01 R/1/…     1     0\n10     3 Nicaragua    NI      yes         1       0 1946-01-01 R/1/…     1     0\n# … with 869,927 more rows, 2 more variables: short <chr>, descr <chr>, and\n#   abbreviated variable names ¹​country_code, ²​importantvote\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nThe code chunk above merged all the variables from the un_roll_calls data set into the un_votes data set. Recall that the un_votes data contains information about how countries voted on particular UN roll call votes, and the un_roll_calls data contains information about the specific votes (such as whether the vote was an “importantvote”). If there were any rows in un_roll_calls that had rcid values which failed to match with an rcid value in un_votes, the row would not get merged.\nWhile left_join() is far and away the most common, and useful, way to merge two data sets when using dplyr, there are various other types of merges which have their niche applications. For a more in-depth look at merging data in dplyr see this page with helpful animations."
  },
  {
    "objectID": "14_loops_simulations.html",
    "href": "14_loops_simulations.html",
    "title": "4  Loops and Simulation",
    "section": "",
    "text": "Thus far in the course we have been working with real data. These data sets contained variables which correspond to tangible phenomena out in the world. While the bulk of social science research uses data of this sort, generating fake data can be surprisingly useful. Fake, or synthetic, data allows us to control every aspect of the data generating process (DGP). By manually tweaking different parameters that govern the DGP, we can better understand how our statistical methods will perform on real data. In this chapter we will learn some of the tools in R used to simulate fake data."
  },
  {
    "objectID": "14_loops_simulations.html#generating-fake-data",
    "href": "14_loops_simulations.html#generating-fake-data",
    "title": "4  Loops and Simulation",
    "section": "\n4.1 Generating Fake Data",
    "text": "4.1 Generating Fake Data\nFirst let’s load in our favorite packages.\n\nlibrary(tidyverse)\nlibrary(here)\n\nsample()\nThe simplest way to generate random data in R is through the function samples(). The primary inputs to sample() are x, which is the set of values you want to draw from and size, which is the number of samples you want to draw. Let’s takes a look at an example.\n\n# Create a vector of character values\nmy_animals <- c(\"cat\", \"dog\", \"fish\", \"monkey\", \"parrot\")\n\n# From the set \"my_animals\", draw 2 at random\nsample(x = my_animals, size = 2)\n\n[1] \"parrot\" \"cat\"   \n\n\nBy default, sample() samples without replacement. This means that once a value from x is drawn, it will be ineligible to be drawn again. The following code gives us an error because size = 100 is larger than the total number of values in my_animals.\n\nsample(x = my_animals, size = 100)\n\nError in sample.int(length(x), size, replace, prob): cannot take a sample larger than the population when 'replace = FALSE'\n\n\nTo fix this we need to sample with replacement by setting replace = TRUE inside sample().\n\nsample(x = my_animals, size = 100, replace = TRUE)\n\n  [1] \"parrot\" \"monkey\" \"cat\"    \"fish\"   \"monkey\" \"parrot\" \"parrot\" \"dog\"   \n  [9] \"fish\"   \"cat\"    \"parrot\" \"cat\"    \"fish\"   \"dog\"    \"cat\"    \"cat\"   \n [17] \"monkey\" \"dog\"    \"monkey\" \"dog\"    \"fish\"   \"parrot\" \"fish\"   \"fish\"  \n [25] \"dog\"    \"monkey\" \"monkey\" \"monkey\" \"fish\"   \"fish\"   \"cat\"    \"fish\"  \n [33] \"fish\"   \"parrot\" \"dog\"    \"fish\"   \"dog\"    \"monkey\" \"monkey\" \"fish\"  \n [41] \"monkey\" \"dog\"    \"fish\"   \"cat\"    \"cat\"    \"fish\"   \"monkey\" \"fish\"  \n [49] \"fish\"   \"dog\"    \"fish\"   \"cat\"    \"parrot\" \"monkey\" \"parrot\" \"cat\"   \n [57] \"fish\"   \"monkey\" \"monkey\" \"fish\"   \"fish\"   \"monkey\" \"cat\"    \"dog\"   \n [65] \"parrot\" \"parrot\" \"monkey\" \"dog\"    \"monkey\" \"fish\"   \"cat\"    \"cat\"   \n [73] \"dog\"    \"dog\"    \"dog\"    \"cat\"    \"monkey\" \"monkey\" \"cat\"    \"dog\"   \n [81] \"monkey\" \"parrot\" \"dog\"    \"monkey\" \"monkey\" \"dog\"    \"fish\"   \"fish\"  \n [89] \"cat\"    \"fish\"   \"dog\"    \"cat\"    \"fish\"   \"cat\"    \"parrot\" \"monkey\"\n [97] \"dog\"    \"monkey\" \"fish\"   \"parrot\"\n\n\nThe decision to sample with, or without, replacement depends on the particular DGP you want to simulate.\nThe sample() function has an additional option which may be useful when generating random values. If you give prob a vector of values, sample() will draw samples from x in proportion to the values in prob. Each value in prob corresponds to the values in x that shares its position in the vector.\n\nmy_animals <- sample(\n  x = my_animals,\n  size = 1000,\n  replace = TRUE,\n  prob = c(0.1, 0.2, 0.2, 0.4, 0.1)\n)\n\nThe code above samples my_animals with 10% weight on cat, 20% weight on dog and fish, 40% weight on monkey, and 10% weight on parrot. We can confirm this worked by inspecting a bar graph of our my_animals data.\n\n# ggplot requires the data be in tibble or data.frame form\ntibble(my_animals) |> \n  ggplot(aes(x = my_animals)) +\n  geom_bar()\n\n\n\n\nSampling from Distributions\nThe sample() function is great for generating random data, but its use is limited by the fact that you have to manually specify the set of values x to draw from. For many common distributions of data (for example the Normal distribution), the set of possible values can range from negative infinity to positive infinity! To help us with this problem, base R comes with dozens of functions for drawing samples from well-known statistical distributions. The first one we will take a look at is rbinom() (r for “random” and binom for “binomial”).\nBinomial Distribution\nThe rbinom() function takes three inputs: n for the number of draws we want, size for the number of binomial trails, and prob for the probability of a “success”. Don’t worry if some of these parameters are unfamiliar, we will cover the binomial distribution in much greater detail in the Probability section of the course. The easiest introduction to the binomial distribution is to set size = 1 and prob = 0.5. This simulates flipping a coin and getting either a heads, 1, or tails, 0 with equal probability. The code below generates 100 coin flips in this way.\n\nrbinom(n = 100, size = 1, prob = 0.5)\n\n  [1] 1 0 0 1 1 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1\n [38] 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1\n [75] 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1\n\n\nWe can replicate the same data generating process using sample() with the following code.\n\nsample(x = c(0, 1),\n       size = 100,\n       replace = TRUE)\n\n  [1] 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1\n [38] 1 0 0 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1\n [75] 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1\n\n\nLet’s tweak the value for prob and see what happens.\n\nrbinom(n = 100, size = 1, prob = 0.8)\n\n  [1] 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1\n [75] 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0\n\n\nNow there are way more 1’s than 0’s! A bar graph can help us visualize the proportions.\n\nbinomial_draws <- rbinom(n = 100, size = 1, prob = 0.8)\n\ntibble(binomial_draws) |> \n  ggplot(aes(x = binomial_draws)) +\n  geom_bar() +\n  scale_x_continuous(breaks = 0:1) +\n  labs(title = \"Binomial Distribution: size = 1, prob = 0.8\")\n\n\n\n\nDifferentiating size from n in rbinom() can be difficult to understand at first, so let’s experiment with the code to help figure it out. When we set n = 100 and size = 1 we were drawing 100 values from a trial that only ran once. Instead, if we reverse these numbers, we would be drawing a single value from 100 trials.\n\nrbinom(n = 1, size = 100, prob = 0.5)\n\n[1] 49\n\n\nAs you can see, the code above gives us a single value which is the sum of 1’s from 100 trials. Because prob = 0.5, this number should be around 50.\nLike in our example of sampling animal names with sample(), it can be useful to create histograms to understand how sampling parameters affect the distribution of draws we get.\n\nbinomial_draws <- rbinom(n = 10000,\n                         size = 7,\n                         prob = 0.5)\n\nThe code chunk above creates an object called binomial_draws which holds 10,000 draws from a binomial distribution with seven trials and where the probability of a successful trial is 0.5. Let’s plot the values of binomial_draws.\n\ntibble(binomial_draws) |> \n  ggplot(aes(x = binomial_draws)) +\n  geom_bar() +\n  scale_x_continuous(breaks = 0:7) +\n  labs(title = \"Binomial Distribution: size = 7, prob = 0.5\")\n\n\n\n\nAs you can see, the results of our random draws cluster around 3 and 4. This is because the expected value of a binomial distribution is the total number of trials multiplied by the probability of a success. So that would be 7 * 0.5 = 3.5 in this example.\nNow let’s try changing prob to equal 0.8 in our simulation. The histogram below shows that the values of binomial_draws are clustered towards the higher end of the distribution with an average value of 7 * 0.8 = 5.6.\n\nbinomial_draws <- rbinom(n = 10000,\n                         size = 7,\n                         prob = 0.8)\n\ntibble(binomial_draws) |> \n  ggplot(aes(x = binomial_draws)) +\n  geom_bar() +\n  scale_x_continuous(breaks = 0:7) +\n  labs(title = \"Binomial Distribution: size = 7, prob = 0.8\")\n\n\n\n\nThe Uniform Distribution\nWhereas the binomial distribution only returns whole numbers when we sample from it, samples drawn from a uniform distribution can be any real number between a specified minimum and maximum. The uniform distribution is useful when you want to generate random numbers with equal weight from within a pre-determined range. To generate random samples from the uniform distribution we use runif(). Like rbinom(), the option n sets the number of random draws we want. We also need to specify our range by including a max and min.\n\n# Generating random numbers from between 0 and 1\nrunif(n = 25, min = 0, max = 1)\n\n [1] 0.54337171 0.71002642 0.48195880 0.22570595 0.44268945 0.95055935\n [7] 0.50057119 0.33930531 0.15806949 0.10024940 0.80346667 0.34544123\n[13] 0.58544779 0.29667138 0.51897068 0.52381265 0.86450114 0.69538650\n[19] 0.50264880 0.69550948 0.60846622 0.73475118 0.01994567 0.37766717\n[25] 0.12406208\n\n\nLet’s graph the random draws we get from a uniform distribution to better understand what’s taking place.\n\nuniform_draws <- runif(10000, min = 0, max = 1)\n\ntibble(uniform_draws) |> \n  ggplot(aes(x = uniform_draws)) +\n  geom_histogram(color = \"white\", bins = 30) +\n  xlim(-0.5, 1.5) +\n  labs(title = \"Unfiform Distribution: min = 0, max = 1\")\n\nWarning: Removed 2 rows containing missing values (geom_bar).\n\n\n\n\n\nThe histogram above shows that values between 0 and 1 are all about equally likely, but we won’t get any draws that are less than 0 or greater than 1.\nThe Normal Distribution\nThe normal distribution creates the familiar bell-shaped curve and has two parameters: the mean which controls the center of the distribution, and the standard deviation which controls the width of the curve. We often model variables as following a normal distribution when we believe they should be drawn from some central value plus or minus some random “noise”. Normal distributions arise naturally when the data-generating process involves the addition of many small fluctuations via some process.\nThe function for generating samples from a normal distribution, rnorm(), follows the same pattern we have seen with rbinom() and ruinf(). First we specify how many samples we want to draw n, then we specify the parameters mean and sd of the normal distribution we want to draw from.\n\nrnorm(n = 100, mean = 0, sd = 1)\n\n  [1]  0.701401693  0.563848586  0.083924053  0.583872090 -2.099774785\n  [6]  0.110344339 -0.787723239 -0.333248439  0.550758584  0.714693017\n [11]  0.120548638  0.150896298 -0.512840980  0.867312927 -0.346141983\n [16] -0.247085534 -0.641254732 -0.188211065 -0.683860328 -1.081965622\n [21]  0.977785717  1.160655145  0.162338679  0.782390816 -1.763856075\n [26] -0.564634301  2.213519318 -0.477646310 -0.004801969  0.094277717\n [31]  0.778655437  1.727779901  0.586957584 -0.967434894 -1.328433688\n [36] -0.245594927  1.629886013  0.590545573 -0.631178866 -1.501219305\n [41] -0.179582084  0.511943231  0.436229912 -0.490867638 -0.667737678\n [46]  1.111207564 -0.462655115  0.153127425 -0.763495384  0.504704055\n [51]  0.159434501 -2.095171559 -0.406393021  0.088125564  0.201007399\n [56] -0.628663068  1.759631869  1.150681772  0.375750165  0.938472032\n [61]  1.560115129 -1.156157677 -0.235717709  0.022263851 -0.532578110\n [66] -1.613624244  0.709041363  1.262709215  1.400904670 -0.910794053\n [71]  0.234888338 -0.795670369 -0.696388954 -0.580520291 -2.437246699\n [76] -0.599461865  1.868526238  0.239810630  1.930599107  1.818698435\n [81]  1.724502768 -0.939630249 -0.667630396 -0.304484103 -0.749651786\n [86]  0.260659673  2.084776203  0.774098464  0.478170287 -0.015065348\n [91]  0.755561815 -1.698254712 -0.392527625 -0.179050762  1.521890755\n [96]  0.767081993 -0.506149202 -0.718145288  0.889710784 -0.425267586\n\n\nLike the uniform distribution, draws from the normal distribution can be any real number. It may be difficult to see from the random draws above that values close to the mean of 0 are more likely than more extreme values. So let’s take a look at the distribution graphically.\n\nnormal_draws <- rnorm(10000, mean = 0, sd = 1)\n\ntibble(normal_draws) |> \n  ggplot(aes(x = normal_draws)) +\n  geom_histogram(color = \"white\", bins = 30) +\n  labs(title = \"Normal Distribution: mean = 0, sd = 1\")\n\n\n\n\nA normal distribution that looks like the one above (mean = 0, standard deviation = 1) is often called a “Standard Normal” distribution, and you will come across it all the time in statistics. Let’s take a look at some other normal distributions by varying their mean and standard deviation values.\n\n# rnorm defaults to mean = 0, sd = 1 if those options are left blank\nnormal_minus_one <- rnorm(10000, mean = -1)\nnormal_zero <- rnorm(10000)\nnormal_one <- rnorm(10000, mean = 1)\n\n# Combining the draws into a single \"long\" data frame\nnormal_data <- cbind(normal_minus_one,\n                     normal_zero,\n                     normal_one) |> \n  as_tibble() |> \n  pivot_longer(cols = everything(),\n               names_to = \"distribution\",\n               values_to = \"normal_draws\")\n\nnormal_data |> \n  ggplot(aes(x = normal_draws)) +\n  geom_histogram(color = \"white\", bins = 90) +\n  facet_wrap(~ distribution, nrow = 3)\n\n\n\n\nChanging the value for mean shifts the distribution along the x-axis but the spread of the data remains the same.\n\nnormal_narrow <- rnorm(10000, sd = 0.5)\nnormal_standard <- rnorm(10000)\nnormal_wide <- rnorm(10000, sd = 1.5)\n\nnormal_data <- cbind(normal_narrow,\n                     normal_standard,\n                     normal_wide) |> \n  as_tibble() |> \n  pivot_longer(cols = everything(),\n               names_to = \"distribution\",\n               values_to = \"normal_draws\")\n\nnormal_data |> \n  ggplot(aes(x = normal_draws)) +\n  geom_histogram(color = \"white\", bins = 90) +\n  facet_wrap(~ distribution, nrow = 3)\n\n\n\n\nChanging the value for only sd keeps each distribution centered at 0, but makes the data more or less spread out.\nSampling from Data\nWhile we’re discussing sampling to create fake data, it’s also handy to know how to sample from real data. Sometimes you will be working with data sets that are so large that you’ll want to test your code on smaller portions first. Or you may want to perform sampling on data when creating bootstrapped confidence intervals (a topic we will return to at the end of this chapter).\nLet’s read in the “county_elections.csv” data once again.\n\ncounty_elections <- read_csv(here(\"data\", \"county_elections.csv\"))\n\nRows: 3114 Columns: 27\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): state_abbrv, county\ndbl (25): countyCode, trump16, clinton16, otherpres16, romney12, obama12, ot...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nDplyr contains a function called sample_n() which lets you easily draw a number of random rows from your data.\n\ncounty_elections |> \n  sample_n(size = 10)\n\n# A tibble: 10 × 27\n   state_abbrv county    count…¹ trump16 clint…² other…³ romne…⁴ obama12 other…⁵\n   <chr>       <chr>       <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 WI          Winnebago   55139   43448   37054    6642   42122   45449    1602\n 2 SD          Custer      46033    3293    1121     307    3062    1335     123\n 3 SD          Hutchins…   46067    2517     692     156    2451     923      51\n 4 NC          Cleveland   37045   28479   14964    1230   25793   17062     485\n 5 TX          Burleson    48051    5316    1491     153    4671    1705      80\n 6 MT          Roosevelt   30085    1797    1560     295    1514    2086      72\n 7 IL          Henderson   17071    2155    1155     208    1541    1978      49\n 8 TN          Cheatham    47021   11297    3878     749   10268    4659     255\n 9 IL          Stephens…   17177   11083    7768    1492   10512   10165     378\n10 KS          Allen       20001    3651    1433     370    3316    1869     125\n# … with 18 more variables: total_population <dbl>, cvap <dbl>,\n#   white_pct <dbl>, black_pct <dbl>, hispanic_pct <dbl>, nonwhite_pct <dbl>,\n#   foreignborn_pct <dbl>, female_pct <dbl>, age29andunder_pct <dbl>,\n#   age65andolder_pct <dbl>, median_hh_inc <dbl>, clf_unemploy_pct <dbl>,\n#   lesshs_pct <dbl>, lesscollege_pct <dbl>, lesshs_whites_pct <dbl>,\n#   lesscollege_whites_pct <dbl>, rural_pct <dbl>, ruralurban_cc <dbl>, and\n#   abbreviated variable names ¹​countyCode, ²​clinton16, ³​otherpres16, …\n# ℹ Use `colnames()` to see all variable names\n\n\nAlternatively, we can use sample_frac() to specify how many random rows to draw based on a proportion of total rows.\n\ncounty_elections |> \n  sample_frac(size = 0.01) # Return 1% of the final data\n\n# A tibble: 31 × 27\n   state_abbrv county    count…¹ trump16 clint…² other…³ romne…⁴ obama12 other…⁵\n   <chr>       <chr>       <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 TN          Giles       47055    7970    2917     250    6915    3760     124\n 2 OK          Kay         40071   12172    3738     893   11499    4627       0\n 3 ID          Bonner      16017   13343    5819    1781   11367    6500     669\n 4 NY          Cattarau…   36009   19692    9497    1972   14655   12040    3072\n 5 OK          Logan       40083   13633    4248    1098   12314    4724       0\n 6 TN          Blount      47009   37443   12100    2665   35441   12934     859\n 7 AL          Covington    1039   13267    2387     286   12153    3158     112\n 8 NE          Knox        31107    3188     720     207    2885    1059      92\n 9 TX          Tyler       48457    6624    1248     144    5910    1668      76\n10 AR          Pulaski      5119   61257   89574    8945   68984   87248    3149\n# … with 21 more rows, 18 more variables: total_population <dbl>, cvap <dbl>,\n#   white_pct <dbl>, black_pct <dbl>, hispanic_pct <dbl>, nonwhite_pct <dbl>,\n#   foreignborn_pct <dbl>, female_pct <dbl>, age29andunder_pct <dbl>,\n#   age65andolder_pct <dbl>, median_hh_inc <dbl>, clf_unemploy_pct <dbl>,\n#   lesshs_pct <dbl>, lesscollege_pct <dbl>, lesshs_whites_pct <dbl>,\n#   lesscollege_whites_pct <dbl>, rural_pct <dbl>, ruralurban_cc <dbl>, and\n#   abbreviated variable names ¹​countyCode, ²​clinton16, ³​otherpres16, …\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nPerforming stratified sampling is very easy using the group_by() function we introduced in the last chapter. The code below draws one random row (i.e. county) from each US state.\n\ncounty_elections |> \n  group_by(state_abbrv) |> \n  sample_n(1)\n\n# A tibble: 50 × 27\n# Groups:   state_abbrv [50]\n   state_abbrv county    count…¹ trump16 clint…² other…³ romne…⁴ obama12 other…⁵\n   <chr>       <chr>       <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 AL          Elmore       1051   27634    8443    1183   26253    8954     278\n 2 AR          Sharp        5135    5407    1472     370    4921    2092     270\n 3 AZ          Cochise      4003   28092   17450    4473   29497   18546     960\n 4 CA          Napa         6055   17411   39199    4762   19526   35870    1572\n 5 CO          Montezuma    8083    7853    3973    1032    7401    4542     375\n 6 CT          Tolland      9013   34194   38506    4724   19768   25957     863\n 7 DC          District…   11001   12723  282830   15715   21381  267070    5313\n 8 DE          Sussex      10005   62611   39333    4064   52119   39975    1135\n 9 FL          Marion      12083  107833   62041    4826   93043   66831    1511\n10 GA          Fannin      13111    9632    1923     257    7857    2028     132\n# … with 40 more rows, 18 more variables: total_population <dbl>, cvap <dbl>,\n#   white_pct <dbl>, black_pct <dbl>, hispanic_pct <dbl>, nonwhite_pct <dbl>,\n#   foreignborn_pct <dbl>, female_pct <dbl>, age29andunder_pct <dbl>,\n#   age65andolder_pct <dbl>, median_hh_inc <dbl>, clf_unemploy_pct <dbl>,\n#   lesshs_pct <dbl>, lesscollege_pct <dbl>, lesshs_whites_pct <dbl>,\n#   lesscollege_whites_pct <dbl>, rural_pct <dbl>, ruralurban_cc <dbl>, and\n#   abbreviated variable names ¹​countyCode, ²​clinton16, ³​otherpres16, …\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nset.seed()\nIf you have been running the code chunks above yourself, you may have noticed that random values you got differed from those printed in this chapter. In fact, each time you run a function like rnorm(n = 100, mean = 0, sd = 1) you will get different results. The function set.seed() is R’s way of making sure that the results of your random simulations are the same for others, and for yourself if you re-run your code. This is an important part of making sure the results of your statistical analyses are reproducible.\nSimply put any number inside set.seed() and R’s randomness will become tied to that particular number.\n\nset.seed(10071992)\n\nsample(my_animals, 3)\n\n[1] \"monkey\" \"dog\"    \"parrot\"\n\n\nRunning the chunk of code above should output \"parrot\" \"cat\" \"monkey\" each time it is run and across different computers, or different versions of R."
  },
  {
    "objectID": "14_loops_simulations.html#loops",
    "href": "14_loops_simulations.html#loops",
    "title": "4  Loops and Simulation",
    "section": "\n4.2 Loops",
    "text": "4.2 Loops\nLoops are handy tools for running the same process repeatedly in R. For example, let’s say we wanted to print out the name of every animal in our vector of my_animals. The tedious way would be to type out the print() function for each animal as in the code below.\n\nprint(\"cat\")\n\n[1] \"cat\"\n\nprint(\"dog\")\n\n[1] \"dog\"\n\nprint(\"monkey\")\n\n[1] \"monkey\"\n\nprint(\"parrot\")\n\n[1] \"parrot\"\n\nprint(\"fish\")\n\n[1] \"fish\"\n\n\nBut with loops we can perform the same task in a much more concise fashion.\n\nmy_animals <- c(\"cat\", \"dog\", \"monkey\", \"parrot\", \"fish\")\n\nfor (animal in my_animals) {\n  print(animal)\n}\n\n[1] \"cat\"\n[1] \"dog\"\n[1] \"monkey\"\n[1] \"parrot\"\n[1] \"fish\"\n\n\nThe basic structure of a loop is shown in the code chunk below. The object set_of_things should be some object in R which holds many values (such as a vector, a data frame, a list, or a matrix). The thing object refers to each element in set_of_things and changes its value each time the loop goes through a single iteration. Typically you will refer to thing in the body of your loop, such as in the function do_something_to(). A loop structured like this runs until every element in set_of_things is iterated through.\n\nfor (thing in set_of_things) {\n  do_something_to(thing)\n}\n\nIt is very common to use loops as a way to run a set of R commands N times, where N is some integer greater than 1. The easiest way to do this is to create a sequence of numbers using : as shown below.\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nThe object 1:10 is then what we will insert into the set_of_things position in our loop in order to iterate through each number in the sequence. When you construct a loop in this manner you should name your thing object i. This is the standard style taken from mathematical indexing, and helps readers of your code better understand the purpose of your loop.\n\nfor (i in 1:10) {\n  i <- i ^ 2\n  print(i)\n}\n\n[1] 1\n[1] 4\n[1] 9\n[1] 16\n[1] 25\n[1] 36\n[1] 49\n[1] 64\n[1] 81\n[1] 100\n\n\nSo far our loops have only printed out their results in the console, but have not given us much to work with afterwards. To save the output of your loop you will typically need to initialize some “holder” object ahead of running the loop. Then, within each iteration of the loop, you can save the result inside the “holder” object.\n\nholder <- NULL # Use NULL to create an empty object\nfor (i in 1:100) {\n  holder[i] <- i ^ 2\n}\n\nThe code chunk above iterates through each number from 1 to 100, squares that number, then saves it in the ith position of the holder object. We can then take the vector of values stored in holder and access them as we would any object saved in our R Environment.\n\nholder\n\n  [1]     1     4     9    16    25    36    49    64    81   100   121   144\n [13]   169   196   225   256   289   324   361   400   441   484   529   576\n [25]   625   676   729   784   841   900   961  1024  1089  1156  1225  1296\n [37]  1369  1444  1521  1600  1681  1764  1849  1936  2025  2116  2209  2304\n [49]  2401  2500  2601  2704  2809  2916  3025  3136  3249  3364  3481  3600\n [61]  3721  3844  3969  4096  4225  4356  4489  4624  4761  4900  5041  5184\n [73]  5329  5476  5625  5776  5929  6084  6241  6400  6561  6724  6889  7056\n [85]  7225  7396  7569  7744  7921  8100  8281  8464  8649  8836  9025  9216\n [97]  9409  9604  9801 10000\n\n\nAlternatively, we could write the functionally-equivalent code chunk below.\n\nholder <- NULL\nfor (i in 1:100) {\n  holder <- c(holder, i ^ 2)\n}\n\nHere we are using the c() function to combine the current value of the loop’s iteration with everything that came before it in the holder object. This method is more flexible than indexing with [i], but can be computationally slower to run because the holder object is re-writen by R each time the loop iterates."
  },
  {
    "objectID": "14_loops_simulations.html#simulations",
    "href": "14_loops_simulations.html#simulations",
    "title": "4  Loops and Simulation",
    "section": "\n4.3 Simulations",
    "text": "4.3 Simulations\nIn the final section of this chapter we will look at a few examples of simulations that have real-world applications. The basic idea behind each of these simulations is to use R’s random sampling functions (such as rnorm() or runif()) within a loop to generate hundreds or thousands of synthetic data points. In the real world we only get one realization of our data to analyze, but common statistical techniques are based on imagining large numbers of repeated samples of data from the same population. Simulations built in R can therefore help us approximate this process.\nCentral Limit Theorem\nThe Central Limit Theorem (CLT) is invoked all the time in frequentist statistical methods. In lay terms, it states that if we calculate the means of many samples from a single population, the distribution of these means will be normally distributed. And this is true regardless (with exceptions) of what the distribution of the population looks like. If this sounds like magic to you, let’s use a simulation to prove it.\n\nn_sims <- 1000\nmeans <- NULL\n\nFirst we will set the number of simulations we want to run with n_sims <- 1000. Generally, running more simulations will produce more accurate results, but doing so will also take more computation time. The object means will be our “holder” object used to store the output of each loop iteration. We set its value to NULL initially because it starts empty.\n\nfor (i in 1:n_sims) {\n  sample_data <- runif(100, min = -1, max = 1)\n  means[i] <- mean(sample_data)\n}\n\nThe loop above contains everything we need to simulate the CLT in action. By setting for (i in 1:n_sims) we make the loop run n_sims times. Within each iteration we generate a new vector of fake data called sample_data which contains 100 draws of a uniform distribution whose range is from -1 to 1. We then take the average value of sample_data with the mean() function and store that into the ith position of the means holder object.\nNow let’s graph our results!\n\ntibble(means) |> \n  ggplot(aes(x = means)) +\n  geom_histogram(color = \"white\", bins = 30) +\n  labs(title = \"Distribution of Means from Simulated Uniform Random Samples\")\n\n\n\n\nAs you can see, the distribution of mean values inside means follow the familiar bell-shaped curve of a normal distribution—despite the fact that the random samples were drawn from a uniform distribution. Amazing!\nPower Analysis\nPower analysis is a method used to figure out how big of a sample you need before running an experiment. Smaller sample sizes make it more difficult to detect an effect of some treatment, but budget constraints often limit how large of a sample we can acquire. To perform a power analysis, you need to have an idea of the possible effect sizes of your treatment, as well as your false-negative threshold (i.e. “if there really is an effect from the treatment, what proportion of times should my experiment detect it?”). Statisticians have developed fancy formulas for calculating optimal sample sizes based on these inputs. But it is much easier to instead write R code which simulates running your particular experiment thousands of times.\n\nn_sims <- 10000\np_values <- NULL\neffect <- 0.5\nsample_size <- 50\n\nAs in the Central Limit Theorem example, we will set n_sims <- 10000. Our “holder” object will be called p_values this time because that is what we will be extracting from every iteration of our loop below. Lastly we need to specify a treatment effect, effect and sample size for our experiment sample_size. The beauty of running simulations in R is that we can easily play around with different values of effect and sample_size and check out the new results.\n\n# Setting seed so we all get the same results\nset.seed(10071992)\n\nfor (i in 1:n_sims) {\n  treated <- rnorm(sample_size / 2, mean = effect)\n  control <- rnorm(sample_size / 2, mean = 0)\n  \n  power_test <- t.test(treated, control)\n  p_values[i] <- power_test$p.value\n}\n\nThe loop above simulates a possible experimental design. First we create two vectors of outcomes, one for the treated group and one for our control group. The number of samples from each group is our sample_size divided by two, and each group is drawn from a normal distribution using rnorm(). However, you will notice that the treated group is being drawn from a normal distribution with mean equal to our specified effect of 0.5, whereas the control group is drawn from a normal distribution with mean = 0. In our simulated experiment, we know that the treatment has some effect because we generated the data to ensure it does! The normal distribution for each group has a standard deviation of one (the default in rnorm()). The variation in outcomes has major implications for our power analysis, but we will omit dealing with that here for simplicity.\nAfter generating sample outcomes for our treated and control groups, we perform a simple t-test to compare the means of each group and store the results in an object called power_test. Then we extract the p-value from the t-test using $p.value and save it into the ith position of the p_values holder object. If power_test$p.value is less than the conventional level of 0.05, then the simulated experiment has successfully detected the effect of the treatment. With our vector of 10,000 p_values in hand, we can now calculate the proportion of them which are below 0.05 using the mean() function.\n\nmean(p_values < 0.05)\n\n[1] 0.4038\n\n\nIt looks like our experiment (with effect size of 0.5 and sample size of 50) was only able to detect a true effect of the treatment about 40% of the time. Ideally we would like this number to above 80% so we will probably need to recruit more than 50 participants for our experiment if we hypothesize that the treatment has a 0.5 standardized effect.\nBootstrapping\nBootstrapping is a technique used to calculate uncertainty regions around our statistical estimates. It approximates a confidence interval by re-sampling the underlying data (with replacement) thousands of times and refitting the statistical model to each of these samples. Bootstrapping is useful when calculating confidence intervals analytically is challenging or impossible (which may occur for complicated models).\nIn this example we will be looking at the coefficient estimate for a linear regression with median household income as the dependent variable and high school percentage as the independent variable. Recall that these variables come from the county_elections data.\n\nn_sims <- 1000\nestimates <- NULL\n\nWe will set n_sims to only 1,000 here because the re-sampling process can take a while. The holder estimates will be used to store the coefficient estimates from our linear regression.\n\nfor (i in 1:n_sims) {\n  county_sample <- county_elections |> \n    sample_n(size = nrow(county_elections), replace = TRUE)\n  \n  lm_model <- lm(median_hh_inc ~ lesshs_pct,\n                 data = county_sample)\n  estimates[i] <- summary(lm_model)$coef[[2]]\n}\n\nThe first step of our simulation is to create a county_sample object which contains random rows from the main county_elections data set. Inside sample_n() we set size = nrow(county_elections) to generate a sample which has the same number of rows as the original data set. Crucially, we also set replace = TRUE so that it is possible to include the same row multiple times in our sampled data. If you think it seems weird to replicate rows like this, you’re not alone! Yet somehow this procedure works to produce valid confidence regions around the estimated coefficient.\nNext we create an object lm_model which stores the linear regression fit on the county_sample data. We can extract the coefficient/slope between lesshs_pct and median_hh_inc using the verbose summary(lm_model)$coef[[2]] (for easier ways to extract regression coefficients see the broom package).\nFinally let’s take a look at all these coefficient values stored in estimates graphically.\n\nbootstrap_plot <- tibble(estimates) |> \n  ggplot(aes(x = estimates)) +\n  geom_histogram(color = \"black\",\n                 fill = \"white\",\n                 bins = 30) +\n  labs(x = \"Bootstrapped Estimates of the Regression Coefficient\") +\n  theme_bw()\n\nbootstrap_plot\n\n\n\n\nSometimes it is handy to save a ggplot graph in an object like we do with bootstrap_plot. This will allow us to add on subsequent geoms without re-writing the entire ggplot code.\n\nlm_model <- lm(median_hh_inc ~ lesshs_pct,\n               data = county_elections)\nestimate <- summary(lm_model)$coef[[2]]\nstandard_error <- summary(lm_model)$coef[[4]]\n\nHow do our simulated bootstrapped estimates, fit on random samples of the county_elections data, compare to the results of a model fit to the original data set? The code chunk above runs the linear regression model on the full county_elections data and saves the single estimated coefficient and standard error.\n\nbootstrap_plot +\n  geom_vline(xintercept = estimate,\n             color = \"maroon\", size = 1.5) +\n  geom_vline(xintercept = estimate + standard_error * 1.96,\n             color = \"darkcyan\", size = 1.5, linetype = \"dashed\") +\n  geom_vline(xintercept = estimate - standard_error * 1.96,\n             color = \"darkcyan\", size = 1.5, linetype = \"dashed\") +\n  labs(caption = \"Full sample estimate in maroon\\n95% confidence interval in cyan\")\n\n\n\n\nAdding these values to our bootstrap_plot using + geom_vline() we can see that the bootstrapped estimates correspond neatly to the full-data model! The center of the bootstrapped distribution is right on the full-data model estimate of the coefficient, and roughly 95% of our simulated results fall within \\pm 1.96 \\times \\mathrm{StandardError} (the definition of a confidence interval for a linear regression coefficient)."
  },
  {
    "objectID": "15_non-wysiwyg.html",
    "href": "15_non-wysiwyg.html",
    "title": "5  LaTeX and Markdown",
    "section": "",
    "text": "Up till now, you should have covered:\n\nStatistical Programming in R\n\n\nThis is only the beginning of R – programming is like learning a language, so learn more as we use it. And yet R is of likely not the only programming language you will want to use. While we cannot introduce everything, we’ll pick out a few that we think are particularly helpful.\nHere will cover\n\nMarkdown\nLaTeX (and BibTeX)\n\nas examples of a non-WYSIWYG editor\ncommand-line are a basic set of tools that you may have to use from time to time. It also clarifies what more complicated programs are doing. Markdown is an example of compiling a plain text file. LaTeX is a typesetting program and git is a version control program – both are useful for non-quantitative work as well.\nPlease familiarize yourself closing with Markdown, and be sure you know how to open an .Rmd file as described below. In class, we will walk through an Rmd file together. LaTeX is included here for your future reference as this is a popular typesetting program among political scientists. This is not needed for Math Camp and is never required for any course. In fact, many prefer R Markdown’s integration rather than a separate typesetting program. This depends on your background and interests but exposure to the range of popular programs and techniques will be helpful moving forward."
  },
  {
    "objectID": "15_non-wysiwyg.html#motivation",
    "href": "15_non-wysiwyg.html#motivation",
    "title": "5  LaTeX and Markdown",
    "section": "\n5.1 Motivation",
    "text": "5.1 Motivation\nStatistical programming is a fast-moving field. The beta version of R was released in 2000, ggplot2 was released on 2005, and RStudio started around 2010. Of course, some programming technologies are quite “old”: (C in 1969, C++ around 1989, TeX in 1978, Linux in 1991, Mac OS in 1984). But it is easy to feel you are falling behind in the recent developments of programming. Today we will do a brief and rough overview of some fundamental and new tools other than R, with the general aim of having you break out of your comfort zone so you won’t be shut out from learning these tools in the future."
  },
  {
    "objectID": "15_non-wysiwyg.html#markdown",
    "href": "15_non-wysiwyg.html#markdown",
    "title": "5  LaTeX and Markdown",
    "section": "\n5.2 Markdown",
    "text": "5.2 Markdown\nAt its core markdown is just plain text. Plain text does not have any formatting embedded in it. Instead, the formatting is coded up as text. Markdown is not a WYSIWYG (What you see is what you get) text editor like Microsoft Word or Google Docs. This will mean that you need to explicitly code for bold{text} rather than hitting Command+B and making your text look bold on your own computer.\nMarkdown is known as a “light-weight” editor, which means that it is relatively easy to write code that will compile. It is quick and easy and satisfies most presentation purposes; you might want to try LaTeX for more involved papers.\n\n5.2.1 markdown commands\nFor italic and bold, use either the asterisks or the underlines,\n*italic*   **bold**\n_italic_   __bold__\nAnd for headers use the hash symbols,\n# Main Header\n## Sub-headers\n\n5.2.2 your own markdown\nRStudio makes it easy to compile your very first markdown file by giving you templates. Got to New > R Markdown, pick a document and click Ok. This will give you a skeleton of a document you can compile – or “knit”.\nRmd is actually a slight modification of real markdown. It is a type of file that R reads and turns into a proper md file. Then, it uses a document-conversion called pandoc to compile your md into documents like PDF or HTML.\n\n\nHow Rmds become PDFs or HTMLs\n\n\n\n5.2.3 A note on plain-text editors\nMultiple software exist where you can edit plain-text (roughly speaking, text that is not WYSIWYG).\n\nRStudio (especially for R-related links)\nTeXMaker, TeXShop (especially for TeX)\n\nemacs, aquamacs (general)\n\nvim (general)\n\nVS Code (general)\n\nSublime Text (general)\n\nAtom (general)\n\nEach has their own keyboard shortcuts and special features. You can browse a couple and see which one(s) you like."
  },
  {
    "objectID": "15_non-wysiwyg.html#latex",
    "href": "15_non-wysiwyg.html#latex",
    "title": "5  LaTeX and Markdown",
    "section": "\n5.3 LaTeX",
    "text": "5.3 LaTeX\nLaTeX is a typesetting program. You’d engage with LaTeX much like you engage with your R code. You will interact with LaTeX in a text editor, and will writing code which will be interpreted by the LaTeX compiler and which will finally be parsed to form your final PDF.\n\n5.3.1 compile online\n\nGo to https://www.overleaf.com\n\nScroll down and go to “CREATE A NEW PAPER” if you don’t have an account.\nLet’s discuss the default template.\nMake a new document, and set it as your main document. Then type in the Minimal Working Example (MWE):\n\n\n\\documentclass{article}\n\\begin{document}\nHello World\n\\end{document}\n\n\n5.3.2 compile your first LaTeX document locally\nLaTeX is a very stable system, and few changes to it have been made since the 1990s. The main benefit: better control over how your papers will look; better methods for writing equations or making tables; overall pleasing aesthetic.\n\nOpen a plain text editor. Then type in the MWE\n\n\n\\documentclass{article}\n\\begin{document}\nHello World\n\\end{document}\n\n\nSave this as hello_world.tex. Make sure you get the file extension right.\nOpen this in your “LaTeX” editor. This can be TeXMaker, Aqumacs, etc..\nGo through the click/dropdown interface and click compile.\n\n5.3.3 main LaTeX commands\nLaTeX can cover most of your typesetting needs, to clean equations and intricate diagrams.\nSome main commands you’ll be using are below, and a very concise cheat sheet here: https://wch.github.io/latexsheet/latexsheet.pdf\nMost involved features require that you begin a specific “environment” for that feature, clearly demarcating them by the notation \\begin{figure} and then \\end{figure}, e.g. in the case of figures.\n\\begin{figure}\n\\includegraphics{histogram.pdf}\n\\end{figure}\nwhere histogram.pdf is a path to one of your files.\nNotice that each line starts with a backslash \\ – in LaTeX this is the symbol to run a command.\nThe following syntax at the endpoints are shorthand for math equations.\n\\[\\int x^2 dx\\]\nthese compile math symbols: \\displaystyle \\int x^2 dx.1\nThe align environment is useful to align your multi-line math, for example.\n\\begin{align}\nP(A \\mid B) &= \\frac{P(A \\cap B)}{P(B)}\\\\\n&= \\frac{P(B \\mid A)P(A)}{P(B)}\n\\end{align}\n\\begin{align}\nP(A \\mid B) &= \\frac{P(A \\cap B)}{P(B)}\\\\\n&= \\frac{P(B \\mid A)P(A)}{P(B)}\n\\end{align}\nRegression tables should be outputted as .tex files with packages like xtable and stargazer, and then called into LaTeX by \\input{regression_table.tex} where regression_table.tex is the path to your regression output.\nFigures and equations should be labelled with the tag (e.g. label{tab:regression} so that you can refer to them later with their tag Table \\ref{tab:regression}, instead of hard-coding Table 2).\nFor some LaTeX commands you might need to load a separate package that someone else has written. Do this in your preamble (i.e. before \\begin{document}):\n\\usepackage[options]{package}\nwhere package is the name of the package and options are options specific to the package.\nFurther Guides\nFor a more comprehensive listing of LaTeX commands, Mayya Komisarchik has a great tutorial set of folders: https://scholar.harvard.edu/mkomisarchik/tutorials-0\nThere is a version of LaTeX called Beamer, which is a popular way of making a slideshow. Slides in markdown is also a competitor. The language of Beamer is the same as LaTeX but has some special functions for slides."
  },
  {
    "objectID": "15_non-wysiwyg.html#bibtex",
    "href": "15_non-wysiwyg.html#bibtex",
    "title": "5  LaTeX and Markdown",
    "section": "\n5.4 BibTeX",
    "text": "5.4 BibTeX\nBibTeX is a reference system for bibliographical tests. We have a .bib file separately on our computer. This is also a plain text file, but it encodes bibliographical resources with special syntax so that a program can rearrange parts accordingly for different citation systems.\n\n5.4.1 what is a .bib file?\nFor example, here is the Nunn and Wantchekon article entry in .bib form.\n@article{nunn2011slave,\n  title={The Slave Trade and the Origins of Mistrust in Africa},\n  author={Nunn, Nathan and Wantchekon, Leonard},\n  journal={American Economic Review},\n  volume={101},\n  number={7},\n  pages={3221--3252},\n  year={2011}\n}\nThe first entry, nunn2011slave, is “pick your favorite” – pick your own name for your reference system. The other slots in this @article entry are entries that refer to specific bibliographical text.\n\n5.4.2 what does LaTeX do with .bib files?\nNow, in LaTeX, if you type\n  \\textcite{nunn2011slave} argue that current variation in the trust among citizens of African countries has historical roots in the European slave trade in the 1600s.\n  \nas part of your text, then when the .tex file is compiled the PDF shows something like\n\nin whatever citation style (APSA, APA, Chicago) you pre-specified!\nAlso at the end of your paper you will have a bibliography with entries ordered and formatted in the appropriate citation.\n\nThis is a much less frustrating way of keeping track of your references – no need to hand-edit formatting the bibliography to conform to citation rules (which biblatex already knows) and no need to update your bibliography as you add and drop references (biblatex will only show entries that are used in the main text).\n\n5.4.3 stocking up on your .bib files\nYou should keep your own .bib file that has all your bibliographical resources. Storing entries is cheap (does not take much memory), so it is fine to keep all your references in one place (but you’ll want to make a new one for collaborative projects where multiple people will compile a .tex file).\nFor example, Gary’s BibTeX file is here: https://github.com/iqss-research/gkbibtex/blob/master/gk.bib\nCitation management software (Mendeley or Zotero) automatically generates .bib entries from your library of PDFs for you, provided you have the bibliography attributes right."
  },
  {
    "objectID": "15_non-wysiwyg.html#extension-optional-exercise",
    "href": "15_non-wysiwyg.html#extension-optional-exercise",
    "title": "5  LaTeX and Markdown",
    "section": "Extension: Optional Exercise",
    "text": "Extension: Optional Exercise\nCreate a LaTeX document for a hypothetical research paper on your laptop and, once you’ve verified it compiles into a PDF, come show it to either one of the instructors.\nYou can also use overleaf if you have preference for a cloud-based system. But don’t swallow the built-in templates without understanding or testing them.\nEach student will have slightly different substantive interests, so we won’t impose much of a standard. But at a minimum, the LaTeX document should have:\n\nA title, author, date, and abstract\nSections\nItalics and boldface\nA figure with a caption and in-text reference to it.\n\nDepending on your subfield or interests, try to implement some of the following:\n\nA bibliographical reference drawing from a separate .bib file\nA table\nA math expression\nA different font\nDifferent page margins\nDifferent line spacing"
  },
  {
    "objectID": "21_vector_matrix.html",
    "href": "21_vector_matrix.html",
    "title": "\n6  Matrix Operations\n",
    "section": "",
    "text": "Vector: A vector in n-space is an ordered list of n numbers. These numbers can be represented as either a row vector or a column vector:\n{\\bf v} = \\begin{bmatrix} v_1 & v_2 & \\dots & v_n\\end{bmatrix}\n{\\bf v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\nWe can also think of a vector as defining a point in n-dimensional space, usually \\mathbb{R}^n; each element of the vector defines the coordinate of the point in a particular direction.\nVector Addition and Subtraction: If two vectors, {\\bf u} and {\\bf v}, have the same size (i.e. have the same number of elements), they can be added (subtracted) together:\n{\\bf u} + {\\bf v} = \\begin{bmatrix} u_1 + v_1 \\\\ u_2 + v_2 \\\\ \\vdots \\\\ u_k + v_n \\end{bmatrix}\n{\\bf u} - {\\bf v} = \\begin{bmatrix} u_1 - v_1 \\\\ u_2 - v_2 \\\\ \\vdots \\\\ u_k - v_n \\end{bmatrix}\nGeometrically, vector addition is witnessed by placing the two vectors, \\mathbf{a} and \\mathbf{b}, tail-to-head. The result, \\mathbf{a}+\\mathbf{b}, is the vector from the open tail to the open head. This is demonstrated in Figure 6.1.\n\n\n\n\nFigure 6.1: Geometry of Vector Addition (Source)\n\n\n\n\nWhen subtracting vectors as \\mathbf{a}-\\mathbf{b} we simply add -\\mathbf{b} to \\mathbf{a}. The vector -\\mathbf{b} has the same length as \\mathbf{b} but points in the opposite direction. This vector has the same length as the one which connects the two heads of \\mathbf{a} and \\mathbf{b} as shown in Figure 6.2.\n\n\n\n\nFigure 6.2: Geometry of Vector Subtraction (Source)\n\n\n\n\nScalar Multiplication: The product of a scalar c (i.e. a constant) and vector {\\bf v} is:\n c{\\bf v} =  \\begin{bmatrix} cv_1 \\\\ cv_2 \\\\ \\dots \\\\ cv_n \\end{bmatrix} \nScalar multiplication changes the length of a vector but not the overall direction (although a negative scalar will scale the vector in the opposite direction through the origin). We can see this geometric interpretation of scalar multiplication in Figure 6.3.\n\n\n\n\nFigure 6.3: Geometric Effect of Scalar Multiplication (Source)"
  },
  {
    "objectID": "21_vector_matrix.html#special-vectors",
    "href": "21_vector_matrix.html#special-vectors",
    "title": "\n6  Matrix Operations\n",
    "section": "\n6.2 Special Vectors",
    "text": "6.2 Special Vectors\nZero Vector: The zero vector is a vector of all zeros. It is the additive identity for vectors. That is, if {\\bf v} is any vector, then {\\bf v} + {\\bf 0} = {\\bf v}. The zero vector is denoted by {\\bf 0}.\nUnit Vectors: A unit vector is a vector of length 1. There are many unit vectors, one for each dimension. For example, in \\mathbb{R}^3, there are three unit vectors:\n{\\bf e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, {\\bf e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\;\\;\\mathrm{ and }\\;\\; {\\bf e}_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}.\nThese are the unit vectors in the x, y, and z directions, respectively. Unit vectors are useful for representing directions in space."
  },
  {
    "objectID": "21_vector_matrix.html#linear-combinations",
    "href": "21_vector_matrix.html#linear-combinations",
    "title": "\n6  Matrix Operations\n",
    "section": "\n6.3 Linear Combinations",
    "text": "6.3 Linear Combinations\n\n\nLinear combinations: The vector {\\bf u} is a linear combination of the vectors {\\bf v}_1, {\\bf v}_2, \\cdots , {\\bf v}_k if\n{\\bf u} = c_1{\\bf v}_1 + c_2{\\bf v}_2 +  \\cdots + c_k{\\bf v}_k\nFor example, \\begin{bmatrix}9 \\\\ 13 \\\\ 17 \\end{bmatrix} is a linear combination of the following three vectors: \\begin{bmatrix}1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\begin{bmatrix} 2 \\\\ 3\\\\ 4\\end{bmatrix}, and \\begin{bmatrix} 3 \\\\ 4 \\\\ 5 \\end{bmatrix}. This is because\n\\begin{bmatrix}9 \\\\ 13 \\\\ 17 \\end{bmatrix} = (2)\\begin{bmatrix}1 \\\\ 2 \\\\ 3 \\end{bmatrix} + (-1)\\begin{bmatrix} 2 \\\\ 3\\\\ 4\\end{bmatrix} + 3\\begin{bmatrix} 3 \\\\ 4 \\\\ 5 \\end{bmatrix}.\nNotice that you can always represent one vector into linear combinations of the unit vectors, where the coefficients are the elements in each coordinates. For example, \\begin{bmatrix} 9 \\\\ 13 \\\\ 17 \\end{bmatrix} is a linear combination of the unit vectors \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, and \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}, with coefficients 9, 13, and 17, respectively."
  },
  {
    "objectID": "21_vector_matrix.html#matrixbasics",
    "href": "21_vector_matrix.html#matrixbasics",
    "title": "\n6  Matrix Operations\n",
    "section": "\n6.4 Matrix",
    "text": "6.4 Matrix\nMatrix: A matrix of size m\\times n is an array of real numbers arranged in m rows by n columns. The dimensionality of the matrix is defined as the number of rows by the number of columns, m \\times n.\n{\\bf A}=\\begin{bmatrix}\n    A_{11} & A_{12} & \\cdots & A_{1n} \\\\\n    A_{21} & A_{22} & \\cdots & A_{2n} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    A_{m1} & A_{m2} & \\cdots & A_{mn}\n\\end{bmatrix}\nThe element of matrix \\mathbf{A} corresponding to row i and column j is written A_{ij}.\nNote that you can think of vectors as special cases of matrices; a column vector of length k is a k \\times 1 matrix, while a row vector of the same length is a 1 \\times k matrix.\n\nExample 6.1 (Matrix Dimensions) Consider the following data matrix, containing observations on the two variables Credit Score and Income:\n\\mathbf{A}=\\begin{bmatrix} 780 & 95000\\\\ 600 & 60000\\\\ 550 & 65000\\\\ 400 & 35000\\\\ 450 & 40000\\\\ 750 & 80000\\end{bmatrix}\nThe dimension of the matrix \\mathbf{A} is 6\\times 2 because \\mathbf{A} has 6 rows and 2 columns. Thus when referring to \\mathbf{A} we might write \\mathbf{A}_{6\\times 2} when the size is important. Note that the number of rows always comes first when specifying the size of a matrix!\n\nIt’s also useful to think of matrices as being made up of a collection of row or column vectors.\nWhen we write\n\\mathbf{A}=[\\mathbf{a}_1 | \\mathbf{a}_2 | \\dots | \\mathbf{a}_n ]\nwe are viewing the matrix \\mathbf{A} as collection of column vectors, \\mathbf{a}_j, in the following way:\n\\mathbf{A}=[ \\mathbf{a}_1 | \\mathbf{a}_2 | \\dots | \\mathbf{a}_n ]=\\left[\\begin{matrix} \\uparrow & \\uparrow &\\uparrow&\\dots & \\uparrow \\\\ \\mathbf{a}_1&\\mathbf{a}_2&\\mathbf{a}_3&\\dots&\\mathbf{a}_n \\\\ \\downarrow &\\downarrow &\\downarrow &\\dots&\\downarrow   \\end{matrix}\\right].\nSimilarly, we can write \\mathbf{A} as a collection of row vectors:\n\\mathbf{A}=\\left[\\begin{matrix} \\mathbf{a}_1 \\\\ \\mathbf{a}_2 \\\\ \\vdots \\\\  \\mathbf{a}_m \\end{matrix}\\right] =  \\left[\\begin{matrix} \\longleftarrow & \\mathbf{a}_1 & \\longrightarrow \\\\ \\longleftarrow & \\mathbf{a}_2 & \\longrightarrow \\\\ \\vdots & \\vdots & \\vdots \\\\ \\longleftarrow & \\mathbf{a}_m & \\longrightarrow \\end{matrix}\\right].\nSometimes, we will want to refer to both rows and columns in the same context. In these situations, we may use \\mathbf{A}_{i \\star} to reference the i-th row and \\mathbf{A}_{\\star j} to reference the j-th column.\nMatrix Addition: Let \\bf A and \\bf B be two m\\times n matrices.\n\\mathbf{A+B}=\\begin{bmatrix} a_{11}+b_{11} & a_{12}+b_{12} & \\cdots & a_{1n}+b_{1n} \\\\ a_{21}+b_{21} & a_{22}+b_{22} & \\cdots & a_{2n}+b_{2n} \\\\ \\vdots & \\vdots  & \\ddots & \\vdots \\\\ a_{m1}+b_{m1} & a_{m2}+b_{m2} & \\cdots & a_{mn}+b_{mn} \\end{bmatrix}\nNote that matrices {\\bf A} and {\\bf B} must have the same dimensionality, in which case they are conformable for addition.\n\nExample 6.2 {\\bf A}=\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}, \\qquad {\\bf B}=\\begin{bmatrix} 1 & 2 & 1 \\\\ 2 & 1 & 2 \\end{bmatrix}\nFind \\mathbf{A}+\\mathbf{B}\n\nScalar Multiplication: Given the scalar s, the scalar multiplication of s {\\bf A} is\n s {\\bf A}=  s \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\end{bmatrix} = \\begin{bmatrix} s a_{11} & s a_{12} & \\cdots & s a_{1n} \\\\ s a_{21} & s a_{22} & \\cdots & s a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ s a_{m1} & s a_{m2} & \\cdots & s a_{mn} \\end{bmatrix}\n\nExample 6.3 s=2,\n{\\bf A}=\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\nFind s {\\bf A}"
  },
  {
    "objectID": "21_vector_matrix.html#transpose",
    "href": "21_vector_matrix.html#transpose",
    "title": "\n6  Matrix Operations\n",
    "section": "\n6.5 Transpose",
    "text": "6.5 Transpose\nOne important transformation we will have to perform on a matrix is to switch the columns into rows. It is not necessary that you see the importance of this transformation right now, but trust that it is something we will need quite frequently.\nMatrix Transpose: The transpose of the m\\times n matrix \\bf A is the n\\times m matrix {\\bf A}^\\top (also written {\\bf A}') obtained by interchanging the rows and columns of \\bf A.\nIn other words, the (i,j)-th element of \\mathbf{A}^\\top is the (j,i)-th element of \\mathbf{A}.\n\\left(\\mathbf{A}^\\top\\right)_{ij} = \\mathbf{A}_{ji}\nNote: If we transpose the transpose of a matrix, we will get back the original matrix. That is, (\\mathbf{A}^\\top)^\\top = \\mathbf{A}.\nFor example,\n{\\bf A}=\\begin{bmatrix} 4&-2&3\\\\0&5&-1\\end{bmatrix}, \\qquad {\\bf A}^\\top=\\begin{bmatrix} 4&0\\\\-2&5\\\\3&-1 \\end{bmatrix}\n{\\bf B}=\\begin{bmatrix} 2\\\\-1\\\\3 \\end{bmatrix}, \\qquad {\\bf B}^\\top=\\begin{bmatrix} 2&-1&3\\end{bmatrix}\nThus, if \\mathbf{A} is a 3\\times 4 matrix then \\mathbf{A}^\\top is a 4\\times 3 matrix as follows:\n\\mathbf{A} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} &A_{14}\\\\ A_{21} & A_{22} & A_{23} &A_{24}\\\\ A_{31} & A_{32} & A_{33} &A_{34}\\end{bmatrix} \\quad  \\mathbf{A}^\\top = \\begin{bmatrix} A_{11} & A_{21} & A_{31} \\\\ A_{12} & A_{22} & A_{32} \\\\ A_{13} & A_{23} & A_{33} \\\\ A_{14} & A_{24} & A_{34}\\end{bmatrix}\n\nExample 6.4 For the following matrices and vectors, determine the transpose:\n\\mathbf{B}=\\begin{bmatrix} 2 & -3 & -4 \\\\5&-6&-7\\\\-8&9&0 \\end{bmatrix} \\qquad \\mathbf{M}=\\begin{bmatrix} -1&2\\\\-3&6\\\\7&-9\\\\5&-1 \\end{bmatrix} \\qquad \\mathbf{x}=\\begin{bmatrix}3\\\\-4\\\\5\\\\6\\end{bmatrix}\nTo find the transpose, we simply create new matrices whose rows are the corresponding columns of each matrix or vector:\n\\mathbf{B}^\\top=\\begin{bmatrix} 2 &5& -8\\\\-3&-6&9\\\\-4&-7&0\\end{bmatrix} \\qquad \\mathbf{M}^\\top = \\begin{bmatrix}-1&-3&7&5\\\\2&6&-9&-1 \\end{bmatrix}  \\mathbf{x}^\\top = \\begin{bmatrix} 3&-4&5&6 \\end{bmatrix}\n\nThe following properties of matrix transpose are useful to know:\n\n({\\bf A+B})^\\top = {\\bf A}^\\top+{\\bf B}^\\top\n({\\bf A}^\\top)^\\top={\\bf A}\n(s{\\bf A})^\\top = s{\\bf A}^\\top\n\n({\\bf AB})^\\top = {\\bf B}^\\top{\\bf A}^\\top; and by induction ({\\bf ABC})^\\top = {\\bf C}^\\top{\\bf B}^\\top{\\bf A}^\\top\n\n\nExample of ({\\bf AB})^\\top = {\\bf B}^\\top{\\bf A}^\\top:\n{\\bf A}=\\begin{bmatrix} 1&3&2\\\\2&-1&3\\end{bmatrix}, \\qquad {\\bf B}=\\begin{bmatrix} 0&1\\\\2&2\\\\3&-1\\end{bmatrix}\n ({\\bf AB})^\\top = \\left[ \\begin{bmatrix} 1&3&2\\\\2&-1&3\\end{bmatrix} \\begin{bmatrix} 0&1\\\\2&2\\\\3&-1\\end{bmatrix} \\right]^\\top = \\begin{bmatrix} 12&7\\\\5&-3 \\end{bmatrix}\n {\\bf B}^\\top{\\bf A}^\\top= \\begin{bmatrix} 0&2&3\\\\1&2&-1 \\end{bmatrix}  \\begin{bmatrix} 1&2\\\\3&-1\\\\2&3 \\end{bmatrix} = \\begin{bmatrix} 12&7\\\\5&-3 \\end{bmatrix}"
  },
  {
    "objectID": "21_vector_matrix.html#special-matrix",
    "href": "21_vector_matrix.html#special-matrix",
    "title": "\n6  Matrix Operations\n",
    "section": "\n6.6 Special Matrices",
    "text": "6.6 Special Matrices\nZero Matrix: The zero matrix is a matrix of all zeros. For example, the 3\\times 4 zero matrix is given by\n{\\bf 0}_{3\\times 4}=\\begin{bmatrix} 0&0&0&0\\\\0&0&0&0\\\\0&0&0&0 \\end{bmatrix}\nSquare Matrix: A square matrix is a matrix with the same number of rows and columns. For example, the 3\\times 3 matrix is a square matrix, but the 3\\times 4 matrix is not.\nIdentity Matrix: The identity matrix is a square matrix with ones on the main diagonal and zeros elsewhere. For example, the 3\\times 3 identity matrix is given by\n{\\bf I}_3=\\begin{bmatrix} 1&0&0\\\\0&1&0\\\\0&0&1 \\end{bmatrix}\nNotice that we can write identity matrix as collections of unit vectors. For example, the 3\\times 3 identity matrix can be written as\n\\mathbf{I}_3=\\left[ \\mathbf{e}_1 | \\mathbf{e}_2 | \\mathbf{e}_3 \\right],\nwhere\n{\\bf e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, {\\bf e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\;\\;\\mathrm{ and }\\;\\; {\\bf e}_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\nare unit vectors in \\mathbb{R}^3, the 3-dimensional vector space.\nSymmetric Matrix: A matrix is symmetric if it is equal to its transpose. That is, if \\mathbf{A} is a symmetric matrix, then\n\\mathbf{A}=\\mathbf{A}^\\top.\nDiagonal Matrix: A matrix is diagonal if all the elements outside the main diagonal are zero.\nFor example:\n\\mathbf{D} = \\begin{bmatrix} \\sigma_1 & 0 & 0 & 0 \\\\ 0 & \\sigma_2 & 0 & 0\\\\ 0&0&\\sigma_3&0\\\\ 0&0&0&\\sigma_4 \\end{bmatrix}\nSince the off diagonal elements are 0, we need only define the diagonal elements for such a matrix. Thus, we will frequently write\n\\mathbf{D}=\\mathrm{diag}\\{\\sigma_1,\\sigma_2,\\sigma_3,\\sigma_4\\}."
  },
  {
    "objectID": "21_vector_matrix.html#vector-inner-product",
    "href": "21_vector_matrix.html#vector-inner-product",
    "title": "\n6  Matrix Operations\n",
    "section": "\n6.7 Vector Inner Product",
    "text": "6.7 Vector Inner Product\nWhen we multiply matrices, we do not perform the operation element-wise as we did with addition and scalar multiplication. Matrix multiplication is, in itself, a very powerful tool for summarizing information. In fact, many of the analytical tools, like linear regression, can all be understood more clearly with a firm grasp on matrix multiplication. Because this operation is so important, we will spend a considerable amount of energy breaking it down in many ways.\nWe often want to summarize the relationship between two vectors into a single number. If the two vectors are pointing in similar directions, the number should be positive. If the two vectors are pointing in opposite directions, the number should be negative. If the two vectors are perpendicular, the number should be zero. This idea leads to the definition of the inner product of two vectors.\nInner Product: The inner product (also called dot product) of two vectors \\mathbf{u} and \\mathbf{v} is defined as\n\\mathbf{u} \\cdot \\mathbf{v}=\\mathbf{u}^\\top\\mathbf{v}=\\sum_{i=1}^n u_i v_i = u_1 v_1 + u_2 v_2+\\cdots u_n v_n.\nThe inner product is a scalar quantity.\nIf {\\bf u} \\cdot {\\bf v} = 0, the two vectors are orthogonal (or perpendicular).\n\n\n\n\nFigure 6.4: Animation of Inner Product between two vectors (Source)\n\n\n\n\nWhy are we doing this? To understand what’s going on intuitively, let’s think about the case of two-dimensions where we can break down the components of the inner product into x and y axis. Denote\n\\mathbf{a}=\\begin{bmatrix}a_x \\\\ a_y\\end{bmatrix}=a_x\\begin{bmatrix}1 \\\\ 0\\end{bmatrix} + a_y\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}\n\\mathbf{b}=\\begin{bmatrix}b_x \\\\ b_y\\end{bmatrix}=b_x\\begin{bmatrix}1 \\\\ 0\\end{bmatrix} + b_y\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}\nSince x and y axis are perpendicular to each other, the inner products of their unit vectors \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} and \\begin{bmatrix}0 \\\\ 1\\end{bmatrix} are zero, thus we have that \\begin{align*}\\mathbf{a}\\cdot\\mathbf{b}\n&=\\left(a_x\\begin{bmatrix}1 \\\\ 0\\end{bmatrix} + a_y\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}\\right)\\cdot\\left(b_x\\begin{bmatrix}1 \\\\ 0\\end{bmatrix} + b_y\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}\\right) \\\\\n&=\na_x b_x \\underbrace{\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}^\\top \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}}_{=1\\cdot 1 + 0\\cdot0=1} +\na_x b_y \\underbrace{\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}^\\top \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}}_{=1\\cdot 0 + 0\\cdot1=0} +\na_y b_x \\underbrace{\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}^\\top \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}}_{=0\\cdot 1 + 1\\cdot0=0} +\na_y b_y \\underbrace{\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}^\\top \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}}_{=0\\cdot 0 + 1\\cdot1=1}\\\\\n&=a_x b_x + a_y b_y\n\\end{align*}\n\n\n\n\nFigure 6.5: Components of Inner Product (Source)\n\n\n\n\n\nExample 6.5 (Vector Inner Product) Let \\mathbf{x}=\\begin{bmatrix} -1 \\\\2\\\\4\\\\0 \\end{bmatrix} \\quad \\mathbf{y}=\\begin{bmatrix} 3 \\\\5\\\\1\\\\7 \\end{bmatrix} \\quad \\mathbf{v}=\\begin{bmatrix} -3 \\\\-2\\\\5\\\\3\\\\-2 \\end{bmatrix} \\quad \\mathbf{u}= \\begin{bmatrix} 2\\\\-1\\\\3\\\\-3\\\\-2 \\end{bmatrix}\nIf possible, compute the following inner products:\n\n\n\\mathbf{x}^\\top\\mathbf{y} \\begin{align*}\n\\mathbf{x}^\\top\\mathbf{y} &=\\begin{bmatrix} -1 &2&4&0 \\end{bmatrix} \\begin{bmatrix} 3 \\\\5\\\\1\\\\7 \\end{bmatrix} \\\\\n&= (-1)(3)+(2)(5)+(4)(1)+(0)(7) = -3+10+4=11\n\\end{align*}\n\n\n\\mathbf{x}^\\top\\mathbf{v} This is not possible because \\mathbf{x} and \\mathbf{v} do not have the same number of elements\n\n\\mathbf{v}^\\top\\mathbf{u} \\begin{align*}\n\\mathbf{v}^\\top\\mathbf{u} &= \\begin{bmatrix} -3 &-2&5&3&-2 \\end{bmatrix} \\begin{bmatrix} 2\\\\-1\\\\3\\\\-3\\\\-2 \\end{bmatrix} \\\\\n&= (-3)(2)+(-2)(-1)+(5)(3)+(3)(-3)+(-2)(-2) = -6+2+15-9+4 = 6\n\\end{align*}\n\n\n\n\nExercise 6.1 Let u = \\begin{bmatrix} 7\\\\1\\\\-5\\\\3\\end{bmatrix}, v = \\begin{bmatrix} 9\\\\-3\\\\2\\\\8 \\end{bmatrix}, w = \\begin{bmatrix} 1\\\\13\\\\ -7\\\\2 \\\\15 \\end{bmatrix}, and c = 2. Calculate the following:\n\nu-v\ncw\nu \\cdot v\nw \\cdot v\n\n\nVector Norm: The norm of a vector is a measure of its length. There are many different ways to calculate the norm, but the most common is the Euclidean norm (which corresponds to our usual conception of distance):\n\\lVert \\mathbf{v}\\rVert = \\sqrt{\\mathbf{v}\\cdot\\mathbf{v}} =  \\sqrt{\\mathbf{v}^\\top\\mathbf{v}} = \\sqrt{ v_1^2 + v_2^2 + \\cdots + v_n^2}.\nThis is merely measuring the distance between the point \\mathbf{v} and the origin.\nIn other words, the norm of a vector is the square root of the sum of the squares of its components, which is also the square root of the inner product of itself. The norm of a vector is always non-negative. The norm of a zero vector is 0. The norm of a unit vector is 1.\nTo compute the distance between two different points, say \\mathbf{x} and \\mathbf{y}, we’d calculate \\begin{align*}\n\\lVert \\mathbf{x}-\\mathbf{y}\\rVert\n&= \\sqrt{(\\mathbf{x}-\\mathbf{y})^\\top(\\mathbf{x}-\\mathbf{y})} \\\\\n&= \\sqrt{(x_1-y_1)^2 + (x_2-y_2)^2 + \\dots + (x_n-y_n)^2}\n\\end{align*}\n\nExample 6.6 Suppose I have two vectors in 3-dimensional space:\n\\mathbf{x}=(1,1,1) \\;\\;\\;\\textrm{   and   }\\;\\;\\; \\mathbf{y}=(1,0,0)\nThen the Euclidean norm of \\mathbf{x} (i.e. its length or distance from the origin) is\n\\lVert \\mathbf{x}\\rVert =\\sqrt{1^2+1^2+1^2}=\\sqrt{3}\nand the Euclidean norm of \\mathbf{y} is\n\\lVert \\mathbf{y} \\rVert =\\sqrt{1^2+0^2+0^2}=1\nand the Euclidean distance between point \\mathbf{x} and point \\mathbf{y} is\n\\lVert \\mathbf{x}-\\mathbf{y} \\rVert =\\sqrt{(1-1)^2 + (1-0)^2 + (1-0)^2} =\\sqrt{2}.\nThe Euclidean norm is crucial to many methods in data analysis as it measures the closeness of two data points."
  },
  {
    "objectID": "21_vector_matrix.html#matrix-multiplication",
    "href": "21_vector_matrix.html#matrix-multiplication",
    "title": "\n6  Matrix Operations\n",
    "section": "\n6.8 Matrix Multiplication",
    "text": "6.8 Matrix Multiplication\nMatrix multiplication is nothing more than a collection of inner products done simultaneously in one operation. We must be careful when multiplying matrices because, as with vectors, the operation is not always possible. Unlike the vector inner product, the order in which you multiply matrices makes a big difference!\nMatrix Multiplication: Let \\mathbf{A} be a m\\times n matrix and \\mathbf{B} be a {k\\times p} matrix. The matrix product \\mathbf{A}\\mathbf{B} is possible if and only if n=k; that is, when the number of columns in \\mathbf{A} is the same as the number of rows in \\mathbf{B}. If this condition holds, then the the product, \\mathbf{A}\\mathbf{B}, is a m\\times p matrix and the (i,j) entry of the product \\mathbf{A}\\mathbf{B} is the inner product of the ith row of \\mathbf{A} and the jth column of \\mathbf{B}:\n(\\mathbf{A}\\mathbf{B})_{ij} = \\mathbf{A}_{i\\star}\\cdot\\mathbf{B}_{\\star j}\nIn other words,\n\\begin{align*}\n\\mathbf{AB}\n&=\\left[\\begin{matrix} \\longleftarrow & \\mathbf{a}_1 & \\longrightarrow \\\\\n\\longleftarrow & \\mathbf{a}_2 & \\longrightarrow \\\\\n\\vdots & \\vdots & \\vdots \\\\\n\\longleftarrow & \\mathbf{a}_m & \\longrightarrow \\end{matrix}\\right]\n\\left[\\begin{matrix} \\uparrow & \\uparrow &\\dots & \\uparrow \\\\\n\\mathbf{b}_1&\\mathbf{b}_2&\\dots&\\mathbf{b}_p \\\\\n\\downarrow &\\downarrow &\\dots&\\downarrow   \\end{matrix}\\right] \\\\\n&=\\begin{bmatrix} \\mathbf{a}_1\\cdot\\mathbf{b}_1 & \\mathbf{a}_1\\cdot\\mathbf{b}_2 & \\cdots & \\mathbf{a}_1\\cdot\\mathbf{b}_p \\\\ \\mathbf{a}_2\\cdot\\mathbf{b}_1 & \\mathbf{a}_2\\cdot\\mathbf{b}_2 & \\cdots & \\mathbf{a}_2\\cdot\\mathbf{b}_p \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\mathbf{a}_m\\cdot\\mathbf{b}_1 & \\mathbf{a}_m\\cdot\\mathbf{b}_2 & \\cdots & \\mathbf{a}_m\\cdot\\mathbf{b}_p \\end{bmatrix}\n\\end{align*} where \\mathbf{a}_i is the ith row of \\mathbf{A} and \\mathbf{b}_j is the jth column of \\mathbf{B}.\n\nExample 6.7 (Steps to Compute Matrix Multiplication) Let \\mathbf{A}=\\begin{bmatrix} 2 & 3 \\\\ -1 & 4 \\\\ 5 & 1 \\end{bmatrix} \\quad \\;\\;\\mathrm{ and }\\;\\; \\quad \\mathbf{B}=\\begin{bmatrix}  0 & -2 \\\\ 2 & -3 \\end{bmatrix}\nWhen we first get started with matrix multiplication, we often follow a few simple steps:\n\nWrite down the matrices and their dimensions. Make sure the “inside” dimensions match - those corresponding to the columns of the first matrix and the rows of the second matrix: \\underset{(3\\times \\red{2})}{\\mathbf{A}} \\underset{(\\red{2} \\times 2)}{\\mathbf{B}} If these dimensions match, then we can multiply the matrices. If they don’t, we stop right there - multiplication is not possible.\nNow, look at the “outer” dimensions - this will tell you the size of the resulting matrix. \\underset{(\\blue{3}\\times 2)}{\\mathbf{A}} \\underset{(2\\times \\blue{2})}{\\mathbf{B}} So the product \\mathbf{A}\\mathbf{B} is a 3\\times 2 matrix.\nFinally, we compute the product of the matrices by multiplying each row of \\mathbf{A} by each column of \\mathbf{B} using inner products. The element in the first row and first column of the product (written (\\mathbf{A}\\mathbf{B})_{11}) will be the inner product of the first row of \\mathbf{A} and the first column of \\mathbf{B}. Then, (\\mathbf{A}\\mathbf{B})_{12} will be the inner product of the first row of \\mathbf{A} and the second column of \\mathbf{B}, etc. \\mathbf{A}\\mathbf{B} =\\begin{bmatrix} (2)(0)+(3)(2) & (2)(-2)+(3)(-3)\\\\ (-1)(0)+(4)(2) & (-1)(-2)+(4)(-3)\\\\ (5)(0)+(1)(2) & (5)(-2)+(1)(-3) \\end{bmatrix} = \\begin{bmatrix} 6&-13\\\\8 & -10\\\\2&-13\\end{bmatrix} \n\n\n\n\nExercise 6.2 (Matrix Multiplication) Suppose we have\n\\mathbf{A}_{4\\times 6} \\quad \\mathbf{B}_{5\\times 5} \\quad \\mathbf{M}_{5\\times 4} \\quad \\mathbf{P}_{6\\times 5}\nCircle the matrix products that are possible to compute and write the dimension of the result.\n\\mathbf{A}\\mathbf{M} \\qquad \\mathbf{M}\\mathbf{A} \\qquad \\mathbf{B}\\mathbf{M}  \\qquad \\mathbf{M}\\mathbf{B} \\qquad \\mathbf{P}\\mathbf{A} \\qquad \\mathbf{P}\\mathbf{M} \\qquad \\mathbf{A}\\mathbf{P} \\qquad \\mathbf{A}^\\top\\mathbf{P} \\qquad \\mathbf{M}^\\top\\mathbf{B}\nLet\n\\mathbf{A}=\\begin{bmatrix} 1&1&0&1\\\\0&1&1&1\\\\1&0&1&0\\end{bmatrix} \\quad \\mathbf{M} = \\begin{bmatrix} -2&1&-1&2&-2\\\\1&-2&0&-1&2\\\\2&1&-3&-2&3 \\\\ 1&3&2&-1&2\\end{bmatrix}\n\\mathbf{C}=\\begin{bmatrix} -1&0&1&0\\\\1&-1&0&0\\\\0&0&1&-1 \\end{bmatrix}\nDetermine the following matrix products, if possible:\n\n\\mathbf{A}\\mathbf{C}\n\\mathbf{A}\\mathbf{M}\n\\mathbf{A}^\\top\\mathbf{C}\n\n\nThe following properties of matrix multiplication and addition are useful to know:\n\nAssociative: \\bf (A+B)+C = A+(B+C)\n\n\\bf (AB)C = A(BC)\nCommutative: \\bf A+B=B+A\n\nDistributive: \\bf A(B+C)=AB+AC\n\n\\bf (A+B)C=AC+BC\n\nCommutative law for multiplication does not hold – the order of multiplication matters:\n\\bf AB \\ne BA\nFor example,\n{\\bf A}=\\begin{bmatrix} 1&2\\\\-1&3\\end{bmatrix}, \\qquad {\\bf B}=\\begin{bmatrix} 2&1\\\\0&1\\end{bmatrix}\n{\\bf AB}=\\begin{bmatrix} 2&3\\\\-2&2\\end{bmatrix}, \\qquad {\\bf BA}=\\begin{bmatrix} 1&7\\\\-1&3\\end{bmatrix}"
  },
  {
    "objectID": "21_vector_matrix.html#matrix-vector-product",
    "href": "21_vector_matrix.html#matrix-vector-product",
    "title": "\n6  Matrix Operations\n",
    "section": "\n6.9 Matrix-Vector Product",
    "text": "6.9 Matrix-Vector Product\nWhat if we want to multiply matrix and vector? A matrix-vector product works exactly the same way as matrix multiplication; after all, a vector \\mathbf{x} is nothing but an n\\times 1 matrix. In order to multiply a matrix by a vector, again we must match the dimensions to make sure they line up correctly. For example, if we have an m\\times n matrix \\mathbf{A}, we can multiply by a 1\\times m row vector \\mathbf{v}^\\top on the left:\n\\mathbf{v}^\\top\\mathbf{A} \\quad \\textrm{works because } \\underset{ (1\\times \\red{m})}{\\mathbf{v}^\\top} \\underset{(\\red{m}\\times n)}{\\mathbf{A}}\n\\Longrightarrow \\textrm{The result will be a   } 1 \\times n \\textrm{ row vector.}\nor we can multiply by an n\\times 1 column vector \\mathbf{x} on the right:\n\\mathbf{A}\\mathbf{x} \\quad \\textrm{works because } \\underset{(m\\times \\red{n})}{\\mathbf{A}}\\underset{(\\red{n}\\times 1)}{\\mathbf{x}} \n\\Longrightarrow \\textrm{The result will be a   } m\\times 1 \\textrm{ column vector.}\nMatrix-vector multiplication works the same way as matrix multiplication: we simply multiply rows by columns until we’ve completed the answer. In the case of \\mathbf{v}^\\top\\mathbf{A}, we’d multiply the row \\mathbf{v} by each of the n columns of \\mathbf{A}, carving out our solution, one entry at a time :\n\\mathbf{v}^\\top\\mathbf{A} = \\begin{bmatrix} \\mathbf{v}^\\top\\mathbf{A}_{*1} & \\mathbf{v}^\\top\\mathbf{A}_{*2} & \\cdots & \\mathbf{v}^\\top\\mathbf{A}_{*n} \\end{bmatrix}.\nIn the case of \\mathbf{A}\\mathbf{x}, we’d multiply each of the m rows of \\mathbf{A} by the column \\mathbf{x}:\n\\mathbf{A}\\mathbf{x} = \\begin{bmatrix} \\mathbf{A}_{1*}\\mathbf{x} \\\\ \\mathbf{A}_{2*}\\mathbf{x} \\\\ \\vdots \\\\ \\mathbf{A}_{m*}\\mathbf{x} \\end{bmatrix}.\n\nExample 6.8 (Matrix-Vector Product) Let \\mathbf{A}=\\begin{bmatrix} 2 & 3 \\\\ -1 & 4 \\\\ 5 & 1 \\end{bmatrix}  \\quad \\mathbf{v}=\\begin{bmatrix} 3\\\\2 \\end{bmatrix} \\quad \\mathbf{q}=\\begin{bmatrix} 2\\\\-1\\\\3\\end{bmatrix}\nDetermine whether the following matrix-vector products are possible. When possible, compute the product.\nLet \\mathbf{A}=\\begin{bmatrix} 2 & 3 \\\\ -1 & 4 \\\\ 5 & 1 \\end{bmatrix}  \\quad \\mathbf{v}=\\begin{bmatrix} 3\\\\2 \\end{bmatrix} \\quad \\mathbf{q}=\\begin{bmatrix} 2\\\\-1\\\\3\\end{bmatrix} d. \\mathbf{v}^\\top\\mathbf{A} \\textrm{Not Possible: Inner dimensions do not match} \\quad \\underset{(1\\times \\red{2})}{\\mathbf{v}^\\top}\\underset{(\\red{3}\\times 2)}{\\mathbf{A}}"
  },
  {
    "objectID": "21_vector_matrix.html#matrix-product-is-linear-combination",
    "href": "21_vector_matrix.html#matrix-product-is-linear-combination",
    "title": "\n6  Matrix Operations\n",
    "section": "\n6.10 Matrix Product IS Linear Combination",
    "text": "6.10 Matrix Product IS Linear Combination\nAll matrix products can be viewed as linear combinations. This vantage point is extremely crucial to our understanding of linear algebra. Let’s start with matrix-vector product and see how we can depict it as a linear combination of the columns of the matrix.\nMatrix-Vector Product as Linear Combination:\nLet \\mathbf{A} be an m\\times n matrix partitioned into columns,\n\\mathbf{A} = \\begin{bmatrix}\\mathbf{a}_1 | \\mathbf{a}_2 | \\dots | \\mathbf{a}_n\\end{bmatrix}\nand let \\mathbf{x} be a n-dimensional vector\n\\mathbf{x}=\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\nThen, \\mathbf{A}\\mathbf{x} is the linear combination of the columns of \\mathbf{A} using coefficients in \\mathbf{x}:\n\\mathbf{A}\\mathbf{x} = x_1\\mathbf{a}_1 + x_2\\mathbf{a}_2 + \\dots + x_n\\mathbf{a}_n.\nThe animation below illustrates the relationship between matrix-vector product and linear combination.\n\n\n\n\nFigure 6.6: Illustration of Matrix-Vector Product as Linear Combinations (Source)\n\n\n\n\n\nExample 6.9 Calculate\n\\begin{bmatrix} 3 & 1 \\\\ 1 & 2\\end{bmatrix}\\begin{bmatrix}-1 \\\\ 2\\end{bmatrix}.\nCalculate\n(-1)\\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix}+(2)\\begin{bmatrix}1 \\\\ 2\\end{bmatrix}.\nInterpret the geometric relationship between the two.\n\nConceptually, while doing matrix-vector product, the matrix \\mathbf{A} is transforming the vector \\mathbf{x} in a specific way (by taking linear cominations) to the vector \\mathbf{A}\\mathbf{x}. This is illustrated in the video below.\n\n\nMatrix Multiplication as Collection of Linear Combinations:\nWe can view matrix multiplication as applying matrix-vector product iteratively.\nIf\n\\mathbf{A}\\mathbf{B}=\\mathbf{C},\nthen the columns of \\mathbf{C} can be viewed as linear combinations of the columns of \\mathbf{A} and the coefficients are given in the elements of each column of \\mathbf{B}. Denote\n\\mathbf{A}_{m\\times n} = \\begin{bmatrix}\\mathbf{a}_1 | \\mathbf{a}_2 | \\dots | \\mathbf{a}_n\\end{bmatrix}\n\\mathbf{B}_{n\\times p} = \\begin{bmatrix}\\mathbf{b}_1 | \\mathbf{b}_2 | \\dots | \\mathbf{b}_p\\end{bmatrix}\n\\mathbf{C}_{m\\times p} = \\begin{bmatrix}\\mathbf{c}_1 | \\mathbf{c}_2 | \\dots | \\mathbf{c}_p\\end{bmatrix}\nThen, for example, \\mathbf{c}_1 is the linear combination of the columns of \\mathbf{A} using coefficients in the first column of \\mathbf{B}, i.e., \\mathbf{b}_1:\n\\mathbf{b}_1=\\begin{bmatrix}B_{11}\\\\ B_{21}\\\\ \\vdots \\\\ B_{n1} \\end{bmatrix},\n\\mathbf{C}_1= B_{11}\\mathbf{a}_1 + B_{21}\\mathbf{a}_2 + \\dots + B_{n1}\\mathbf{a}_n=\\mathbf{A}\\mathbf{b}_1,\nwhich is the matrix-vector product of \\mathbf{A} and \\mathbf{b}_1. As a result, we can write the whole matrix multiplication as a collection of matrix-vector products by columns:\n\\mathbf{C} = \\begin{bmatrix}\\mathbf{c}_1 | \\mathbf{c}_2 | \\dots | \\mathbf{c}_p\\end{bmatrix} = \\begin{bmatrix}\\mathbf{A}\\mathbf{b}_1 | \\mathbf{A}\\mathbf{b}_2 | \\cdots | \\mathbf{A}\\mathbf{b}_p\\end{bmatrix}.\nThe animation below illustrates the relationship between matrix multiplication and linear combinations.\n\n\n\n\nIllustration of Matrix Multiplication as Linear Combinations (Source)\n\n\n\n\nIn fact, when doing matrix multiplication, we’re actually applying a series of transformations, illustrated nicely in the video below."
  },
  {
    "objectID": "21_vector_matrix.html#answers-to-examples-and-exercises",
    "href": "21_vector_matrix.html#answers-to-examples-and-exercises",
    "title": "\n6  Matrix Operations\n",
    "section": "Answers to Examples and Exercises",
    "text": "Answers to Examples and Exercises\nAnswer to Exercise 6.1:\n\n\\begin{bmatrix} -2 &4&-7&-5 \\end{bmatrix}\n\\begin{bmatrix} 2 &26&-14&4&30 \\end{bmatrix}\n63 -3 -10 + 24 = 74\nundefined\n\nAnswer to Example 6.2:\n{\\bf A+B}=\\begin{bmatrix} 2 & 4 & 4 \\\\ 6 & 6 & 8 \\end{bmatrix}\nAnswer to Example 6.3:\ns {\\bf A} = \\begin{bmatrix} 2 & 4 & 6 \\\\ 8 & 10 & 12 \\end{bmatrix}"
  },
  {
    "objectID": "22_linear_systems.html",
    "href": "22_linear_systems.html",
    "title": "\n7  Systems of Linear Equations\n",
    "section": "",
    "text": "Linear equations take form of a_1 x_1 + a_2 x_2 + \\cdots + a_n x_n = b\n\n\na_i are parameters or coefficients\n\nx_i are variables or unknowns\n\nLinear because only one variable per term and degree is at most 1.\nWe are often interested in solving linear systems like\n\\left\\{\\begin{array}{ll} x-3y &= -3\\\\ 2x +y &= 8 \\end{array}\\right.\nMore generally, we might have a system of m equations in n unknowns \\begin{matrix}\n        a_{11}x_1  & + & a_{12}x_2 & + & \\cdots & + & a_{1n}x_n & = & b_1\\\\\n        a_{21}x_1  & + & a_{22}x_2 & + & \\cdots & + & a_{2n}x_n & = & b_2\\\\\n        \\vdots     &   &     &   & \\vdots &   &     & \\vdots & \\\\\n        a_{m1}x_1  & + & a_{m2}x_2 & + & \\cdots & + & a_{mn}x_n & = & b_m\n        \\end{matrix}\nA solution to a linear system of m equations in n unknowns is a set of n numbers x_1, x_2, \\cdots, x_n that satisfy each of the m equations.\nExample: x=3 and y=2 is the solution to the above 2\\times 2 linear system. If you graph the two lines, you will find that they intersect at (3,2).\nDoes a linear system have one, no, or multiple solutions? For a system of 2 equations with 2 unknowns (i.e., two lines):\n\n\nOne solution: The lines intersect at exactly one point.\n\nNo solution: The lines are parallel.\n\nInfinite solutions: The lines coincide.\n\nMethods to solve linear systems:\n\nSubstitution\nElimination of variables\nMatrix methods\n\n\nExercise 7.1 Provide a system of 2 equations with 2 unknowns that has\n\none solution\nno solution\ninfinite solutions"
  },
  {
    "objectID": "22_linear_systems.html#systems-of-equations-as-matrices",
    "href": "22_linear_systems.html#systems-of-equations-as-matrices",
    "title": "\n7  Systems of Linear Equations\n",
    "section": "\n7.2 Systems of Equations as Matrices",
    "text": "7.2 Systems of Equations as Matrices\nMatrices provide an easy and efficient way to represent linear systems such as \\begin{matrix}\n        a_{11}x_1  & + & a_{12}x_2 & + & \\cdots & + & a_{1n}x_n & = & b_1\\\\\n        a_{21}x_1  & + & a_{22}x_2 & + & \\cdots & + & a_{2n}x_n & = & b_2\\\\\n        \\vdots     &   &     &   & \\vdots &   &     & \\vdots & \\\\\n        a_{m1}x_1  & + & a_{m2}x_2 & + & \\cdots & + & a_{mn}x_n & = & b_m\n        \\end{matrix}\nas {\\bf A x = b} where\nThe m \\times n {\\bf A} is an array of m n real numbers arranged in m rows by n columns: {\\bf A}=\\begin{bmatrix}\n    a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n    a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n    \\vdots &  & \\ddots & \\vdots \\\\\n    a_{m1} & a_{m2} & \\cdots & a_{mn}\n    \\end{bmatrix}\nThe unknown quantities are represented by the vector {\\bf x}=\\begin{bmatrix} x_1\\\\x_2\\\\\\vdots\\\\x_n \\end{bmatrix}.\nThe right hand side of the linear system is represented by the vector {\\bf b}=\\begin{bmatrix} b_1\\\\b_2\\\\\\vdots\\\\b_m \\end{bmatrix}.\nAugmented Matrix: When we append \\bf b to the coefficient matrix \\bf A, we get the augmented matrix \\widehat{\\bf A}=[\\bf A | b] \\begin{bmatrix}\n      a_{11} & a_{12} & \\cdots & a_{1n} & | & b_1\\\\\n      a_{21} & a_{22} & \\cdots & a_{2n} & | & b_2\\\\\n      \\vdots &  & \\ddots & \\vdots & | & \\vdots\\\\\n      a_{m1} & a_{m2} & \\cdots & a_{mn} & | & b_m\n      \\end{bmatrix}\n\nExercise 7.2 Create an augmented matrix that represent the following system of equations:\n2x_1 -7x_2 + 9x_3 -4x_4 = 8\n41x_2 + 9x_3 -5x_6 = 11\nx_1 -15x_2 -11x_5 = 9"
  },
  {
    "objectID": "22_linear_systems.html#finding-solutions-to-augmented-matrices-and-systems-of-equations",
    "href": "22_linear_systems.html#finding-solutions-to-augmented-matrices-and-systems-of-equations",
    "title": "\n7  Systems of Linear Equations\n",
    "section": "\n7.3 Finding Solutions to Augmented Matrices and Systems of Equations",
    "text": "7.3 Finding Solutions to Augmented Matrices and Systems of Equations\nRow Echelon Form: Our goal is to translate our augmented matrix or system of equations into row echelon form. This will provide us with the values of the vector x which solve the system. We use the row operations to change coefficients in the lower triangle of the augmented matrix to 0. An augmented matrix of the form \\begin{bmatrix}\n      \\fbox{$a'_{11}$}& a'_{12} & a'_{13}& \\cdots & a'_{1n} & | & b'_1\\\\\n      0 & \\fbox{$a'_{22}$} & a'_{23}& \\cdots & a'_{2n} & | & b'_2\\\\\n      0 & 0 & \\fbox{$a'_{33}$}& \\cdots & a'_{3n} & | & b'_3\\\\\n      0 & 0 &0 & \\ddots & \\vdots  & | & \\vdots \\\\\n      0 & 0 &0 &0 & \\fbox{$a'_{mn}$} & | & b'_m\n      \\end{bmatrix}\nis said to be in row echelon form — each row has more leading zeros than the row preceding it.\nReduced Row Echelon Form: We can go one step further and put the matrix into reduced row echelon form. Reduced row echelon form makes the value of x which solves the system very obvious. For a system of m equations in m unknowns, with no all-zero rows, the reduced row echelon form would be\n\\begin{bmatrix}\n      \\fbox{$1$}  &  0 &   0 &    0  &   0 & | & b^*_1\\\\\n      0  &  \\fbox{$1$} &   0 &    0  &   0 & | & b^*_2\\\\\n      0  &  0 &   \\fbox{$1$} &    0  &   0 & | & b^*_3\\\\\n      0  &  0 &   0 &\\ddots &   0 & | &\\vdots\\\\\n      0  &  0 &   0 &    0  &   \\fbox{$1$} & | & b^*_m\n      \\end{bmatrix}\nGaussian and Gauss-Jordan elimination: We can conduct elementary row operations to get our augmented matrix into row echelon or reduced row echelon form. The methods of transforming a matrix or system into row echelon and reduced row echelon form are referred to as Gaussian elimination and Gauss-Jordan elimination, respectively.\nElementary Row Operations: To do Gaussian and Gauss-Jordan elimination, we use three basic operations to transform the augmented matrix into another augmented matrix that represents an equivalent linear system – equivalent in the sense that the same values of x_j solve both the original and transformed matrix/system:\nInterchanging Rows: Suppose we have the augmented matrix {\\widehat{\\bf A}}=\\begin{bmatrix} a_{11} & a_{12} & | & b_1\\\\\n        a_{21} & a_{22} & | & b_2\n        \\end{bmatrix} If we interchange the two rows, we get the augmented matrix \\begin{bmatrix}\n        a_{21} & a_{22} & | & b_2\\\\\n        a_{11} & a_{12} & | & b_1\n        \\end{bmatrix} which represents a linear system equivalent to that represented by matrix \\widehat{\\bf A}.\nMultiplying by a Constant: If we multiply the second row of matrix \\widehat{\\bf A} by a constant c, we get the augmented matrix \\begin{bmatrix}\n        a_{11} & a_{12} & | & b_1\\\\\n        c a_{21} & c a_{22} & | & c b_2\n        \\end{bmatrix} which represents a linear system equivalent to that represented by matrix \\widehat{\\bf A}.\nAdding (subtracting) Rows: If we add (subtract) the first row of matrix \\widehat{\\bf A} to the second, we obtain the augmented matrix \\begin{bmatrix}\n        a_{11} & a_{12} & | & b_1\\\\\n        a_{11}+a_{21} & a_{12}+a_{22} & | & b_1+b_2\n        \\end{bmatrix} which represents a linear system equivalent to that represented by matrix \\widehat{\\bf A}.\n\nExample 7.1 Solve the following system of equations by using elementary row operations: \\begin{matrix}\n      x  & - & 3y & = & -3\\\\\n      2x & + &  y & = &  8\n      \\end{matrix}\n\n\nExercise 7.3 Put the following system of equations into augmented matrix form. Then, using Gaussian or Gauss-Jordan elimination, solve the system of equations by putting the matrix into row echelon or reduced row echelon form.\n\n \\begin{cases}\n    x + y + 2z = 2\\\\\n    3x - 2y + z = 1\\\\\n    y - z = 3\n\\end{cases}\n \\begin{cases}\n    2x + 3y - z = -8\\\\\n    x + 2y - z = 12\\\\\n  -x -4y + z = -6\n\\end{cases}"
  },
  {
    "objectID": "22_linear_systems.html#answers-to-examples-and-exercises",
    "href": "22_linear_systems.html#answers-to-examples-and-exercises",
    "title": "\n7  Systems of Linear Equations\n",
    "section": "Answers to Examples and Exercises",
    "text": "Answers to Examples and Exercises\nAnswer to Exercise 7.1:\nThere are many answers to this. Some possible simple ones are as follows:\n\nOne solution: \\begin{matrix}\n           -x  & + & y & = & 0\\\\\n           x & + &  y & = &  2\n           \\end{matrix}\nNo solution: \\begin{matrix}\n           -x  & + & y & = & 0\\\\\n           x & - &  y & = &  2\n           \\end{matrix}\nInfinite solutions: \\begin{matrix}\n           -x  & + & y & = & 0\\\\\n           2x & - &  2y & = &  0\n           \\end{matrix}\n\nAnswer to Exercise 7.2:\n\\begin{bmatrix}  2 & -7 & 9 & -4 & 0 & 0| & 8\\\\  0 & 41 & 9 & 0 & 0 & 5 | & 11\\\\  1 & -15 & 0 & 0 & -11 & 0 | & 9  \\end{bmatrix}\nAnswer to Example 7.1:\n\\begin{matrix}\n      x  & - & 3y & = & -3\\\\\n      2x & + &  y & = &  8\n      \\end{matrix}\n\\begin{matrix}\n      x  & - & 3y & = & -3\\\\\n         &   & 7y & = & 14\\\\            \n      \\end{matrix}\n\\begin{matrix}\n      x  & - & 3y & = & -3\\\\\n         &   & y & = & 2\\\\          \n      \\end{matrix}\n\\begin{matrix}\n      x & = & 3\\\\\n      y & = & 2\\\\           \n      \\end{matrix}\nAnswer to Exercise 7.3:\n\nx = 2, y = 2, z = -1\nx = -17, y = -3, z = -35"
  },
  {
    "objectID": "23_matrix_inverse.html",
    "href": "23_matrix_inverse.html",
    "title": "\n8  Matrix Inverse and Linear Independence\n",
    "section": "",
    "text": "Identity Matrix: The n\\times n identity matrix {\\bf I}_n is the matrix whose diagonal elements are 1 and all off-diagonal elements are 0. Examples:  {\\bf I}_2=\\begin{bmatrix} 1&0\\\\0&1 \\end{bmatrix}, \\qquad {\\bf I}_3=\\begin{bmatrix} 1&0&0\\\\ 0&1&0\\\\\n            0&0&1 \\end{bmatrix}\nInverse Matrix: An n\\times n matrix {\\bf A} is nonsingular or invertible if there exists an n\\times n matrix {\\bf A}^{-1} such that {\\bf A} {\\bf A}^{-1} = {\\bf A}^{-1} {\\bf A} = {\\bf I}_n where {\\bf A}^{-1} is the inverse of {\\bf A}. If there is no such {\\bf A}^{-1}, then {\\bf A} is singular or not invertible.\nExample: Let {\\bf A} = \\begin{bmatrix} 2&3\\\\2&2 \\end{bmatrix}, \\qquad {\\bf B}=\\begin{bmatrix} -1&\\frac{3}{2}\\\\ 1&-1\n        \\end{bmatrix} Since {\\bf A} {\\bf B} = {\\bf B} {\\bf A} = {\\bf I}_n we conclude that {\\bf B} is the inverse, {\\bf A}^{-1}, of {\\bf A} and that {\\bf A} is nonsingular.\nProperties of the Inverse:\n\nIf the inverse exists, it is unique.\nIf {\\bf A} is nonsingular, then {\\bf A}^{-1} is nonsingular.\n({\\bf A}^{-1})^{-1} = {\\bf A}\nIf {\\bf A} and {\\bf B} are nonsingular, then {\\bf A}{\\bf B} is nonsingular\n({\\bf A}{\\bf B})^{-1} = {\\bf B}^{-1}{\\bf A}^{-1}\nIf {\\bf A} is nonsingular, then ({\\bf A}^\\top)^{-1}=({\\bf A}^{-1})^\\top\n\nProcedure to Find {\\bf A}^{-1}: We know that if {\\bf B} is the inverse of {\\bf A}, then {\\bf A} {\\bf B} = {\\bf B} {\\bf A} = {\\bf I}_n Looking only at the first and last parts of this {\\bf A} {\\bf B} = {\\bf I}_n Solving for {\\bf B} is equivalent to solving for n linear systems, where each column of {\\bf B} is solved for the corresponding column in {\\bf I}_n. We can solve the systems simultaneously by augmenting {\\bf A} with {\\bf I}_n and performing Gauss-Jordan elimination on {\\bf A}. If Gauss-Jordan elimination on [{\\bf A} | {\\bf I}_n] results in [{\\bf I}_n | {\\bf B} ], then {\\bf B} is the inverse of {\\bf A}. Otherwise, {\\bf A} is singular.\nTo summarize: To calculate the inverse of {\\bf A}\n\nForm the augmented matrix [ {\\bf A} | {\\bf I}_n]\nUsing elementary row operations, transform the augmented matrix to reduced row echelon form.\n\nThe result of step 2 is an augmented matrix [ {\\bf C} | {\\bf B} ].\n\nIf {\\bf C}={\\bf I}_n, then {\\bf B}={\\bf A}^{-1}.\nIf {\\bf C}\\ne{\\bf I}_n, then \\bf C has a row of zeros. This means {\\bf A} is singular and {\\bf A}^{-1} does not exist.\n\n\n\n\nExample 8.1 Find the inverse of the following matrix:\n{\\bf A}=\\begin{bmatrix} 1&1&1\\\\0&2&3\\\\5&5&1 \\end{bmatrix}\n\n\nExercise 8.1 Find the inverse of the following matrix:\n{\\bf A}=\\begin{bmatrix} 1&0&4\\\\0&2&0\\\\0&0&1 \\end{bmatrix}"
  },
  {
    "objectID": "23_matrix_inverse.html#linear-systems-and-inverse",
    "href": "23_matrix_inverse.html#linear-systems-and-inverse",
    "title": "\n8  Matrix Inverse and Linear Independence\n",
    "section": "\n8.2 Linear Systems and Inverse",
    "text": "8.2 Linear Systems and Inverse\nLet’s return to the matrix representation of a linear system\n\\bf{Ax} = \\bf{b}\nIf \\bf{A} is an n\\times n matrix,then \\bf{Ax}=\\bf{b} is a system of n equations in n unknowns. Suppose \\bf{A} is nonsingular. Then \\bf{A}^{-1} exists. To solve this system, we can multiply each side by \\bf{A}^{-1} and reduce it as follows: \\begin{align*} \\bf{A}^{-1} (\\bf{A} \\bf{x}) & =  \\bf{A}^{-1} \\bf{b} \\\\ (\\bf{A}^{-1} \\bf{A})\\bf{x} & =  \\bf{A}^{-1} \\bf{b}\\\\ \\bf{I}_n \\bf{x}     & =  \\bf{A}^{-1} \\bf{b}\\\\ \\bf{x} & =  \\bf{A}^{-1} \\bf{b} \\end{align*}\nHence, given \\bf{A} and \\bf{b} and given that \\bf{A} is nonsingular, then \\bf{x} = \\bf{A}^{-1} \\bf{b} is a unique solution to this system.\n\nExercise 8.2 Use the inverse matrix to solve the following linear system:\n\\begin{align*}\n-3x + 4y &= 5 \\\\\n2x - y &= -10\n\\end{align*}\nHint: the linear system above can be written in the matrix form\n\\textbf{A}\\textbf{z} = \\textbf{b}\ngiven\n\\textbf{A} = \\begin{bmatrix} -3&4\\\\2&-1 \\end{bmatrix},\n\\textbf{z} = \\begin{bmatrix} x\\\\y \\end{bmatrix},\nand\n\\textbf{b} = \\begin{bmatrix} 5\\\\-10 \\end{bmatrix}"
  },
  {
    "objectID": "23_matrix_inverse.html#when-is-matrix-invertible",
    "href": "23_matrix_inverse.html#when-is-matrix-invertible",
    "title": "\n8  Matrix Inverse and Linear Independence\n",
    "section": "\n8.3 When is Matrix Invertible?",
    "text": "8.3 When is Matrix Invertible?\nThe following statements for an n\\times n square matrix \\mathbf{A} are equivalent:\n\n\n\\mathbf{A} is invertible: \n  \\mathbf{A}^{-1} \\textrm{ exists and } \\mathbf{A}^{-1}\\mathbf{A}=\\mathbf{I}_n\n  \n\nThe system \\mathbf{A}\\mathbf{x} = \\mathbf{b} has a unique solution for all \\mathbf{b}\\neq\\mathbf{0} (zero vector)\nIf \\mathbf{A}\\mathbf{x} = \\mathbf{0} (zero vector), it implies that \\mathbf{x} = \\mathbf{0} (zero vector)\nThe column vectors in \\mathbf{A} are linearly independent and spans \\mathbb{R}^n\n\nThe rank of \\mathbf{A} is n: \n  \\mathrm{rank}(\\mathbf{A})=n\n  \n\nThe determinant of \\mathbf{A} is not zero: \n  \\det(\\mathbf{A}) \\neq 0\n  \n\n\nConceptually, we want columns of \\mathbf{A} contains enough vectors to reach all coordinates (span) \\mathbb{R}^n.\nHowever, when we include too many vectors, we will have redundant vectors when the new vectors does not provide new information (you can already express the new vectors using the existing vectors) and so the vectors become linearly dependent"
  },
  {
    "objectID": "23_matrix_inverse.html#linearindependence",
    "href": "23_matrix_inverse.html#linearindependence",
    "title": "\n8  Matrix Inverse and Linear Independence\n",
    "section": "\n8.4 Linear Independence",
    "text": "8.4 Linear Independence\nLinear Dependence: A set of vectors \\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\} is linearly dependent if some vector \\mathbf{v}_i is a linear combination of the other vectors \\mathbf{v}_j\nIf all vectors cannot be written as linear combinations of the other vectors, then the set of vectors are linearly independent\n\\mathbf{v}_i is not a linear combination of the other vectors \\mathbf{v}_j, i.e.,\n\n{\\color{red}\\mathbf{v}_i}\n\\neq\n\\sum_{j\\neq i} c_j \\mathbf{v}_j\n= \\underbrace{c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_n \\mathbf{v}_n}_{\\textrm{linear combination {\\color{red}excluding} } {\\color{red}\\mathbf{v}_i}}\n\\;\\;\\;\\;\\;\\; \\textrm{ for all } i\n\nSubtracting \\mathbf{v}_i, this is equivalent to\n\n\\underbrace{\\mathbf{0}}_{\\textrm{zero vector!}}\n\\neq\n{\\color{red} - \\mathbf{v}_i} +\n\\sum_{j\\neq i} c_j \\mathbf{v}_j\n=\n\\underbrace{c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + {\\color{red} (-1) \\mathbf{v}_i} + \\cdots + c_n \\mathbf{v}_n}_{\\textrm{linear combination of } \\mathbf{v}_1, \\mathbf{v}_2, \\ldots, {\\color{red} \\mathbf{v}_i}, \\ldots, \\mathbf{v}_n}\n\\;\\; \\textrm{ for all } i.\n\nThis is equivalent to saying: The only solution to\n\n\\underbrace{\\mathbf{0}}_{\\textrm{zero vector!}}\n=\n\\underbrace{c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots +  \\mathbf{v}_i + \\cdots + c_n \\mathbf{v}_n}_{\\textrm{linear combination of } \\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_i, \\ldots, \\mathbf{v}_n}\n\nis c_1 = c_2 = \\cdots = c_n = 0.\nThis is equivalent to the statement that \\mathbf{A}\\mathbf{x} = \\mathbf{0} implies \\mathbf{x} = \\mathbf{0}.\nLinear independence: A set of vectors {\\bf v}_1, {\\bf v}_2, \\cdots , {\\bf v}_k is linearly independent if the only solution to the equation\nc_1{\\bf v}_1 + c_2{\\bf v}_2 +  \\cdots + c_k{\\bf v}_k = 0\nis c_1 = c_2 = \\cdots = c_k = 0. If another solution exists, the set of vectors is linearly dependent.\nA set S of vectors is linearly dependent if and only if at least one of the vectors in S can be written as a linear combination of the other vectors in S.\nLinear independence is only defined for sets of vectors with the same number of elements; any linearly independent set of vectors in n-space contains at most n vectors.\nSince \\begin{bmatrix}9 \\\\ 13 \\\\ 17 \\end{bmatrix} is a linear combination of \\begin{bmatrix}1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\begin{bmatrix} 2 \\\\ 3\\\\ 4\\end{bmatrix}, and \\begin{bmatrix} 3 \\\\ 4 \\\\ 5 \\end{bmatrix}, these 4 vectors constitute a linearly dependent set.\n\nExample 8.2 Are the following sets of vectors linearly independent?\n\n\n\\begin{bmatrix}2 \\\\ 3 \\\\ 1 \\end{bmatrix} and \\begin{bmatrix}4 \\\\ 6 \\\\ 1 \\end{bmatrix}\n\n\n\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix}0 \\\\ 5 \\\\ 0 \\end{bmatrix}, and \\begin{bmatrix}10 \\\\ 10 \\\\ 0 \\end{bmatrix}\n\n\n\n\nExercise 8.3 Are the following sets of vectors linearly independent?\n\n{\\bf v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} , {\\bf v}_2 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix} , {\\bf v}_3 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \n{\\bf v}_1 = \\begin{bmatrix} 2 \\\\ 2 \\\\ 1 \\end{bmatrix} , {\\bf v}_2 = \\begin{bmatrix} -4 \\\\ 6 \\\\ 5 \\end{bmatrix} , {\\bf v}_3 = \\begin{bmatrix} -2 \\\\ 8 \\\\ 6 \\end{bmatrix}"
  },
  {
    "objectID": "23_matrix_inverse.html#rank-of-a-matrix",
    "href": "23_matrix_inverse.html#rank-of-a-matrix",
    "title": "\n8  Matrix Inverse and Linear Independence\n",
    "section": "\n8.5 Rank of a Matrix",
    "text": "8.5 Rank of a Matrix\nAnother way to think about linear independence is to consider the rank of a matrix.\nRank: The rank of a matrix is the number of linearly independent rows or columns in the matrix. The rank of a matrix is denoted by \\textrm{rank}(\\mathbf{A}).\nFor example \\begin{bmatrix} 1 & 2 & 3 \\\\\n              0 & 4 & 5 \\\\\n              0 & 0 & 6 \\end{bmatrix}\nRank = 3\n\\begin{bmatrix} 1 & 2 & 3 \\\\\n    0 & 4 & 5 \\\\\n    0 & 0 & 0 \\end{bmatrix}\nRank = 2\n\nExercise 8.4 Find the rank of each matrix below:\n(Hint: transform the matrices into row echelon form. Remember that the number of nonzero rows of a matrix in row echelon form is the rank of that matrix)\n\n\\begin{bmatrix} 1 & 1 & 2 \\\\\n  2 & 1 & 3 \\\\\n  1 & 2 & 3 \\end{bmatrix}\n\\begin{bmatrix} 1 & 3 & 3 & -3 & 3\\\\\n1 & 3 & 1 & 1 & 3 \\\\\n1 & 3 & 2 & -1 & -2 \\\\\n1 & 3 & 0 & 3 & -2 \\end{bmatrix}\n\n\nAnswer to Exercise 8.4:\n\nrank is 2\nrank is 3"
  },
  {
    "objectID": "23_matrix_inverse.html#determinants",
    "href": "23_matrix_inverse.html#determinants",
    "title": "\n8  Matrix Inverse and Linear Independence\n",
    "section": "\n8.6 Determinants",
    "text": "8.6 Determinants\n\n\nSingularity: Determinants can be used to determine whether a square matrix is nonsingular.\nA square matrix is nonsingular if and only if its determinant is not zero.\nDeterminant of a 1 \\times 1 matrix, equals |\\mathbf{A}|=|a_{11}|=a_{11}\nDeterminant of a 2 \\times 2 matrix,\n\\mathbf{A}=\\begin{vmatrix} a_{11}&a_{12}\\\\ a_{21}&a_{22} \\end{vmatrix}\n\\begin{align*}\\det({\\bf A}) &= |{\\bf A}|\\\\\n        &= a_{11}|a_{22}| - a_{12}|a_{21}|\\\\\n        &= a_{11}a_{22} - a_{12}a_{21}\n\\end{align*}\nWe can extend the second to last equation above to get the definition of the determinant of a 3 \\times 3 matrix: \\begin{align*}\n        \\begin{vmatrix} a_{11}&a_{12}&a_{13}\\\\  a_{21} & a_{22}&a_{23}\\\\ a_{31}&a_{32}&a_{33} \\end{vmatrix}\n            &=\n            a_{11} \\begin{vmatrix} a_{22}&a_{23}\\\\ a_{32}&a_{33} \\end{vmatrix}\n            - a_{12} \\begin{vmatrix} a_{21}&a_{23}\\\\ a_{31}&a_{33} \\end{vmatrix}\n            + a_{13} \\begin{vmatrix} a_{21}&a_{22}\\\\ a_{31}&a_{32}\n            \\end{vmatrix}\\\\\n            &= a_{11}(a_{22}a_{33} - a_{23}a_{32}) - a_{12}(a_{21}a_{33} - a_{23}a_{31}) + a_{13}(a_{21}a_{32} - a_{22}a_{31})\n\\end{align*}\nLet’s extend this now to any n\\times n matrix. Let’s define \\mathbf{A}_{ij} as the (n-1)\\times (n-1) submatrix of \\mathbf{A} obtained by deleting row i and column j. Let the (i,j)th minor of \\mathbf{A} be the determinant of \\mathbf{A}_{ij}:\nM_{ij} = \\left|\\mathbf{A}_{ij}\\right|\nThen for any n\\times n matrix \\mathbf{A}\n|\\mathbf{A}| = a_{11}M_{11} - a_{12}M_{12} + \\cdots + (-1)^{n+1} a_{1n} M_{1n}\nFor example, in figuring out whether the following matrix has an inverse?\n\\mathbf{A}=\\begin{bmatrix} 1&1&1\\\\0&2&3\\\\5&5&1 \\end{bmatrix}\n\nCalculate its determinant. \\begin{align*}\n|\\mathbf{A}| &= 1(2-15) - 1(0-15) + 1(0-10) \\nonumber\\\\\n&= -13+15-10 \\nonumber\\\\\n&= -8\\nonumber\n\\end{align*}\nSince |{\\bf A}|\\ne 0, we conclude that {\\bf A} has an inverse.\n\n\nExercise 8.5 Determine whether the following matrices are nonsingular:\n\n\\begin{bmatrix}\n  1 & 0 & 1\\\\\n  2 & 1 & 2\\\\\n  1 & 0 & -1\n  \\end{bmatrix}\n\\begin{bmatrix}\n     2 & 1 & 2\\\\\n     1 & 0 & 1\\\\\n     4 & 1 & 4\n\\end{bmatrix}"
  },
  {
    "objectID": "23_matrix_inverse.html#getting-inverse-of-a-matrix-using-its-determinant",
    "href": "23_matrix_inverse.html#getting-inverse-of-a-matrix-using-its-determinant",
    "title": "\n8  Matrix Inverse and Linear Independence\n",
    "section": "\n8.7 Getting Inverse of a Matrix using its Determinant",
    "text": "8.7 Getting Inverse of a Matrix using its Determinant\nThus far, we have a number of algorithms to\n\nFind the solution of a linear system,\nFind the inverse of a matrix\n\nbut these remain just that — algorithms. At this point, we have no way of telling how the solutions x_j change as the parameters a_{ij} and b_i change, except by changing the values and “rerunning” the algorithms.\nWith determinants, we can provide an explicit formula for the inverse and therefore provide an explicit formula for the solution of an n\\times n linear system.\nHence, we can examine how changes in the parameters and b_i affect the solutions x_j.\nDeterminant Formula for the Inverse of a 2 \\times 2:\nThe determinant of a 2 \\times 2 matrix \\mathbf{A}=\\begin{bmatrix} a & b\\\\ c & d\\\\ \\end{bmatrix} is defined as:\n\\det(\\mathbf{A}) = a d - b c\nThe inverse of \\mathbf{A} is given by:\n\\frac{1}{\\det({\\bf A})} \\begin{bmatrix} d & -b \\\\ -c & a\\\\ \\end{bmatrix}\nFor example, Let’s calculate the inverse of matrix A from Exercise 8.2 using the determinant formula.\nRecall,\n\\mathbf{A} = \\begin{bmatrix}\n              -3 & 4\\\\\n              2 & -1\\\\\n          \\end{bmatrix}\n\\det(\\mathbf{A}) = (-3)(-1) - (4)(2) = 3 - 8  = -5\n\\frac{1}{\\det(\\mathbf{A})} \\begin{bmatrix}\n              -1 & -4\\\\\n              -2 & -3\\\\\n          \\end{bmatrix}\n\\frac{1}{-5} \\begin{bmatrix}\n              -1 & -4\\\\\n              -2 & -3\\\\\n          \\end{bmatrix}\n\\begin{bmatrix}\n              \\frac{1}{5} & \\frac{4}{5}\\\\\n              \\frac{2}{5} & \\frac{3}{5}\\\\\n          \\end{bmatrix}\n\nExercise 8.6 Caculate the inverse of \\mathbf{A} \\mathbf{A} = \\begin{bmatrix}\n              3 & 5\\\\\n              -7 & 2\\\\\n          \\end{bmatrix}"
  },
  {
    "objectID": "23_matrix_inverse.html#answers-to-examples-and-exercises",
    "href": "23_matrix_inverse.html#answers-to-examples-and-exercises",
    "title": "\n8  Matrix Inverse and Linear Independence\n",
    "section": "Answers to Examples and Exercises",
    "text": "Answers to Examples and Exercises\nAnswer to Example 8.2:\n\nyes\nno\n\nAnswer to Exercise 8.3:\n\nyes\nno (-v_1 -v_2 + v_3 = 0)\n\nAnswer to Exercise 8.4:\n\nrank is 2\nrank is 3\n\nAnswer to Example 8.1:\n\\left(\\begin{array}{ccc|ccc}  1&1&1&1&0&0\\\\  0&2&3&0&1&0\\\\  5&5&1&0&0&1 \\end{array} \\right)\n\\left(\\begin{array}{ccc|ccc}  1&1&1 &1 &0&0\\\\  0&2&3 &0 &1&0\\\\  0&0&-4&-5&0&1 \\end{array} \\right)\n\\left(\\begin{array}{ccc|ccc}  1&1&1&1 &0&0\\\\  0&2&3&0 &1&0\\\\  0&0&1&5/4&0&-1/4 \\end{array} \\right)\n\\left(\\begin{array}{ccc|ccc}  1&1&0&-1/4 &0&1/4\\\\  0&2&0&-15/4&1&3/4\\\\  0&0&1&5/4 &0&-1/4 \\end{array} \\right)\n\\left(\\begin{array}{ccc|ccc}  1&1&0&-1/4 &0 &1/4\\\\  0&1&0&-15/8&1/2&3/8\\\\  0&0&1&5/4 &0 &-1/4 \\end{array} \\right)\n\\left(\\begin{array}{ccc|ccc}  1&0&0&13/8 &-1/2&-1/8\\\\  0&1&0&-15/8&1/2 &3/8\\\\  0&0&1&5/4 &0 &-1/4 \\end{array} \\right)\n{\\bf A}^{-1} = \\left(\\begin{array}{ccc}  13/8 &-1/2&-1/8\\\\  -15/8&1/2 &3/8\\\\  5/4 &0 &-1/4 \\end{array} \\right)\nAnswer to Exercise 8.1:\n\n{\\bf A}^{-1}=\\begin{bmatrix} 1&0&-4\\\\0&\\frac{1}{2}&0\\\\0&0&1 \\end{bmatrix}\n\nAnswer to Exercise 8.2:\n\\textbf{z} = \\bf{A}^{-1} \\bf{b} = \\begin{bmatrix}  1/5 &4/5\\\\  2/5&3/5 \\end{bmatrix} \\begin{bmatrix}  5 \\\\  -10 \\end{bmatrix}= \\begin{bmatrix}  -7 \\\\  -4 \\end{bmatrix} = \\begin{bmatrix}  x \\\\  y \\end{bmatrix}\nAnswer to Exercise 8.5:\n\nnonsingular\nsingular\n\nAnswer to Exercise 8.6:\n\\begin{bmatrix}  \\frac{2}{41} & \\frac{-5}{41}\\\\  \\frac{7}{41} & \\frac{3}{41}\\\\  \\end{bmatrix}"
  },
  {
    "objectID": "31_limits.html",
    "href": "31_limits.html",
    "title": "9  Limits",
    "section": "",
    "text": "Solving limits, i.e. finding out the value of functions as its input moves closer to some value, is important for the social scientist’s mathematical toolkit for two related tasks. The first is for the study of calculus, which will be in turn useful to show where certain functions are maximized or minimized. The second is for the study of statistical inference, which is the study of inferring things about things you cannot see by using things you can see."
  },
  {
    "objectID": "31_limits.html#example-the-central-limit-theorem",
    "href": "31_limits.html#example-the-central-limit-theorem",
    "title": "9  Limits",
    "section": "Example: The Central Limit Theorem",
    "text": "Example: The Central Limit Theorem\nPerhaps the most important theorem in statistics is the Central Limit Theorem,\n\nTheorem 9.1 (Central Limit Theorem (i.i.d. case)) For any series of independent and identically distributed random variables X_1, X_2, \\cdots, we know the distribution of its sum even if we do not know the distribution of X. The distribution of the sum is a Normal distribution.\n\\frac{\\bar{X}_n - \\mu}{\\sigma / \\sqrt{n}} \\xrightarrow{d} \\text{Normal}(0, 1),\nwhere \\mu is the mean of X and \\sigma is the standard deviation of X. The arrow is read as “converges in distribution to”. \\text{Normal}(0, 1) indicates a Normal Distribution with mean 0 and variance 1.\nThat is, the limit of the distribution of the lefthand side is the distribution of the righthand side.\n\nThe sign of a limit is the arrow “\\rightarrow”. Although we have not yet covered probability so we have not described what distributions and random variables are, it is worth foreshadowing the Central Limit Theorem. The Central Limit Theorem is powerful because it gives us a guarantee of what would happen if n \\rightarrow \\infty, which in this case means we collected more data."
  },
  {
    "objectID": "31_limits.html#example-the-law-of-large-numbers",
    "href": "31_limits.html#example-the-law-of-large-numbers",
    "title": "9  Limits",
    "section": "Example: The Law of Large Numbers",
    "text": "Example: The Law of Large Numbers\nA finding that perhaps rivals the Central Limit Theorem is the Law of Large Numbers:\n\nTheorem 9.2 ((Weak) Law of Large Numbers) For any draw of identically distributed independent variables with mean \\mu, the sample average after n draws, \\bar{X}_n, converges in probability to the true mean as n \\rightarrow \\infty:\n\\lim\\limits_{n\\to \\infty} P(|\\bar{X}_n - \\mu | > \\varepsilon) = 0\nA shorthand of which is \\bar{X}_n \\xrightarrow{p} \\mu, where the arrow is read as “converges in probability to”.\n\nIntuitively, the more data, the more accurate is your guess. For example, Figure 9.1 shows how the sample average from many coin tosses converges to the true value : 0.5.\n\n\n\n\nFigure 9.1: As the number of coin tosses goes to infinity, the average probabiity of heads converges to 0.5"
  },
  {
    "objectID": "31_limits.html#sequences",
    "href": "31_limits.html#sequences",
    "title": "9  Limits",
    "section": "\n9.1 Sequences",
    "text": "9.1 Sequences\nWe need a couple of steps until we get to limit theorems in probability. First we will introduce a “sequence”, then we will think about the limit of a sequence, then we will think about the limit of a function.\nA sequence \\{x_n\\}=\\{x_1, x_2, x_3, \\ldots, x_n\\} is an ordered set of real numbers, where x_1 is the first term in the sequence and y_n is the nth term. Generally, a sequence is infinite, that is it extends to n=\\infty. We can also write the sequence as \\{x_n\\}^\\infty_{n=1}\nwhere the subscript and superscript are read together as “from 1 to infinity.”\n\nExample 9.1 How do these sequences behave?\n\n\\{A_n\\}=\\left\\{ 2-\\frac{1}{n^2} \\right\\}\n\\{B_n\\}=\\left\\{\\frac{n^2+1}{n} \\right\\}\n\\{C_n\\}=\\left\\{(-1)^n \\left(1-\\frac{1}{n}\\right) \\right\\}\n\n\nWe find the sequence by simply “plugging in” the integers into each n. The important thing is to get a sense of how these numbers are going to change. Example 1’s numbers seem to come closer and closer to 2, but will it ever surpass 2? Example 2’s numbers are also increasing each time, but will it hit a limit? What is the pattern in Example 3? Graphing helps you make this point more clearly. See the sequence of n = 1, ...20 for each of the three examples in Figure 9.2.\n\n\n\n\nFigure 9.2: Behavior of Some Sequences"
  },
  {
    "objectID": "31_limits.html#the-limit-of-a-sequence",
    "href": "31_limits.html#the-limit-of-a-sequence",
    "title": "9  Limits",
    "section": "\n9.2 The Limit of a Sequence",
    "text": "9.2 The Limit of a Sequence\nThe notion of “converging to a limit” is the behavior of the points in Example 9.1. In some sense, that’s the counterfactual we want to know. What happens as n\\rightarrow \\infty?\n\nSequences like 1 above that converge to a limit.\nSequences like 2 above that increase without bound.\nSequences like 3 above that neither converge nor increase without bound — alternating over the number line.\n\n\nDefinition 9.1 (Limit of a Sequence) The sequence \\{y_n\\} has the limit L, which we write as \\lim\\limits_{n \\to \\infty} y_n =L, if for any \\epsilon>0 there is an integer N (which depends on \\epsilon) with the property that |y_n -L|<\\epsilon for each n>N. \\{y_n\\} is said to converge to L. If the above does not hold, then \\{y_n\\} diverges.\n\nWe can also express the behavior of a sequence as bounded or not:\n\nBounded: if |y_n|\\le K for all n\n\nMonotonically Increasing: y_{n+1}>y_n for all n\n\nMonotonically Decreasing: y_{n+1}<y_n for all n\n\n\nA limit is unique: If \\{y_n\\} converges, then the limit L is unique.\nIf a sequence converges, then the sum of such sequences also converges. Let \\lim\\limits_{n \\to \\infty} y_n = y and \\lim\\limits_{n \\to \\infty} z_n =z. Then\n\n\\lim\\limits_{n \\to \\infty} [k y_n + \\ell z_n]= k y + \\ell z\n\\lim\\limits_{n \\to \\infty} y_n z_n = yz\n\n\\lim\\limits_{n \\to \\infty} \\frac{y_n}{z_n} = \\frac{y}{z}, provided z\\neq 0\n\n\nThis looks reasonable enough. The harder question, obviously is when the parts of the fraction don’t converge. If \\lim_{n\\to\\infty} y_n = \\infty and \\lim_{n\\to\\infty} z_n = \\infty, What is \\lim_{n\\to\\infty} y_n - z_n? What is \\lim_{n\\to\\infty} \\frac{y_n}{z_n}?\nIt is nice for a sequence to converge in limit. We want to know if complex-looking sequences converge or not. The name of the game here is to break that complex sequence up into sums of simple fractions where n only appears in the denominator: \\frac{1}{n}, \\frac{1}{n^2}, and so on. Each of these will converge to 0, because the denominator gets larger and larger. Then, because of the properties above, we can then find the final sequence.\n\nExample 9.2 Find the limit of\n\\lim_{n\\to \\infty} \\frac{n + 3}{n}.\nAt first glance, n + 3 and n both grow to \\infty, so it looks like we need to divide infinity by infinity. However, we can express this fraction as a sum, then the limits apply separately:\n\\lim_{n\\to \\infty} \\frac{n + 3}{n} = \\lim_{n\\to \\infty} \\left(1 + \\frac{3}{n}\\right) =  \\underbrace{\\lim_{n\\to \\infty}1}_{1} +  \\underbrace{\\lim_{n\\to \\infty}\\left(\\frac{3}{n}\\right)}_{0}\nso, the limit is actually 1.\n\nAfter some practice, the key to intuition is whether one part of the fraction grows “faster” than another. If the denominator grows faster to infinity than the numerator, then the fraction will converge to 0, even if the numerator will also increase to infinity. In a sense, limits show how not all infinities are the same.\n\nExercise 9.1 Find the following limits of sequences, then explain in English the intuition for why that is the case.\n\n\\lim\\limits_{n\\to\\infty} \\frac{2n}{n^2 + 1}\n\\lim\\limits_{n\\to\\infty} (n^3 - 100n^2)"
  },
  {
    "objectID": "31_limits.html#limitsfun",
    "href": "31_limits.html#limitsfun",
    "title": "9  Limits",
    "section": "\n9.3 Limits of a Function",
    "text": "9.3 Limits of a Function\nWe’ve now covered functions and just covered limits of sequences, so now is the time to combine the two.\nA function f is a compact representation of some behavior we care about. Like for sequences, we often want to know if f(x) approaches some number L as its independent variable x moves to some number c (which is usually 0 or \\pm\\infty). If it does, we say that the limit of f(x), as x approaches c, is L: \\lim\\limits_{x \\to c} f(x)=L. Unlike a sequence, x is a continuous number, and we can move in decreasing order as well as increasing.\nFor a limit L to exist, the function f(x) must approach L from both the left (increasing) and the right (decreasing).\n\nDefinition 9.2 (Limit of a function) Let f(x) be defined at each point in some open interval containing the point c. Then L equals \\lim\\limits_{x \\to c} f(x) if for any (small positive) number \\epsilon, there exists a corresponding number \\delta>0 such that if 0<|x-c|<\\delta, then |f(x)-L|<\\epsilon.\n\nA neat, if subtle result is that f(x) does not necessarily have to be defined at c for \\lim\\limits_{x \\to c} to exist.\n\nProposition 9.1 Let f and g be functions with \\lim\\limits_{x \\to c} f(x)=k and \\lim\\limits_{x \\to c} g(x)=\\ell.\n\n\\lim\\limits_{x \\to c}[f(x)+g(x)]=\\lim\\limits_{x \\to c} f(x)+ \\lim\\limits_{x \\to c} g(x)\n\\lim\\limits_{x \\to c} kf(x) = k\\lim\\limits_{x \\to c} f(x)\n\\lim\\limits_{x \\to c} f(x) g(x) = \\left[\\lim\\limits_{x \\to c} f(x)\\right]\\cdot \\left[\\lim\\limits_{x \\to c} g(x)\\right]\n\n\\lim\\limits_{x \\to c} \\frac{f(x)}{g(x)} = \\frac{\\lim\\limits_{x \\to c} f(x)}{\\lim\\limits_{x \\to c} g(x)}, provided \\lim\\limits_{x \\to c} g(x)\\ne 0.\n\n\nSimple limits of functions can be solved as we did limits of sequences. Just be careful which part of the function is changing.\n\nExample 9.3 Find the limit of the following functions.\n\n\\lim_{x \\to c} k\n\\lim_{x \\to c} x\n\\lim_{x\\to 2} (2x-3)\n\\lim_{x \\to c} x^n\n\n\nLimits can get more complex in roughly two ways. First, the functions may become large polynomials with many moving pieces. Second,the functions may become discontinuous.\nThe function can be thought of as a more general or “smooth” version of sequences. For example,\n\nExercise 9.2 Find the limit of\n\\lim_{x\\to\\infty} \\frac{(x^4 +3x−99)(2−x^5)}{(18x^7 +9x^6 −3x^2 −1)(x+1)}\n\nNow, the functions will become a bit more complex:\n\nExercise 9.3 Solve the following limits of functions\n\n\\lim\\limits_{x\\to 0} |x|\n\\lim\\limits_{x\\to 0} \\left(1+\\frac{1}{x^2}\\right)\n\n\nSo there are a few more alternatives about what a limit of a function could be:\n\nRight-hand limit: The value approached by f(x) when you move from right to left.\nLeft-hand limit: The value approached by f(x) when you move from left to right.\nInfinity: The value approached by f(x) as x grows infinitely large. Sometimes this may be a number; sometimes it might be \\infty or -\\infty.\nNegative infinity: The value approached by f(x) as x grows infinitely negative. Sometimes this may be a number; sometimes it might be \\infty or -\\infty.\n\nThe distinction between left and right becomes important when the function is not determined for some values of x. What are those cases in the examples below?\n\n\n\n\nFunctions which are not defined in some areas"
  },
  {
    "objectID": "31_limits.html#continuity",
    "href": "31_limits.html#continuity",
    "title": "9  Limits",
    "section": "\n9.4 Continuity",
    "text": "9.4 Continuity\nTo repeat a finding from the limits of functions: f(x) does not necessarily have to be defined at c for \\lim\\limits_{x \\to c} to exist. Functions that have breaks in their lines are called discontinuous. Functions that have no breaks are called continuous. Continuity is a concept that is more fundamental to, but related to that of “differentiability”, which we will cover next in calculus.\n\nDefinition 9.3 (Continuity”) Suppose that the domain of the function f includes an open interval containing the point c. Then f is continuous at c if \\lim\\limits_{x \\to c} f(x) exists and if \\lim\\limits_{x \\to c} f(x)=f(c). Further, f is continuous on an open interval (a,b) if it is continuous at each point in the interval.\n\nTo prove that a function is continuous for all points is beyond this practical introduction to math, but the general intuition can be grasped by graphing.\n\nExample 9.4 For each function, determine if it is continuous or discontinuous.\n\nf(x) = \\sqrt{x}\nf(x) = e^x\nf(x) = 1 + \\frac{1}{x^2}\n\nf(x) = \\text{floor}(x).\n\nThe floor is the smaller of the two integers bounding a number. So \\text{floor}(x = 2.999) = 2, \\text{floor}(x = 2.0001) = 2, and \\text{floor}(x = 2) = 2.\n\n\nSolution. In Figure Figure 9.3, we can see that the first two functions are continuous, and the next two are discontinuous. f(x) = 1 + \\frac{1}{x^2} is discontinuous at x= 0, and f(x) = \\text{floor}(x) is discontinuous at each whole number.\n\n\n\n\n\nFigure 9.3: Continuous and Discontinuous Functions\n\n\n\n\nSome properties of continuous functions:\n\n\nIf f and g are continuous at point c, then f+g, f-g, f \\cdot g, |f|, and \\alpha f are continuous at point c also. f/g is continuous, provided g(c)\\ne 0.\nBoundedness: If f is continuous on the closed bounded interval [a,b], then there is a number K such that |f(x)|\\le K for each x in [a,b].\nMax/Min: If f is continuous on the closed bounded interval [a,b], then f has a maximum and a minimum on [a,b]. They may be located at the end points.\n\n\n\nExercise 9.4 Let f(x) = \\frac{x^2 + 2x}{x}.\n\nGraph the function. Is it defined everywhere?\nWhat is the functions limit at x \\rightarrow 0?"
  },
  {
    "objectID": "31_limits.html#answers-to-examples",
    "href": "31_limits.html#answers-to-examples",
    "title": "9  Limits",
    "section": "Answers to Examples",
    "text": "Answers to Examples\nExample 9.1\nSolution.\n\n\\{A_n\\}=\\left\\{ 2-\\frac{1}{n^2} \\right\\} = \\left\\{1, \\frac{7}{4}, \\frac{17}{9}, \\frac{31}{16}, \\frac{49}{25}, \\ldots\\right\\} = 2\n\\{B_n\\}=\\left\\{\\frac{n^2+1}{n} \\right\\} = \\left\\{2, \\frac{5}{2}, \\frac{10}{3}, \\frac{17}{4}..., \\right\\}\n\\{C_n\\}=\\left\\{(-1)^n \\left(1-\\frac{1}{n}\\right) \\right\\} = \\left\\{0, \\frac{1}{2}, -\\frac{2}{3}, \\frac{3}{4}, -\\frac{4}{5}\\right\\}\n\nExercise 9.1\n\nSolution. Plot the function and you’ll see the following limits:\n\n0\n\\infty\n\n\nExample 9.3\nSolution.\n\nk\nc\n\\lim_{x\\to 2} (2x-3) = 2\\lim\\limits_{x\\to 2} x - 3\\lim\\limits_{x\\to 2} 1 = 1\n\\lim_{x \\to c} x^n = \\lim\\limits_{x \\to c} x \\cdots[\\lim\\limits_{x \\to c} x] = c\\cdots c =c^n\n\nExercise 9.2\n\nSolution. Although this function seems large, the thing our eyes should focus on is where the highest order polynomial remains. That will grow the fastest, so if the highest order term is on the denominator, the fraction will converge to 0, if it is on the numerator it will converge to negative infinity. Previewing the multiplication by hand, we can see that the -x^9 on the numerator will be the largest power. So the answer will be -\\infty. We can also confirm this by writing out fractions:\n\\begin{align*}  \n& \\lim_{x\\to\\infty}\\frac{\\left(1 + \\frac{3}{x^3} - \\frac{99}{4x^4}\\right)\\left(-\\frac{2}{x^5} + 1\\right)}{\\left(1 + \\frac{9}{18x} - \\frac{3}{18x^5} - \\frac{1}{18x^7} \\right)\\left(1 + \\frac{1}{x}\\right)} \\\\\n&\\times \\frac{x^4}{1} \\times -\\frac{x^5}{1} \\times \\frac{1}{18x^7}\\times \\frac{1}{x}\\\\\n=& 1 \\times \\lim_{-x\\to\\infty} \\frac{x}{18}\n\\end{align*}\n\nExercise 9.4\n\nSolution. See Figure 9.4. We can say \\lim_{x\\to 0}f(x) = 2. Note that we can express f(x) =\n    \\left\\{\n    \\begin{array}{ll}\n        x+2 & x \\neq 2; \\\\\n        \\textrm{undefined} & x = 2 \\\\\n    \\end{array}\n    \\right.\n\n\n\n\n\nFigure 9.4: A function undedefined at x = 0"
  },
  {
    "objectID": "32_derivatives.html",
    "href": "32_derivatives.html",
    "title": "10  Differential Calculus",
    "section": "",
    "text": "Calculus is a fundamental part of any type of statistics exercise. Although you may not be taking derivatives and integral in your daily work as an analyst, calculus undergirds many concepts we use: maximization, expectation, and cumulative probability."
  },
  {
    "objectID": "32_derivatives.html#sec-derivintro",
    "href": "32_derivatives.html#sec-derivintro",
    "title": "10  Differential Calculus",
    "section": "\n10.1 Derivatives",
    "text": "10.1 Derivatives\nThe derivative of f at x is its rate of change at x: how much f(x) changes with a change in x. The rate of change is a fraction — rise over run — but because not all lines are straight and the rise over run formula will give us different values depending on the range we examine, we need to take a limit.\n\nDefinition 10.1 (Derivative) Let f be a function whose domain includes an open interval containing the point x. The derivative of f at x is given by\n\\frac{d}{dx}f(x) =\\lim\\limits_{h\\to 0} \\frac{f(x+h)-f(x)}{(x+h)-x} = \\lim\\limits_{h\\to 0} \\frac{f(x+h)-f(x)}{h}\nThere are a two main ways to denote a derivate:\n\nLeibniz Notation: \\frac{d}{dx}(f(x))\n\nPrime or Lagrange Notation: f'(x)\n\n\n\nIf f(x) is a straight line, the derivative is the slope. For a curve, the slope changes by the values of x, so the derivative is the slope of the line tangent to the curve at x. See, For example, Figure 10.1.\n\n\n\n\nFigure 10.1: The Derivative as a Slope\n\n\n\n\nIf f'(x) exists at a point x_0, then f is said to be differentiable at x_0. That also implies that f(x) is continuous at x_0.\nProperties of derivatives\nSuppose that f and g are differentiable at x and that \\alpha is a constant. Then the functions f\\pm g, \\alpha f, f g, and f/g (provided g(x)\\ne 0) are also differentiable at x. Additionally,\nConstant rule: \\left[k f(x)\\right]' = k f'(x)\nSum rule: \\left[f(x)\\pm g(x)\\right]' = f'(x)\\pm g'(x)\nWith a bit more algebra, we can apply the definition of derivatives to get a formula for of the derivative of a product and a derivative of a quotient.\nProduct rule: \\left[f(x)g(x)\\right]^\\prime = f^\\prime(x)g(x)+f(x)g^\\prime(x)\nQuotient rule: \\left[f(x)/g(x)\\right]^\\prime = \\frac{f^\\prime(x)g(x) - f(x)g^\\prime(x)}{[g(x)]^2}, ~g(x)\\neq 0\nFinally, one way to think of the power of derivatives is that it takes a function a notch down in complexity. The power rule applies to any higher-order function:\nPower rule: \\left[x^k\\right]^\\prime = k x^{k-1}\nFor any real number k (that is, both whole numbers and fractions). The power rule is proved by induction, a neat method of proof used in many fundamental applications to prove that a general statement holds for every possible case, even if there are countably infinite cases. We’ll show a simple case where k is an integer here.\n\nProof. We would like to prove that\n\\left[x^k\\right]^\\prime = k x^{k-1}\nfor any integer k.\nFirst, consider the first case (the base case) of k = 1. We can show by the definition of derivatives (setting f(x) = x^1 = 1) that\n[x^1]^\\prime = \\lim_{h \\rightarrow 0}\\frac{(x + h) - x}{(x + h) - x}= 1.\nBecause 1 is also expressed as 1 x^{1- 1}, the statement we want to prove holds for the case k =1.\nNow, that the statement holds for some integer m. That is, assume\n\\left[x^m\\right]^\\prime = m x^{m-1}\nThen, for the case m + 1, using the product rule above, we can simplify\n\\begin{align*}\n\\left[x^{m + 1}\\right]^\\prime &= [x^{m}\\cdot x]^\\prime\\\\\n&= (x^m)^\\prime\\cdot x + (x^m)\\cdot (x)^\\prime\\\\\n&= m x^{m - 1}\\cdot x + x^m ~~\\because \\text{by previous assumption}\\\\\n&= mx^m + x^m\\\\\n&= (m + 1)x^m\\\\\n&= (m + 1)x^{(m + 1) - 1}\n\\end{align*}\nTherefore, the rule holds for the case k = m + 1 once we have assumed it holds for k = m. Combined with the first case, this completes proof by induction – we have now proved that the statement holds for all integers k = 1, 2, 3, \\cdots.\nTo show that it holds for real fractions as well, we can prove expressing that exponent by a fraction of two integers.\n\n\n\nThese “rules” become apparent by applying the definition of the derivative above to each of the things to be “derived”, but these come up so frequently that it is best to repeat until it is muscle memory.\n\nExercise 10.1 For each of the following functions, find the first-order derivative f^\\prime(x).\n\nf(x)=c\nf(x)=x\nf(x)=x^2\nf(x)=x^3\nf(x)=\\frac{1}{x^2}\nf(x)=(x^3)(2x^4)\nf(x) = x^4 - x^3 + x^2 - x + 1\nf(x) = (x^2 + 1)(x^3 - 1)\nf(x) = 3x^2 + 2x^{1/3}\nf(x)=\\frac{x^2+1}{x^2-1}"
  },
  {
    "objectID": "32_derivatives.html#derivpoly",
    "href": "32_derivatives.html#derivpoly",
    "title": "10  Differential Calculus",
    "section": "\n10.2 Higher-Order Derivatives",
    "text": "10.2 Higher-Order Derivatives\n\n\nThe first derivative is applying the definition of derivatives on the function, and it can be expressed as\nf'(x),  ~~ y',  ~~ \\frac{d}{dx}f(x), ~~ \\frac{dy}{dx}\nWe can keep applying the differentiation process to functions that are themselves derivatives. The derivative of f'(x) with respect to x, would then be f''(x)=\\lim\\limits_{h\\to 0}\\frac{f'(x+h)-f'(x)}{h} and we can therefore call it the Second derivative:\nf''(x), ~~ y'', ~~ \\frac{d^2}{dx^2}f(x), ~~ \\frac{d^2y}{dx^2}\nSimilarly, the derivative of f''(x) would be called the third derivative and is denoted f'''(x). And by extension, the nth derivative is expressed as \\frac{d^n}{dx^n}f(x), \\frac{d^ny}{dx^n}.\n\nExample 10.1 \\begin{align*}\nf(x) &=x^3\\\\\nf^{\\prime}(x) &=3x^2\\\\\nf^{\\prime\\prime}(x) &=6x \\\\\nf^{\\prime\\prime\\prime}(x) &=6\\\\\nf^{\\prime\\prime\\prime\\prime}(x) &=0\\\\\n\\end{align*}\n\nEarlier, in Section 10.1, we said that if a function differentiable at a given point, then it must be continuous. Further, if f'(x) is itself continuous, then f(x) is called continuously differentiable. All of this matters because many of our findings about optimization rely on differentiation, and so we want our function to be differentiable in as many layers. A function that is continuously differentiable infinitly is called “smooth”. Some examples: f(x) = x^2, f(x) = e^x."
  },
  {
    "objectID": "32_derivatives.html#the-chain-rule",
    "href": "32_derivatives.html#the-chain-rule",
    "title": "10  Differential Calculus",
    "section": "\n10.3 The Chain Rule",
    "text": "10.3 The Chain Rule\n\n\nAs useful as the above rules are, many functions you’ll see won’t fit neatly in each case immediately. Instead, they will be functions of functions. For example, the difference between x^2 + 1^2 and (x^2 + 1)^2 may look trivial, but the sum rule can be easily applied to the former, while it’s actually not obvious what do with the latter.\nComposite functions are formed by substituting one function into another and are denoted by (f\\circ g)(x)=f[g(x)]. To form f[g(x)], the range of g must be contained (at least in part) within the domain of f. The domain of f\\circ g consists of all the points in the domain of g for which g(x) is in the domain of f.\n\nExample 10.2 Let f(x)=\\ln x for 0<x<\\infty and g(x)=x^2 for -\\infty<x<\\infty.\nThen\n(f\\circ g)(x)=\\ln x^2, -\\infty<x<\\infty - \\{0\\}\nAlso\n(g\\circ f)(x)=[\\ln x]^2, 0<x<\\infty\nNotice that f\\circ g and g\\circ f are not the same functions.\n\nWith the notation of composite functions in place, now we can introduce a helpful additional rule that will deal with a derivative of composite functions as a chain of concentric derivatives.\nChain Rule:\nLet y=(f\\circ g)(x)= f[g(x)]. The derivative of y with respect to x is \\frac{d}{dx} \\{ f[g(x)] \\} = f'[g(x)] g'(x)\nWe can read this as: “the derivative of the composite function y is the derivative of f evaluated at g(x), times the derivative of g.”\nThe chain rule can be thought of as the derivative of the “outside” times the derivative of the “inside”, remembering that the derivative of the outside function is evaluated at the value of the inside function.\n\nThe chain rule can also be written as \\frac{dy}{dx}=\\frac{dy}{dg(x)} \\frac{dg(x)}{dx} This expression does not imply that the dg(x)’s cancel out, as in fractions. They are part of the derivative notation and you can’t separate them out or cancel them.)\n\n\nExample 10.3 Find f^\\prime(x) for f(x) = (3x^2+5x-7)^6.\n\nThe direct use of a chain rule is when the exponent of is itself a function, so the power rule could not have applied generaly:\nGeneralized Power Rule:\nIf f(x)=[g(x)]^p for any rational number p, f^\\prime(x) =p[g(x)]^{p-1}g^\\prime(x)"
  },
  {
    "objectID": "32_derivatives.html#derivatives-of-logs-and-exponents",
    "href": "32_derivatives.html#derivatives-of-logs-and-exponents",
    "title": "10  Differential Calculus",
    "section": "\n10.4 Derivatives of logs and exponents",
    "text": "10.4 Derivatives of logs and exponents\n\n\nNatural logs and exponents (they are inverses of each other; see Prerequisites) crop up everywhere in statistics. Their derivative is a special case from the above, but quite elegant.\n\nTheorem 10.1 The functions e^x and the natural logarithm \\ln(x) are continuous and differentiable in their domains, and their first derivate is\n(e^x)^\\prime = e^x\n\\ln(x)^\\prime = \\frac{1}{x}\nAlso, when these are composite functions, it follows by the generalized power rule that\n\\left(e^{g(x)}\\right)^\\prime = e^{g(x)} \\cdot g^\\prime(x)\n\\left(\\ln g(x)\\right)^\\prime = \\frac{g^\\prime(x)}{g(x)}, ~~\\text{if}~~ g(x) > 0\n\nWe will relegate the proofs to small excerpts.\nDerivatives of exponents\nTo repeat the main rule in Theorem 10.1, the intuition is that\n\nDerivative of e^x is itself: \\frac{d}{dx}e^x = e^x (See Figure 10.2)\nSame thing if there were a constant in front: \\frac{d}{dx}\\alpha e^x = \\alpha e^x\n\nSame thing no matter how many derivatives there are in front: \\frac{d^n}{dx^n} \\alpha e^x = \\alpha e^x\n\nChain Rule: When the exponent is a function of x, remember to take derivative of that function and add to product. \\frac{d}{dx}e^{g(x)}= e^{g(x)} g^\\prime(x)\n\n\n\n\n\n\nFigure 10.2: Derivative of the Exponential Function\n\n\n\n\n\nExample 10.4 Find the derivative for the following.\n\nf(x)=e^{-3x}\nf(x)=e^{x^2}\nf(x)=(x-1)e^x\n\n\nDerivatives of logs\nThe natural log is the mirror image of the natural exponent and has mirroring properties, again, to repeat the theorem,\n\nlog prime x is one over x (Figure 10.3):\n\n\\frac{d}{dx} \\ln x = \\frac{1}{x}\n\nExponents become multiplicative constants:\n\n\\frac{d}{dx} \\ln x^k = \\frac{d}{dx} k \\ln x = \\frac{k}{x}\n\nChain rule again:\n\n\\frac{d}{dx} \\ln u(x) = \\frac{u'(x)}{u(x)}\\quad\n\nFor any positive base b,\n\n\\frac{d}{dx} b^x = (\\ln b)\\left(b^x\\right)\n\n\n\n\nFigure 10.3: Derivative of the Natural Log\n\n\n\n\n\nExample 10.5 Find dy/dx for the following.\n\nf(x)=\\ln(x^2+9)\nf(x)=\\ln(\\ln x)\nf(x)=(\\ln x)^2\nf(x)=\\ln e^x\n\n\nOutline of Proof\nWe actually show the derivative of the log first, and then the derivative of the exponential naturally follows.\nThe general derivative of the log at any base a is solvable by the definition of derivatives.\n\\begin{align*}\n(\\ln_a x)^\\prime = \\lim\\limits_{h\\to 0} \\frac{1}{h}\\ln_{a}\\left(1 + \\frac{h}{x}\\right)\n\\end{align*}\nRe-express g = \\frac{h}{x} and get \\begin{align*}\n(\\ln_a x)^\\prime &= \\frac{1}{x}\\lim_{g\\to 0}\\ln_{a} (1 + g)^{\\frac{1}{g}}\\\\\n&= \\frac{1}{x}\\ln_a e\n\\end{align*}\nBy definition of e. As a special case, when a = e, then (\\ln x)^\\prime = \\frac{1}{x}.\nNow let’s think about the inverse, taking the derivative of y = a^x.\n\\begin{align*}\ny &= a^x \\\\\n\\Rightarrow \\ln y &= x \\ln a\\\\\n\\Rightarrow \\frac{y^\\prime}{y} &= \\ln a\\\\\n\\Rightarrow  y^\\prime = y \\ln a\\\\\n\\end{align*}\nThen in the special case where a = e,\n(e^x)^\\prime = (e^x)"
  },
  {
    "objectID": "32_derivatives.html#partial-derivatives",
    "href": "32_derivatives.html#partial-derivatives",
    "title": "10  Differential Calculus",
    "section": "\n10.5 Partial Derivatives",
    "text": "10.5 Partial Derivatives\nWhat happens when there’s more than variable that is changing?\n\nIf you can do ordinary derivatives, you can do partial derivatives: just hold all the other input variables constant except for the one you’re differentiating with respect to. (Joe Blitzstein’s Math Notes)\n\nSuppose we have a function f now of two (or more) variables and we want to determine the rate of change relative to one of the variables. To do so, we would find its partial derivative, which is defined similar to the derivative of a function of one variable.\nPartial Derivative: Let f be a function of the variables (x_1,\\ldots,x_n). The partial derivative of f with respect to x_i is\n\\frac{\\partial f}{\\partial x_i} (x_1,\\ldots,x_n) = \\lim\\limits_{h\\to 0} \\frac{f(x_1,\\ldots,x_i+h,\\ldots,x_n)-f(x_1,\\ldots,x_i,\\ldots,x_n)}{h}\nOnly the ith variable changes — the others are treated as constants.\nWe can take higher-order partial derivatives, like we did with functions of a single variable, except now the higher-order partials can be with respect to multiple variables.\n\nExample 10.6 Notice that you can take partials with regard to different variables.\nSuppose f(x,y)=x^2+y^2. Then\n\\begin{align*}\n\\frac{\\partial f}{\\partial x}(x,y) &=\\\\\n\\frac{\\partial f}{\\partial y}(x,y) &=\\\\\n\\frac{\\partial^2 f}{\\partial x^2}(x,y) &=\\\\\n\\frac{\\partial^2 f}{\\partial x \\partial y}(x,y) &=\n\\end{align*}\n\n\nExercise 10.2 Let f(x,y)=x^3 y^4 +e^x -\\ln y. What are the following partial derivaitves?\n\\begin{align*}\n\\frac{\\partial f}{\\partial x}(x,y) &=\\\\\n\\frac{\\partial f}{\\partial y}(x,y) &=\\\\\n\\frac{\\partial^2 f}{\\partial x^2}(x,y) &=\\\\\n\\frac{\\partial^2 f}{\\partial x \\partial y}(x,y) &=\n\\end{align*}"
  },
  {
    "objectID": "32_derivatives.html#taylorapprox",
    "href": "32_derivatives.html#taylorapprox",
    "title": "10  Differential Calculus",
    "section": "\n10.6 Taylor Approximation",
    "text": "10.6 Taylor Approximation\n\n\nA common form of approximation used in statistics involves derivatives. A Taylor series is a way to represent common functions as infinite series (a sum of infinite elements) of the function’s derivatives at some point a.\nFor example, Taylor series are very helpful in representing nonlinear (read: difficult) functions as linear (read: manageable) functions. One can thus approximate functions by using lower-order, finite series known as Taylor polynomials. If a=0, the series is called a Maclaurin series.\nSpecifically, a Taylor series of a real or complex function f(x) that is infinitely differentiable in the neighborhood of point a is:\n\\begin{align*}\n    f(x) &= f(a) + \\frac{f'(a)}{1!} (x-a) +  \\frac{f''(a)}{2!} (x-a)^2 + \\cdots\\\\\n     &= \\sum_{n=0}^\\infty \\frac{f^{(n)} (a)}{n!} (x-a)^n\n\\end{align*}\nTaylor Approximation: We can often approximate the curvature of a function f(x) at point a using a 2nd order Taylor polynomial around point a:\nf(x) = f(a) + \\frac{f'(a)}{1!} (x-a) +  \\frac{f''(a)}{2!} (x-a)^2 + R_2\nR_2 is the remainder (R for remainder, 2 for the fact that we took two derivatives) and often treated as negligible, giving us:\nf(x) \\approx f(a) + f'(a)(x-a) +  \\dfrac{f''(a)}{2} (x-a)^2\nThe more derivatives that are added, the smaller the remainder R and the more accurate the approximation. Proofs involving limits guarantee that the remainder converges to 0 as the order of derivation increases."
  },
  {
    "objectID": "33_optimization.html",
    "href": "33_optimization.html",
    "title": "11  Optimization",
    "section": "",
    "text": "To optimize, we use derivatives and calculus. Optimization is to find the maximum or minimum of a functon, and to find what value of an input gives that extremum. This has obvious uses in engineering. Many tools in the statistical toolkit use optimization. One of the most common ways of estimating a model is through “Maximum Likelihood Estimation”, done via optimizing a function (the likelihood).\nOptimization also comes up in Economics, Formal Theory, and Political Economy all the time. A go-to model of human behavior is that they optimize a certain utility function. Humans are not pure utility maximizers, of course, but nuanced models of optimization – for example, adding constraints and adding uncertainty – will prove to be quite useful."
  },
  {
    "objectID": "33_optimization.html#maxima-and-minima",
    "href": "33_optimization.html#maxima-and-minima",
    "title": "11  Optimization",
    "section": "\n11.1 Maxima and Minima",
    "text": "11.1 Maxima and Minima\nThe first derivative, f'(x), quantifies the slope of a function. Therefore, it can be used to check whether the function f(x) at the point x is increasing or decreasing at x.\n\n\nIncreasing: f'(x)>0\n\n\nDecreasing: f'(x)<0\n\n\nNeither increasing nor decreasing: f'(x)=0 i.e. a maximum, minimum, or saddle point\n\nSo for example, f(x) = x^2 + 2 and f^\\prime(x) = 2x\n\n\n\n\nMaxima and Minima\n\n\n\n\n\nExercise 11.1 (Plotting a mazimum and minimum) Plot f(x)=x^3+ x^2 + 2, plot its derivative, and identifiy where the derivative is zero. Is there a maximum or minimum?\n\n\n\n\nThe second derivative f''(x) identifies whether the function f(x) at the point x is\n\nConcave / concave down: f''(x)<0\n\nConvex / Concave up: f''(x)>0\n\n\nMaximum (Minimum): x_0 is a local maximum (minimum) if f(x_0)>f(x) (f(x_0)<f(x)) for all x within some open interval containing x_0. x_0 is a global maximum (minimum) if f(x_0)>f(x) (f(x_0)<f(x)) for all x in the domain of f.\nGiven the function f defined over domain D, all of the following are defined as critical points:\n\nAny interior point of D where f'(x)=0.\nAny interior point of D where f'(x) does not exist.\nAny endpoint that is in D.\n\nThe maxima and minima will be a subset of the critical points.\nSecond Derivative Test of Maxima/Minima: We can use the second derivative to tell us whether a point is a maximum or minimum of f(x).\n\nLocal Maximum: f'(x)=0 and f''(x)<0\n\nLocal Minimum: f'(x)=0 and f''(x)>0\n\nNeed more info: f'(x)=0 and f''(x)=0\n\n\nGlobal Maxima and Minima Sometimes no global max or min exists — e.g., f(x) not bounded above or below. However, there are three situations where we can fairly easily identify global max or min.\n\n\nFunctions with only one critical point. If x_0 is a local max or min of f and it is the only critical point, then it is the global max or min.\n\nGlobally concave up or concave down functions. If f''(x) is never zero, then there is at most one critical point. That critical point is a global maximum if f''<0 and a global minimum if f''>0.\n\nFunctions over closed and bounded intervals must have both a global maximum and a global minimum.\n\n\nExample 11.1 (Maxima and Minima by drawing) Find any critical points and identify whether they are a max, min, or saddle point:\n\nf(x)=x^2+2\nf(x)=x^3+2\n\nf(x)=|x^2-1|, x\\in [-2,2]"
  },
  {
    "objectID": "33_optimization.html#concavity-of-a-function",
    "href": "33_optimization.html#concavity-of-a-function",
    "title": "11  Optimization",
    "section": "\n11.2 Concavity of a Function",
    "text": "11.2 Concavity of a Function\nConcavity helps identify the curvature of a function, f(x), in 2 dimensional space.\n\nDefinition 11.1 (Concave Function) A function f is strictly concave over the set S \\forall x_1,x_2 \\in S and \\forall a \\in (0,1), f(ax_1 + (1-a)x_2) > af(x_1) + (1-a)f(x_2) line connecting two points on a concave function will lie the function.\n\n\n\n\n\n\n\nDefinition 11.2 (Convex Function) Convex: A function f is strictly convex over the set S \\forall x_1,x_2 \\in S and \\forall a \\in (0,1), f(ax_1 + (1-a)x_2) < af(x_1) + (1-a)f(x_2)\nAny line connecting two points on a convex function will lie above the function.\n\nSecond Derivative Test of Concavity: The second derivative can be used to understand concavity.\nIf\n\\left\\{\\begin{array}{lll}\nf''(x) < 0 & \\Rightarrow & \\text{Concave}\\\\\nf''(x) > 0 & \\Rightarrow & \\text{Convex}\n\\end{array}\\right.\nQuadratic Forms\nQuadratic forms is shorthand for a way to summarize a function. This is important for finding concavity because\n\nApproximates local curvature around a point — e.g., used to identify max vs min vs saddle point.\nThey are simple to express even in n dimensions:\nHave a matrix representation.\n\nQuadratic Form: A polynomial where each term is a monomial of degree 2 in any number of variables:\n\\begin{align*}\n\\text{One variable: }& Q(x_1) = a_{11}x_1^2\\\\\n\\text{Two variables: }& Q(x_1,x_2) = a_{11}x_1^2 + a_{12}x_1x_2 + a_{22}x_2^2\\\\\n\\text{N variables: }& Q(x_1,\\cdots,x_n)=\\sum\\limits_{i\\le j} a_{ij}x_i x_j\n\\end{align*}\nwhich can be written in matrix terms:\nOne variable\nQ(\\mathbf{x}) = x_1^\\top a_{11} x_1\nN variables: \\begin{align*}\nQ(\\mathbf{x}) &=\\begin{bmatrix} x_1 & x_2 & \\cdots & x_n \\end{bmatrix}\\begin{bmatrix}\na_{11}&\\frac{1}{2}a_{12}&\\cdots&\\frac{1}{2}a_{1n}\\\\\n\\frac{1}{2}a_{12}&a_{22}&\\cdots&\\frac{1}{2}a_{2n}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\n\\frac{1}{2}a_{1n}&\\frac{1}{2}a_{2n}&\\cdots&a_{nn}\n\\end{bmatrix}\n\\begin{bmatrix} x_1\\\\x_2\\\\\\vdots\\\\x_n\\end{bmatrix}\\\\\n&= \\mathbf{x}^\\top\\mathbf{Ax}\n\\end{align*}\nFor example, the Quadratic on \\mathbb{R}^2: \\begin{align*}\n  Q(x_1,x_2)&=\\begin{bmatrix} x_1& x_2 \\end{bmatrix} \\begin{bmatrix} a_{11}&\\frac{1}{2} a_{12}\\\\\n  \\frac{1}{2}a_{12}&a_{22}\\end{bmatrix} \\begin{bmatrix} x_1\\\\x_2 \\end{bmatrix} \\\\\n  &= a_{11}x_1^2 + a_{12}x_1x_2 + a_{22}x_2^2\n\\end{align*}\nDefiniteness of Quadratic Forms\nWhen the function f(\\mathbf{x}) has more than two inputs, determining whether it has a maxima and minima (remember, functions may have many inputs but they have only one output) is a bit more tedious. Definiteness helps identify the curvature of a function, Q(\\textbf{x}), in n dimensional space.\nDefiniteness: By definition, a quadratic form always takes on the value of zero when x = 0, Q(\\textbf{x})=0 at \\textbf{x}=0. The definiteness of the matrix \\textbf{A} is determined by whether the quadratic form Q(\\textbf{x})=\\textbf{x}^\\top\\textbf{A}\\textbf{x} is greater than zero, less than zero, or sometimes both over all \\mathbf{x}\\ne 0."
  },
  {
    "objectID": "33_optimization.html#gradient-and-foc",
    "href": "33_optimization.html#gradient-and-foc",
    "title": "11  Optimization",
    "section": "\n11.3 Gradient and FOC",
    "text": "11.3 Gradient and FOC\nWe can see from a graphical representation that if a point is a local maxima or minima, it must meet certain conditions regarding its derivative. These are so commonly used that we refer these to “First Order Conditions” (FOCs) and “Second Order Conditions” (SOCs) in the economic tradition.\nWhen we examined functions of one variable x, we found critical points by taking the first derivative, setting it to zero, and solving for x. For functions of n variables, the critical points are found in much the same way, except now we set the partial derivatives equal to zero. Note: We will only consider critical points on the interior of a function’s domain.\nIn a derivative, we only took the derivative with respect to one variable at a time. When we take the derivative separately with respect to all variables in the elements of \\mathbf{x} and then express the result as a vector, we use the term Gradient and Hessian.\n\nDefinition 11.3 (Gradient) Given a function f(\\textbf{x}) in n variables, the gradient \\nabla f(\\mathbf{x}) (the greek letter nabla ) is a row vector, where the ith element is the partial derivative of f(\\textbf{x}) with respect to x_i:\n\\nabla f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f(\\mathbf{x})}{\\partial x_1} &  \\frac{\\partial f(\\mathbf{x})}{\\partial x_2} & \\cdots & \\frac{\\partial f(\\mathbf{x})}{\\partial x_n} \\end{bmatrix}\n\nThe gradient points in the direction of the steepest rate of increase at each point \\mathbf{x}.\nBefore we know whether a point is a maxima or minima, if it meets the FOC it is a “Critical Point”.\n\nDefinition 11.4 (Critical Point) \\mathbf{x}^* is a critical point if and only if \\nabla f(\\mathbf{x}^*)=\\mathbf{0} (the vector of zeros). If the partial derivative of f(x) with respect to x^* is 0, then \\mathbf{x}^* is a critical point. To solve for \\mathbf{x}^*, find the gradient, set each element equal to 0, and solve the system of equations. \\mathbf{x}^* = \\begin{bmatrix} x_1^*\\\\x_2^*\\\\ \\vdots \\\\ x_n^*\\end{bmatrix}\n\n\nExample 11.2 Example: Given a function f(\\mathbf{x})=(x_1-1)^2+x_2^2+1, find the (1) Gradient and (2) Critical point of f(\\mathbf{x}).\n\n\nSolution. Gradient\n\\begin{align*}\n\\nabla f(\\mathbf{x}) &=\n  \\begin{bmatrix}\n  \\frac{\\partial f(\\mathbf{x})}{\\partial x_1} &\n  \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}\n  \\end{bmatrix}\n  =\n  \\begin{bmatrix} 2(x_1-1) &\n  2x_2\n  \\end{bmatrix}\n\\end{align*}\nCritical Point \\mathbf{x}^*:\n\\begin{align*}\n&\\frac{\\partial f(\\mathbf{x})}{\\partial x_1} = 2(x_1-1) = 0 & \\Rightarrow x_1^* = 1\\\\\n&\\frac{\\partial f(\\mathbf{x})}{\\partial x_2} = 2x_2 = 0 & \\Rightarrow   x_2^* = 0\\\\\n\\end{align*}\nSo \\mathbf{x}^* = (1,0)"
  },
  {
    "objectID": "33_optimization.html#hessian-and-soc",
    "href": "33_optimization.html#hessian-and-soc",
    "title": "11  Optimization",
    "section": "\n11.4 Hessian and SOC",
    "text": "11.4 Hessian and SOC\nWhen we found a critical point for a function of one variable, we used the second derivative as a indicator of the curvature at the point in order to determine whether the point was a min, max, or saddle (second derivative test of concavity). For functions of n variables, we use second order partial derivatives as an indicator of curvature.\n\nDefinition 11.5 (Hessian) Given a function f(\\mathbf{x}) in n variables, the hessian \\mathbf{H(x)} is an n\\times n matrix, where the (i,j)th element is the second order partial derivative of f(\\mathbf{x}) with respect to x_i and x_j:\n\\mathbf{H(x)}=\\begin{bmatrix} \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_1^2}&\\frac{\\partial^2f(\\mathbf{x})}{\\partial x_1 \\partial x_2}& \\cdots & \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_1 \\partial x_n}\\\\ \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_2 \\partial x_1}&\\frac{\\partial^2f(\\mathbf{x})}{\\partial x_2^2}& \\cdots & \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_2 \\partial x_n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_n \\partial x_1}&\\frac{\\partial^2f(\\mathbf{x})}{\\partial x_n \\partial x_2}& \\cdots & \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_n^2} \\end{bmatrix}\n\nNote that the hessian will be a symmetric matrix because \\frac{\\partial f(\\mathbf{x})}{\\partial x_1\\partial x_2} = \\frac{\\partial f(\\mathbf{x})}{\\partial x_2\\partial x_1}.\nAlso note that given that f(\\mathbf{x}) is of quadratic form, each element of the hessian will be a constant.\nThese definitions will be employed when we determine the Second Order Conditions of a function:\nGiven a function f(\\mathbf{x}) and a point \\mathbf{x}^* such that \\nabla f(\\mathbf{x}^*)=0,\n\nHessian is Positive Definite around \\mathbf{x}^* \\quad \\Longrightarrow \\quad Local Min\nHessian is Negative Definite around \\mathbf{x}^* \\quad \\Longrightarrow \\quad Local Max\nHessian is Indefinite around \\mathbf{x}^* \\quad \\Longrightarrow \\quad Saddle Point\n\nFurthermore, there’s an easier way to check whether a 2\\times 2 matrix is positive or negative definite.\nDefiniteness of 2 \\times 2 Matrix: For a 2 \\times 2 matrix\n\\mathbf{A}=\\begin{bmatrix}A_{11} & A_{12}\\\\ A_{21} & A_{22}\\end{bmatrix}\n\nIf \\mathrm{det}(\\mathbf{A}) > 0 and A_{11}>0, then \\mathbf{A} is positive definite\nIf \\mathrm{det}(\\mathbf{A}) > 0 and A_{11}<0, then \\mathbf{A} is negative definite\nIf \\mathrm{det}(\\mathbf{A}) < 0, then \\mathbf{A} is indefinite\n\n\nExample 11.3 (Max and min with two dimensions) We found that the only critical point of f(\\mathbf{x})=(x_1-1)^2+x_2^2+1 is at \\mathbf{x}^*=(1,0). Is it a min, max, or saddle point?\n\n\nSolution. The Hessian is \\begin{align*}\n\\mathbf{H(x)} &= \\begin{bmatrix} 2&0\\\\0&2 \\end{bmatrix}\n\\end{align*} Since \\mathrm{det}(\\mathbf{H}(x)) = 4 > 0 and H_{11}=2>0, the Hessian is positive definite.\nMaxima, Minima, or Saddle Point? Since the Hessian is positive definite and the gradient equals 0, x^\\star = (1,0) is a local minimum.\nNote: Alternate check of definiteness is to check the sign of the quardratic form:\n\\mathbf{x}^\\top \\mathbf{H}(\\mathbf{x}^*) \\mathbf{x} = \\begin{bmatrix} x_1 & x_2 \\end{bmatrix} \\begin{bmatrix} 2&0\\\\0&2 \\end{bmatrix} \\begin{bmatrix} x_1\\\\x_2\\end{bmatrix} = 2x_1^2+2x_2^2\nFor any \\mathbf{x}\\ne 0, 2(x_1^2+x_2^2)>0, so the Hessian is positive definite and \\mathbf{x}^* is a strict local minimum.\n\nDefiniteness and Concavity\nAlthough definiteness helps us to understand the curvature of an n-dimensional function, it does not necessarily tell us whether the function is globally concave or convex.\nWe need to know whether a function is globally concave or convex to determine whether a critical point is a global min or max. We can use the definiteness of the Hessian to determine whether a function is globally concave or convex:\n\nHessian is Positive Semidefinite \\forall \\mathbf{x} \\quad \\Longrightarrow \\quad Globally Convex\nHessian is Negative Semidefinite \\forall \\mathbf{x} \\quad \\Longrightarrow \\quad Globally Concave\n\nNotice that the definiteness conditions must be satisfied over the entire domain."
  },
  {
    "objectID": "33_optimization.html#global-maxima-and-minima",
    "href": "33_optimization.html#global-maxima-and-minima",
    "title": "11  Optimization",
    "section": "\n11.5 Global Maxima and Minima",
    "text": "11.5 Global Maxima and Minima\nGlobal Max/Min Conditions: Given a function f(\\mathbf{x}) and a point \\mathbf{x}^* such that \\nabla f(\\mathbf{x}^*)=0,\n\n\nf(\\mathbf{x}) Globally Convex \\quad \\Longrightarrow \\quad Global Min\n\nf(\\mathbf{x}) Globally Concave \\quad \\Longrightarrow \\quad Global Max\n\nNote that showing that \\mathbf{H(x^*)} is negative semidefinite is not enough to guarantee \\mathbf{x}^* is a local max. However, showing that \\mathbf{H(x)} is negative semidefinite for all \\mathbf{x} guarantees that x^* is a global max. (The same goes for positive semidefinite and minima.)\n\nExample 11.4 Take f_1(x)=x^4 and f_2(x)=-x^4.\n\nBoth have x=0 as a critical point.\n\nUnfortunately, f''_1(0)=0 and f''_2(0)=0, so we can’t tell whether x=0 is a min or max for either. However, f''_1(x)=12x^2 and f''_2(x)=-12x^2.\n\nFor all x, f''_1(x)\\ge 0 and f''_2(x)\\le 0 — i.e., f_1(x) is globally convex and f_2(x) is globally concave.\nSo x=0 is a global min of f_1(x) and a global max of f_2(x).\n\n\n\nExercise 11.2 Given f(\\mathbf{x})=x_1^3-x_2^3+9x_1x_2, find any maxima or minima.\n\nSolution.\n\nFirst order conditions\n\nGradient \\nabla f(\\mathbf{x}) \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\\n\\frac{\\partial f}{\\partial x_2}\\end{bmatrix} =\n\\begin{bmatrix} 3x_1^2+9x_2 \\\\ -3x_2^2+9x_1 \\end{bmatrix}\n\nCritical Points \\mathbf{x^*}\n\nSet the gradient equal to zero and solve for x_1 and x_2. We have two equations and two unknowns. Solving for x_1 and x_2, we get two critical points: \\mathbf{x}_1^*=(0,0) and \\mathbf{x}_2^*=(3,-3).\n3x_1^2 + 9x_2 = 0 \\quad \\Rightarrow \\quad 9x_2 = -3x_1^2 \\quad \\Rightarrow \\quad x_2 = -\\frac{1}{3}x_1^2\n-3x_2^2 + 9x_1 = 0 \\quad \\Rightarrow \\quad -3(-\\frac{1} {3}x_1^2)^2 + 9x_1 = 0\n\\Rightarrow \\quad -\\frac{1}{3}x_1^4 + 9x_1 = 0 \\quad \\Rightarrow \\quad x_1^3 = 27x_1 \\quad \\Rightarrow \\quad x_1 = 3\n3(3)^2 + 9x_2 = 0 \\quad \\Rightarrow \\quad x_2 = -3\n\n\n\n\nSecond order conditions.\n\n\nHessian \\mathbf{H(x)}\n\\begin{bmatrix} 6x_1&9\\\\9&-6x_2 \\end{bmatrix}\n\n\nHessian \\mathbf{H(x_1^*)}\n\\begin{bmatrix} 0&9\\\\9&0\\end{bmatrix}\n\n\nLeading principal minors of \\mathbf{H(x_1^*)}\nM_1=0; M_2=-81\n\n\nDefiniteness of \\mathbf{H(x_1^*)}?\n\n\n\\mathbf{H(x_1^*)} is indefinite\n\n\n\nMaxima, Minima, or Saddle Point for \\mathbf{x_1^*}?\n\nSince \\mathbf{H(x_1^*)} is indefinite, \\mathbf{x}_1^*=(0,0) is a saddle point.\n\n\n\nHessian\n\\mathbf{H(x_2^*)}=\\begin{bmatrix} 18&9\\\\9&18\\end{bmatrix}\n\n\nLeading principal minors of \\mathbf{H(x_2^*)}\nM_1=18; M_2=243\n\n\nDefiniteness of \\mathbf{H(x_2^*)}?\n\n\n\\mathbf{H(x_2^*)} is positive definite\n\n\n\nMaxima, Minima, or Saddle Point for \\mathbf{x}_2^*?\n\nSince \\mathbf{H(x_2^*)} is positive definite, \\mathbf{x}_1^*=(3,-3) is a strict local minimum\n\n\n\n\nGlobal concavity/convexity.\n\nIs f(x) globally concave/convex?\n\nNo. In evaluating the Hessians for \\mathbf{x}_1^* and \\mathbf{x}_2^* we saw that the Hessian is not positive semidefinite at x = (0,0).\n\n\nAre any \\mathbf{x^*} global minima or maxima?\n\nNo. Since the function is not globally concave/convex, we can’t infer that \\mathbf{x}_2^*=(3,-3) is a global minimum. In fact, if we set x_1=0, the f(\\mathbf{x})=-x_2^3, which will go to -\\infty as x_2\\to \\infty."
  },
  {
    "objectID": "34_intergrals.html#the-indefinite-integral",
    "href": "34_intergrals.html#the-indefinite-integral",
    "title": "12  Integral Calculus",
    "section": "\n12.1 The Indefinite Integral",
    "text": "12.1 The Indefinite Integral\nSo far, we’ve been interested in finding the derivative f=F' of a function F. However, sometimes we’re interested in exactly the reverse: finding the function F for which f is its derivative. We refer to F as the antiderivative of f.\n\nDefinition 12.1 (Antiderivative) The antiverivative of a function f(x) is a differentiable function F whose derivative is f.\nF^\\prime = f.\n\nAnother way to describe is through the inverse formula. Let DF be the derivative of F. And let DF(x) be the derivative of F evaluated at x. Then the antiderivative is denoted by D^{-1} (i.e., the inverse derivative). If DF=f, then F=D^{-1}f.\nThis definition bolsters the main takeaway about integrals and derivatives: They are inverses of each other.\n\nExercise 12.1 (Antiderivative) Find the antiderivative of the following:\n\nf(x) = \\frac{1}{x^2}\nf(x) = 3e^{3x}\n\n\nWe know from derivatives how to manipulate F to get f. But how do you express the procedure to manipulate f to get F? For that, we need a new symbol, which we will call indefinite integration.\n\nDefinition 12.2 (Indefinite Integral) The indefinite integral of f(x) is written\n\\int f(x) dx \nand is equal to the antiderivative of f.\n\n\nExample 12.1 Draw the function f(x) and its indefinite integral, \\int\\limits f(x) dx\nf(x) = (x^2-4)\n\n\nSolution. The Indefinite Integral of the function f(x) = (x^2-4) can, for example, be F(x) = \\frac{1}{3}x^3 - 4x. But it can also be F(x) = \\frac{1}{3}x^3 - 4x + 1, because the constant 1 disappears when taking the derivative.\n\nSome of these functions are plotted in the bottom panel of Figure 12.1 as dotted lines.\n\n\n\n\nFigure 12.1: The Many Indefinite Integrals of a Function\n\n\n\n\nNotice from these examples that while there is only a single derivative for any function, there are multiple antiderivatives: one for any arbitrary constant c. c just shifts the curve up or down on the y-axis. If more information is present about the antiderivative — e.g., that it passes through a particular point — then we can solve for a specific value of c.\nProperties of Integration\nSome useful properties of integrals follow by virtue of being the inverse of a derivative.\n\n\n12.1.1 Properties of Integration\n\nConstants are allowed to slip out: \\int a f(x)dx = a\\int f(x)dx\n\nIntegration of the sum is sum of integrations: \\int [f(x)+g(x)]dx=\\int f(x)dx + \\int g(x)dx\n\nReverse Power-rule: \\int x^n dx = \\frac{1}{n+1} x^{n+1} + c\n\nExponents are still exponents: \\int e^x dx = e^x +c\n\nRecall the derivative of \\ln(x) is one over x, and so: \\int \\frac{1}{x} dx = \\ln x + c\n\nReverse chain-rule: \\int e^{f(x)}f^\\prime(x)dx = e^{f(x)}+c\n\nMore generally: \\int [f(x)]^n f'(x)dx = \\frac{1}{n+1}[f(x)]^{n+1}+c\n\nRemember the derivative of a log of a function: \\int \\frac{f^\\prime(x)}{f(x)}dx=\\ln f(x) + c\n\n\n\n\nExample 12.2 (Common Integration) Simplify the following indefinite integrals:\n\n\\int 3x^2 dx\n\\int (2x+1)dx\n\\int e^x e^{e^x} dx"
  },
  {
    "objectID": "34_intergrals.html#the-definite-integral",
    "href": "34_intergrals.html#the-definite-integral",
    "title": "12  Integral Calculus",
    "section": "\n12.2 The Definite Integral",
    "text": "12.2 The Definite Integral\nIf there is a indefinite integral, there must be a definite integral. Indeed there is, but the notion of definite integrals comes from a different objective: finding the are a under a function. We will find, perhaps remarkably, that the formula we find to get the sum turns out to be expressible by the anti-derivative.\nSuppose we want to determine the area A(R) of a region R defined by a curve f(x) and some interval a\\le x \\le b.\n\n\n\n\nFigure 12.2: The Riemann Integral as a Sum of Evaluations\n\n\n\n\nOne way to calculate the area would be to divide the interval a\\le x\\le b into n subintervals of length \\Delta x and then approximate the region with a series of rectangles, where the base of each rectangle is \\Delta x and the height is f(x) at the midpoint of that interval. A(R) would then be approximated by the area of the union of the rectangles, which is given by S(f,\\Delta x)=\\sum\\limits_{i=1}^n f(x_i)\\Delta x and is called a Riemann sum.\nAs we decrease the size of the subintervals \\Delta x, making the rectangles “thinner,” we would expect our approximation of the area of the region to become closer to the true area. This allows us to express the area as a limit of a series:\nA(R)=\\lim\\limits_{\\Delta x\\to 0}\\sum\\limits_{i=1}^n f(x_i)\\Delta x\nFigure 12.2 shows that illustration. The curve depicted is f(x) = -15(x - 5) + (x - 5)^3 + 50. We want approximate the area under the curve between the x values of 0 and 10. We can do this in blocks of arbitrary width, where the sum of rectangles (the area of which is width times f(x) evaluated at the midpoint of the bar) shows the Riemann Sum. As the width of the bars \\Delta x becomes smaller, the better the estimate of A(R).\nThis is how we define the “Definite” Integral:\n\nDefinition 12.3 (The Definite Integral (Riemann)) If for a given function f the Riemann sum approaches a limit as \\Delta x \\to 0, then that limit is called the Riemann integral of f from a to b. We express this with the \\int, symbol, and write \\int\\limits_a^b f(x) dx= \\lim\\limits_{\\Delta x\\to 0} \\sum\\limits_{i=1}^n f(x_i)\\Delta x\nThe most straightforward of a definite integral is the definite integral. That is, we read\n\\int\\limits_a^b f(x) dx as the definite integral of f from a to b and we defined as the area under the “curve” f(x) from point x=a to x=b.\n\nThe fundamental theorem of calculus shows us that this sum is, in fact, the antiderivative.\n\nTheorem 12.1 (First Fundamental Theorem of Calculus) Let the function f be bounded on [a,b] and continuous on (a,b). Then, suggestively, use the symbol F(x) to denote the definite integral from a to x\nF(x)=\\int\\limits_a^x f(t)dt, \\quad a\\le x\\le b\nThen F(x) has a derivative at each point in (a,b) and F^\\prime(x)=f(x), \\quad a<x<b That is, the definite integral function of f is the one of the antiderivatives of some f.\n\nThis is again a long way of saying that that differentiation is the inverse of integration. But now, we’ve covered definite integrals.\nThe second theorem gives us a simple way of computing a definite integral as a function of indefinite integrals.\n\n\n12.2.1 Second Fundamental Theorem of Calculus\nLet the function f be bounded on [a,b] and continuous on (a,b). Let F be any function that is continuous on [a,b] such that F^\\prime(x)=f(x) on (a,b). Then \\int\\limits_a^bf(x)dx = F(b)-F(a)\n\n\nSo the procedure to calculate a simple definite integral \\int\\limits_a^b f(x)dx is then\n\nFind the indefinite integral F(x).\nEvaluate F(b)-F(a).\n\n\nExample 12.3 (Definite Integral of a monomial) Solve \\int\\limits_1^3 3x^2 dx. Let f(x) = 3x^2.\n\n\nExercise 12.2 What is the value of \\int\\limits_{-2}^2 e^x e^{e^x} dx?\n\nProperties for Definite Integrals\nThe area-interpretation of the definite integral provides some useful properties for definite integrals\n\n\nThere is no area below a point: \\int\\limits_a^a f(x)dx=0\n\nReversing the limits changes the sign of the integral: \\int\\limits_a^b f(x)dx=-\\int\\limits_b^a f(x)dx\n\nSums can be separated into their own integrals: \\int\\limits_a^b [\\alpha f(x)+\\beta g(x)]dx = \\alpha \\int\\limits_a^b f(x)dx + \\beta \\int\\limits_a^b g(x)dx\n\nAreas can be combined as long as limits are linked: \\int\\limits_a^b f(x) dx +\\int\\limits_b^c f(x)dx = \\int\\limits_a^c f(x)dx\n\n\n\n\nExercise 12.3 Simplify the following definite intergrals.\n\n\\int\\limits_1^1 3x^2 dx =\n\\int\\limits_0^4 (2x+1)dx=\n\\int\\limits_{-2}^0 e^x e^{e^x} dx + \\int\\limits_0^2 e^x e^{e^x} dx ="
  },
  {
    "objectID": "34_intergrals.html#integration-by-substitution",
    "href": "34_intergrals.html#integration-by-substitution",
    "title": "12  Integral Calculus",
    "section": "\n12.3 Integration by Substitution",
    "text": "12.3 Integration by Substitution\nFrom the second fundamental theorem of calculus, we now that a quick way to get a definite integral is to first find the indefinite integral, and then just plug in the bounds.\nSometimes the integrand (the thing that we are trying to take an integral of) doesn’t appear integrable using common rules and antiderivatives. A method one might try is integration by substitution, which is related to the Chain Rule.\nSuppose we want to find the indefinite integral \\int g(x)dx but g(x) is complex and none of the formulas we have seen so far seem to apply immediately. The trick is to come up with a new function u(x) such that g(x)=f[u(x)]u'(x).\nWhy does an introduction of yet another function end of simplifying things? Let’s refer to the antiderivative of f as F. Then the chain rule tells us that \\frac{d}{dx} F[u(x)]=f[u(x)]u'(x). So, F[u(x)] is the antiderivative of g. We can then write \\int g(x) dx= \\int f[u(x)]u'(x)dx = \\int \\frac{d}{dx} F[u(x)]dx = F[u(x)]+c\nTo summarize, the procedure to determine the indefinite integral \\int g(x)dx by the method of substitution:\n\nIdentify some part of g(x) that might be simplified by substituting in a single variable u (which will then be a function of x).\nDetermine if g(x)dx can be reformulated in terms of u and du.\nSolve the indefinite integral.\nSubstitute back in for x\n\n\nSubstitution can also be used to calculate a definite integral. Using the same procedure as above, \\int\\limits_a^b g(x)dx=\\int\\limits_c^d f(u)du = F(d)-F(c) where c=u(a) and d=u(b).\n\nExample 12.4 (Integration by Substitution I) Solve the indefinite integral \\int x^2 \\sqrt{x+1}dx.\n\nFor the above problem, we could have also used the substitution u=\\sqrt{x+1}. Then x=u^2-1 and dx=2u du. Substituting these in, we get \\int x^2\\sqrt{x+1}dx=\\int (u^2-1)^2 u 2u du which when expanded is again a polynomial and gives the same result as above.\nAnother case in which integration by substitution is is useful is with a fraction.\n\nExample 12.5 (Integration by Substitutiton II) Simplify \\int\\limits_0^1 \\frac{5e^{2x}}{(1+e^{2x})^{1/3}}dx."
  },
  {
    "objectID": "34_intergrals.html#integration-by-parts",
    "href": "34_intergrals.html#integration-by-parts",
    "title": "12  Integral Calculus",
    "section": "\n12.4 Integration by Parts",
    "text": "12.4 Integration by Parts\nAnother useful integration technique is integration by parts, which is related to the Product Rule of differentiation. The product rule states that \\frac{d}{dx}(uv)=u\\frac{dv}{dx}+v\\frac{du}{dx} Integrating this and rearranging, we get \\int u\\frac{dv}{dx}dx= u v - \\int v \\frac{du}{dx}dx or \\int u(x) v'(x)dx=u(x)v(x) - \\int v(x)u'(x)dx\nMore easily remembered with the mnemonic “Ultraviolet Voodoo”: \\int u dv = u v - \\int v du where du=u'(x)dx and dv=v'(x)dx.\nFor definite integrals, this is simply\n\\int\\limits_a^b u\\frac{dv}{dx}dx = \\left. u v \\right|_a^b - \\int\\limits_a^b v \\frac{du}{dx}dx\nOur goal here is to find expressions for u and dv that, when substituted into the above equation, yield an expression that’s more easily evaluated.\n\nExample 12.6 (Integration by Parts I) Simplify the following integrals. These seemingly obscure forms of integrals come up often when integrating distributions.\n\\int x e^{ax} dx\n\n\nSolution. Let u=x and \\frac{dv}{dx} = e^{ax}. Then du=dx and v=(1/a)e^{ax}. Substituting this into the integration by parts formula, we obtain\\begin{align*}\n\\int x e^{ax} dx &= u v - \\int v du\\nonumber\\\\\n                &=x\\left( \\frac{1}{a}e^{ax}\\right) -\\int\\frac{1}{a}e^{ax}dx\\nonumber\\\\\n                &=\\frac{1}{a}xe^{ax}-\\frac{1}{a^2}e^{ax}+c\\nonumber\n\\end{align*}\n\n\n\nExercise 12.4 (Integration by Parts II) \nIntegrate\n\n\\int x^n e^{ax} dx\n\nIntegrate\n\n\\int x^3 e^{-x^2} dx"
  },
  {
    "objectID": "34_intergrals.html#answers-to-examples-and-exercises",
    "href": "34_intergrals.html#answers-to-examples-and-exercises",
    "title": "12  Integral Calculus",
    "section": "Answers to Examples and Exercises",
    "text": "Answers to Examples and Exercises\nExercise 10.1\nSolution.\n\nf^\\prime(x)= 0\nf^\\prime(x)= 1\nf^\\prime(x)= 2x^3\nf\\prime(x)= 3x^2\nf\\prime(x)= -2x^{-3}\nf\\prime(x)= 14x^6\nf\\prime(x) = 4x^3 - 3x^2 + 2x -1\nf\\prime(x) = 5x^4 + 3x^2 - 2x\nf\\prime(x) = 6x + \\frac{2}{3}x^{\\frac{-2}{3}}\nf\\prime(x)= \\frac{-4x}{x^4 - 2x^2 + 1}\n\nExample 10.3\n\nSolution. For convenience, define f(z) = z^6 and z = g(x) = 3x^2+5x-7. Then, y=f[g(x)] and\n\\begin{align*}\n\\frac{d}{dx}y&= f^\\prime(z) g^\\prime(x) \\\\\n&= 6(3x^2+5x-7)^5 (6x + 5)\n\\end{align*}\n\nExample 10.4\nSolution.\n\nLet u(x)=-3x. Then u^\\prime(x)=-3 and f^\\prime(x)=-3e^{-3x}.\nLet u(x)=x^2. Then u^\\prime(x)=2x and f^\\prime(x)=2xe^{x^2}.\n\nExample 10.5\nSolution.\n\nLet u(x)=x^2+9. Then u^\\prime(x)=2x and \\frac{dy}{dx}= \\frac{u^\\prime(x)}{u(x)} = \\frac{2x}{(x^2+9)}\n\nLet u(x)=\\ln x. Then u^\\prime(x)=1/x and \\frac{dy}{dx} = \\frac{1}{(x\\ln x)}.\nUse the generalized power rule. \\frac{dy}{dx} = \\frac{(2 \\ln x)}{x}\n\nWe know that \\ln e^x=x and that dx/dx=1, but we can double check. Let u(x)=e^x. Then u^\\prime(x)=e^x and \\frac{dy}{dx} = \\frac{u^\\prime(x)}{u(x)} = \\frac{e^x}{e^x} = 1.\n\n\nExample 12.3\n\nSolution. What is F(x)? From the power rule, recognize \\frac{d}{dx}x^3 = 3x^2 so\n\\begin{align*}\nF(x) &= x^3\\\\\n\\int\\limits_1^3 f(x) dx &= F(x = 3) - F(x  - 1)\\\\\n&= 3^3 - 1^3\\\\\n&=26\n\\end{align*}\n\nExample 12.4\n\nSolution. The problem here is the \\sqrt{x+1} term. However, if the integrand had \\sqrt{x} times some polynomial, then we’d be in business. Let’s try u=x+1. Then x=u-1 and dx=du. Substituting these into the above equation, we get\n\\begin{align*}\n            \\int x^2\\sqrt{x+1}dx&= \\int (u-1)^2\\sqrt{u}du\\\\\n            &= \\int (u^2-2u+1)u^{1/2}du\\\\\n            &= \\int (u^{5/2}-2u^{3/2}+u^{1/2})du\n\\end{align*}\nWe can easily integrate this, since it is just a polynomial. Doing so and substituting u=x+1 back in, we get \\int x^2\\sqrt{x+1}dx=2(x+1)^{3/2}\\left[\\frac{1}{7}(x+1)^2 -\n\\frac{2}{5}(x+1)+\\frac{1}{3}\\right]+c\n\nExample 12.5\n\nSolution. When an expression is raised to a power, it is often helpful to use this expression as the basis for a substitution. So, let u=1+e^{2x}. Then du=2e^{2x}dx and we can set 5e^{2x}dx=5du/2. Additionally, u=2 when x=0 and u=1+e^2 when x=1. Substituting all of this in, we get\n\\begin{align*}\n\\int\\limits_0^1 \\frac{5e^{2x}}{(1+e^{2x})^{1/3}}dx\n            &= \\frac{5}{2}\\int\\limits_2^{1+e^2}\\frac{du}{u^{1/3}}\\\\\n            &= \\frac{5}{2}\\int\\limits_2^{1+e^2} u^{-1/3}du\\\\\n            &= \\left. \\frac{15}{4} u^{2/3} \\right|_2^{1+e^2}\\\\\n            &= 9.53\n\\end{align*}\n\nExercise 12.4\nSolution.\n\n\\int x^n e^{ax} dx\n\nAs in the first problem, let\nu=x^n, dv=e^{ax}dx\nThen du=n x^{n-1}dx and v=(1/a)e^{ax}.\nSubstituting these into the integration by parts formula gives \\begin{align*}\n    \\int x^n e^{ax} dx &= u v - \\int v du\\nonumber\\\\\n    &=x^n\\left( \\frac{1}{a}e^{ax}\\right) - \\int\\frac{1}{a}e^{ax} n x^{n-1} dx\\nonumber\\\\\n    &=\\frac{1}{a}x^n e^{ax} - \\frac{n}{a}\\int x^{n-1}e^{ax}dx\\nonumber\n\\end{align*}\nNotice that we now have an integral similar to the previous one, but with x^{n-1} instead of x^n.\nFor a given n, we would repeat the integration by parts procedure until the integrand was directly integratable — e.g., when the integral became \\int e^{ax}dx.\n\n\\int x^3 e^{-x^2} dx\n\nWe could, as before, choose u=x^3 and dv=e^{-x^2}dx. But we can’t then find v — i.e., integrating e^{-x^2}dx isn’t possible. Instead, notice that \\frac{d}{dx}e^{-x^2} = -2xe^{-x^2}, which can be factored out of the original integrand \\int x^3 e^{-x^2} dx = \\int x^2 (xe^{-x^2})dx.\nWe can then let u=x^2 and dv=x e^{-x^2}dx. Thedu=2x dx and v=-\\frac{1}{2}e^{-x^2}. Substituting these in, we have \\begin{align*}\n    \\int x^3 e^{-x^2} dx &= u v - \\int v du\\nonumber\\\\\n    &= x^2 \\left( -\\frac{1}{2}e^{-x^2}\\right) -\\int \\left(-\\frac{1}{2}e^{-x^2}\\right)2x dx\\nonumber\\\\\n    &= -\\frac{1}{2}x^2 e^{-x^2}+\\int x e^{-x^2}dx\\nonumber\\\\\n    &= -\\frac{1}{2}x^2 e^{-x^2}-\\frac{1}{2}e^{-x^2}+c\\nonumber\n\\end{align*}"
  },
  {
    "objectID": "41_measuring_uncertainty.html",
    "href": "41_measuring_uncertainty.html",
    "title": "13  Measuring Uncertainty",
    "section": "",
    "text": "Social science research is hard—in part because human behavior is so unpredictable. Unlike fields such as physics and chemistry, our theories are rarely deterministic. Rather, we express our arguments in terms of probabilities or likelihoods. “Democratic countries tend to have lower levels of corruption”. Or “the belief that their vote matters increases the likelihood that an individual will turn out on election day”. Theories like these highlight why statistics and probability theory are such useful tools for social scientists. The goal of statistics is to use data to measure our uncertainty about a proposition, or hypothesis. And probability is the language we use to describe this uncertainty.\nIn this section of the book we will cover the theory of probability that underlies commonly used statistical methods you will encounter in subsequent courses.\nThe material for this section is drawn from the following sources:"
  },
  {
    "objectID": "41_measuring_uncertainty.html#what-is-probability",
    "href": "41_measuring_uncertainty.html#what-is-probability",
    "title": "13  Measuring Uncertainty",
    "section": "\n13.1 What is Probability?",
    "text": "13.1 What is Probability?\nWhat does the phrase “the probability of a coin flip coming up heads is one half” mean? You may be surprised to learn that there are many different answers to this question. In this section we will discuss the various definitions of probability philosophers and statisticians have come up with. As we will see, many commonly used definitions have appealing qualities. But each ultimately falls short in important ways. The exception being viewing probability as an extension of logic. This idea, developed by Edwin T. Jaynes in his book Probability Theory: the Logic of Science (2003), is rarely found in introductions to probability, but as we will see, is the most coherent way to understand and reason about probabilistic statements. Before diving into probability-as-logic, let’s take a look at the strengths and weaknesses of definitions with which you may be more familiar.\nThe Classical Definition\nThe mathematics of probability originated in the study of gambling games. People wanted to know the answer to questions like “if I roll two six-sided dice, what is the probability that their values sum to seven”? This would have implications for how much to wager on a given bet. In the 15th and 16th centuries, mathematicians came up with the following solution.\n\n\\text{P(event A)} = \\frac{\\text{\\# of ways event A can occur}}{\\text{\\# of all possible outcomes}}\n\nThis classical definition of probability is still used in many introductory textbooks today—probably because it can be intuitively applied to most common examples used in teaching (e.g. flipping coins, rolling dice, dealing cards from a deck, or drawing balls from an urn). What is the probability that a coin will come up heads when we flip it? Well, there are two possible outcomes when we flip a coin: either it lands heads or tails, and there is one way for it to land heads. So our calculation would be:\n\n\\frac{\\text{heads}}{\\text{[heads, tails]}} = \\frac{1}{1 + 1} = \\frac{1}{2}\n\nThe classical definition also makes it clear to us that events which can happen in more ways are more likely. When we roll two six-sided dice, there are 6*6 = 36 possible outcomes. Getting a sum equal to seven can happen in five ways [(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)] whereas getting a sum equal to two can only happen in one way [(1, 1)]. Therefore, according to the classical definition of probability, rolling two dice and getting a sum of seven is five times more likely than getting a sum of two.\nThe problem with the classical definition of probability is that is does a poor job describing uncertainty outside of simple games of chance. One of the main challenges comes from having to enumerate the “number of all possible outcomes” in the denominator. We can figure this out easily enough for flipping coins and rolling dice because there is an apparent symmetry in the outcome-space, but in the social sciences we rarely study such clear-cut systems. Take the statement “after receiving the encouragement treatment, an individual will turn out to vote with 70% probability.” There are only two outcomes: either the individual votes or they do not vote. So should the probability of voting always be 50/50? There is no apparent way to cut up the outcome space such that the classical probability formula gives us an answer like 7/10.\nAnother problem with the classical definition is that it requires each event in the denominator to be equi-probable. This is easy to assume when the events have a symmetric quality to them: flipping a coin, drawing cards from a deck, rolling dice etc. But what if we wanted to know the probability of rolling a specific value from a mis-shapen die? Furthmore, the fact that the denominator term requires events to be “equi-probable” means that this definition of probability is circular!\nThe Frequentist Definition\nBeginning in the 19th century, a new definition of probability known as frequentism began to be widely used. According to frequentism, the probability of an event is defined by its long-run numerical frequency. We know that the probability of a coin toss coming up heads is 1/2 because, if we were to toss a coin enough times, in the limit 50% of the tosses would come up heads. The mathematical basis for frequentism comes from Swiss mathematician Jacob Bernoulli’s Law of Large Numbers.\n\n“Take a large enough sample that you can be morally certain, to whatever degree that means to you, the ratio in the sample is within your desired tolerance of the [true ratio].”1\n\nFrequentism has an appealing objective quality to it because probabilities can supposedly be arrived at via empirical frequencies. But how can we ever truly verify that these frequencies match reality? In social science research we only get one shot of our data if we run a survey or a field experiment. And what about data from one-time events such as elections or wars? “The probability that the Democrats retain control of Congress in 2022 is 0.4.” According to frequentists, we would have to imagine re-running the 2022 mid-term election an infinite number of times in order to make sense of this statement. But if we re-ran the election under all the same circumstances, wouldn’t the outcome remain the same? Like the classical definition of probability, frequentism “works” for simple examples but leads to paradoxes when applied to modern scientific problems. Despite this, most statistical methods used today are based on the frequentist paradigm.\n\nThe Subjective Definition\nStanding in radical opposition to the frequentist and classical definitions is a view that probability statements are no more than someone’s subjective degree of belief about a proposition. The nice thing about the subjective definition is that it is often how we talk about probability in our day to day lives. The statement “the probability that the Democrats retain control of Congress in 2022 is 0.4” is a perfectly valid way to express how certain we are about some outcome. We can also apply this to things like coin flips: “I don’t have any reason to think either heads or tails is more likely, so the probability of heads is 50/50.”\nUnfortunately, the subjective view of probability does not provide any way to compare competing probability statements. If I say the probability of flipping a coin and getting heads is 1/2, but you say it is 1/8, who can we say is more correct without some sort of empirical verification? This problem also exists for one-time events. Let’s say I claim that the Democrats have a 20% chance of winning the election, and you claim they have an 80% chance of winning the election. The election happens and the Democrats win—who made the better prediction?\nSubjective probability is often associated with Bayesian theories of probability. Bayes’ theorem, however, is simply a well-established mathematical relation. And it is used in every type of probability framework. If you are not familiar with Bayes’ theorem, the basic version looks like\n\n\\text{P}(A|B) = \\frac{\\text{P}(A) \\ \\text{P}(B|A)}{\\text{P}(B)}\n\nThe most controversial part of this formula is \\text{P}(A), also known as the prior. Frequentist statistical methods ignore this part of Bayes’ theorem when conducting inference, in part because of its association with a “subjective” way of thinking. But by doing so they implicitly assume all values of \\text{P}(A) are equally likely—which is itself a subjective choice! We will cover Bayes’ theorem extensively in our chapter on conditional probability.\n\n\n\n\n\nThe Axiomatic Definition\nIn 1933 Russian mathematician Andrei Kolmogorov worked out a set of axioms governing how probability worked. Kolmogorov used a new field of mathematics called measure theory to construct a set of rules that probabilities must follow. Some of these rules might be familiar to you:\n\nThe probability of some event is a number greater than or equal to zero, and less than or equal to one.\nThe probability of an event, and the probability of not that event, must sum to one.\nThe probability of event A happening or event B happening is the sum of each of their independent probabilities minus their joint probability.\n\nAlong with a few others, these rules formed the basis upon which to calculate any complex probability problem. Thus, the axiomatic definition of probability is something along the lines of “probability is a function that maps events to a real number, obeying the axioms of probability.”2\nThat is all well and good, but the problem with the axiomatic definition is that it does not tell us anything about where probabilities come from, or what they mean in the real world. The axioms apply equally well to frequentist probability as well as subjective probability—which, as we saw, are philosophical antagonists.\nProbability as Logic\nThere are many ways to answer the question “what is probability?” But each of the definitions we’ve encountered so far have come with big drawbacks which limit their usefulness in social science research. Luckily there is one last way to conceptualize probability coming to our rescue. What if I told you that this final definition A) has the empirical rigor of frequentist and classical approaches, B) easily incorporates subjective background knowledge in a principled manner, and C) uses all the convenient mathematical rules from Kolmogorov’s axioms!\nThe trick is to think about probability as an extension of logic. Traditional Aristotelian logic deals with propositions that are only true or false such as “if P then Q; P; therefore Q”. Or the contrapositive “if P then Q; not Q; therefore not P”. But we never work with 100% true or 100% false propositions in social science. Rather, we would like to make inductive statements like “if P then Q is more plausible; Q; therefore P is more plausible”. Here is a more tangible example. Say we have a theory that being a democracy makes countries less likely to start wars. We then go out and collect data on countries and find a negative association between democracy and wars started. Therefore we conclude that our theory has been made more plausible.\nViewing probability as an extension of logic comes to us primarily from Edwin T. Jaynes and his 2003 book Probability Theory: the Logic of Science. For Jaynes, probability is the language we use to reason about propositions when we are operating in a state of incomplete information. Contrary to what proponents of frequentism might claim, probabilities do not exist as tangible properties about some set of events. Jaynes calls this the “mind-projection fallacy”—confusing the nature of things as they are with the practice of gathering knowledge about something. In other words, mistaking ontology for epistemology.\nOne of Jaynes’s major contributions was the insistence that all probability statements are provisional on background information or assumptions. So we should really write all probabilities as\n\n\\text{P}(A|X)\n\nWhere the “|” symbol means “conditional on”, and X stands for all the background knowledge we have relevant to the plausibility of proposition A. The beauty of this approach is that it allows us to combine the subjective and the objective when trying to solve a scientific problem. Reasonable people might disagree about what assumptions go into any particular X. But given some fixed X, everyone’s conclusions will necessarily follow the logic of probability and end up in the same place.\nThe Jaynesian view of probability is also Bayesian in the sense that we will be using all parts of Bayes’ theorem to conduct scientific inference. The prior probability of the hypothesis is a crucial ingredient in coming up with an accurate posterior probability of the hypothesis—the probability we assign to the hypothesis after collecting data on it."
  },
  {
    "objectID": "42_logic_probability.html",
    "href": "42_logic_probability.html",
    "title": "14  The Logic of Probability",
    "section": "",
    "text": "Armed now with our understanding of probability as an extension of logic, in this chapter we will learn to use the three basic logical operators on probability statements: NOT, AND, and OR. These three simple tools form the building blocks of every probability problem we will encounter.1"
  },
  {
    "objectID": "42_logic_probability.html#combining-probability-statements",
    "href": "42_logic_probability.html#combining-probability-statements",
    "title": "14  The Logic of Probability",
    "section": "\n14.1 Combining Probability Statements",
    "text": "14.1 Combining Probability Statements\nSo far we have only been looking at probability statements individually. “What is the probability of flipping a coin and getting heads?” or “What is the probability that the social democrat candidate wins?” But in real data analysis it is rare to analyze probabilities in isolation. The logical operators AND and OR provide the tools for combining probability statements into a single probability. “What is the probability of flipping a coin and getting heads AND rolling a dice and getting a six?” “What is the probability that the social democrat candidate wins OR the green party candidate wins?” One way of thinking about complex statistical models is that all they are doing is combining a bunch of probability statements together in order to generate predictions.\nAND\nWhen calculating the joint probability of two things occurring—that is, the probability of A AND the probability of B—we use the following notation:\n\n\\text{P}(A, B)\n\nIf A and B are independent, meaning that the probability of one does not depend on the other, this calculation is very simple. The following formula is known as the product rule of probability:\n\n\\text{P}(A, B) = \\text{P}(A) * \\text{P}(B)\n\nThis formula can get generalized to include any number of individual probabilities. By considering \\text{P}(A, B) to be a single probability we get\n\n\\text{P}(\\text{P}(A, B), C) = \\text{P}(A, B) * \\text{P}(C) = \\text{P}(A) * \\text{P}(B) * \\text{P}(C)\n\nEasy!\nUnfortunately, the probabilities we would like to combine with AND are rarely independent of one another. We will cover what to do in those situations in the next chapter.\nOR\nCalculating the probability of one event OR another event is slightly more complicated than combining those two events with AND. This is because the OR calculation is different depending on whether the two events are mutually exclusive or not. If two events are mutually exclusive, their joint probability is zero:\n\n\\text{P}(A, B) = 0\n\nIntuitively, if event A happens that makes event B is impossible, or vice-versa. Flipping a heads and flipping a tails are two mutually exclusive events because each one precludes the possibility of the other (in a single flip). Combining probabilities of mutually exclusive events with OR is actually pretty easy—we simply add up each individual probability.\n\n\\text{P}(heads) \\ OR \\ \\text{P}(tails) = \\frac{1}{2} + \\frac{1}{2} = 1\n\nIt is certain that when we flip a coin we will get a heads OR a tails.\nThings get trickier when trying to use OR on probabilities that are not mutually exclusive. Say we wanted to know the probability of flipping heads OR rolling a number less than six on a six-sided die. If we try adding these probabilities we get:\n\n\\text{P}(heads) \\ OR \\ \\text{P}(<6) = \\frac{1}{2} + \\frac{5}{6} = \\frac{4}{3}\n\nUh oh! Our final probability of 4/3 is greater than 1, which is impossible! The problem is that we are double counting outcomes where both events occur. In order to fix this, we need to subtract the joint probability of both events occurring from the sum of both events individually. This gives us the sum rule of probability:\n\n\\text{P}(A) \\ OR \\ \\text{P}(B) = \\text{P}(A) + \\text{P}(B) - \\text{P}(A, B)\n\nRecall how we defined whether two events are mutually exclusive earlier: \\text{P}(A, B) = 0. So if the two probabilities we want to combine with OR are mutually exclusive, that term drops out of the sum rule equation and we are able to add the individual probabilities together as before: \\text{P}(A) + \\text{P}(B)"
  },
  {
    "objectID": "42_logic_probability.html#the-binomial-probability-distribution",
    "href": "42_logic_probability.html#the-binomial-probability-distribution",
    "title": "14  The Logic of Probability",
    "section": "\n14.2 The Binomial Probability Distribution",
    "text": "14.2 The Binomial Probability Distribution\nWe encountered probability distributions back in chapter 4 in the context of generating fake data. Now we will look deeper into the math behind how probability distributions work. So far in this chapter we have only been able to use probability theory to solve very specific problems one at a time. “What is the probability of flipping a heads and rolling a six?” Sure, we now have the tools to answer this question, but we will have to recalculate everything from scratch if we changed “rolling a six” to “rolling a five or six”. Probability distributions are functions which we can use as templates to solve an entire class of problems at once!\nEach type of probability distribution describes a specific data generating process. In other words, given some causal sequence of events, what data should we expect to see? The binomial distribution comes about when we want to model the probability of getting a number of successful outcomes, given a number of trials and a probability of a successful outcome. The “bi” refers to having two types of outcomes: successes and not-successes. In social science research, the binomial distribution is used all the time to model binary outcomes via logistic regression. Examples include:\n\nDid a country go to war or not in a particular year?\nDid an individual vote?\nWas the bill passed or vetoed?\n\nAn Example\nThe binomial distribution has three parameters which govern its shape\n\n\nk, the number of successes\n\nn, the number of trials\n\np, the probability of success in each trial\n\nLet’s use a concrete example of voting behavior to make things clearer. Say we wanted to know the probability that an individual named Max turned out to vote in two of the past three elections, given that Max’s turnout rate is 30%. To solve this question we would plug in the following values for the parameters in a binomial distribution:\n\n\n2, the number of times Max turned out to vote\n\n3, the number of elections\n\n0.3, Max’s turn out rate^[For this example we are assuming that Max’s probability of turning out to vote in each election is independent of one another. In other words, voting in a previous election will not influence Max’s probability to vote in the next election.\n\nLet’s try to get a handle on this problem by counting the number of outcomes we care about. In a series of three elections, the possible ways Max can vote in exactly two of them are:\n\n\\text{V} = \\text{Max turned out to vote}\n\n\n\\text{N} = \\text{Max did not turn out to vote}\n\n\n\\text{VVN, VNV, NVV}\n\nEach of these sequences is equally likely because we assumed that each turn out decision Max makes is independent of the last. Notice how each of these sequences is also mutually exclusive.\n\n\\text{P(VVN, VNV, NVV)} = 0\n\nOne, and only one, of these sequences of votes will actually occur. Because each of these sequences is mutually exclusive, therefore, we can use the sum rule to add their probabilities together!\n\n\\text{P(VVN)} + \\text{P(VNV)} + \\text{P(NVV)}\n\nOr in more compact form:\n\n3 * \\text{P(Sequence where Max voted twice)}\n\nNow let’s work on calculating \\text{P(Sequence where Max voted twice)} and we will be all done. Because we know that each of the three sequences is equally likely, we will work with one of the sequences, \\text{P(VVN)}, and generalize from there. We know \\text{P(V)} = 0.3 because we assumed Max has an overall turnout rate of 30%. And from the negation rule, we know that \\text{P(N)} = 0.7 because \\text{P(V)} + \\text{P(N)} = 1. It is certain that Max either turns out to vote or he does not. Now we have everything we need to solve for \\text{P(VVN)} using the product rule:\n\n\\text{P(VVN)} = \\text{P(V)} * \\text{P(V)} * \\text{P(N)} = 0.3 * 0.3 * 0.7 = 0.063\n\nWe can now take that answer and multiply it by 3, the amount of outcomes where Max voted twice:\n\n3 * 0.063 = 0.189\n\nTo conclude, Max’s probability of turning out to vote in exactly two out of three elections, given that he turns out to vote 30% of the time, is 0.189. We can confirm this by using R’s dbinom() function:\n\ndbinom(x = 2, size = 3, prob = 0.3)\n\n[1] 0.189\n\n\nGeneralizing the Binomial Distribution\nMax’s voting behavior gave us a nice look at the binomial distribution in action. However, our solution relied on counting the relatively few sequences in which Max could have voted twice in three elections. If we wanted to know the probability of Max voting five times in ten elections, writing all the possible outcomes out would take forever! The solution involves using the binomial coefficient:\n\n{n \\choose k}\n\nWe read this expression as “n choose k”. It tells us the number of ways we can select the k outcomes we care about from the total number of trials n using the following formula:\n\n{n \\choose k} = \\frac{n!}{k!(n-k)!}\n\nR has a function for computing this value without typing out the whole formula.\n\nchoose(n = 10, k = 5)\n\n[1] 252\n\n\nWe also want to find an expression for multiplying many independent probabilities together, as we did when we calculated \\text{P(VVN)}. Rather than manually counting how many values for \\text{P(V)} and \\text{P(N)} we need to multiply together, let’s use exponents:\n\n\\text{P(VVN)} = \\text{P(V)}^2 * \\text{P(N)}^1\n\nWhich becomes the following in terms of k and n:\n\n\\text{P(V)}^k * \\text{P(N)}^{n-k}\n\nAnd we also want everything in terms of \\text{P(V)}, which is easy to do using our rule of negation:\n\n\\text{P(V)}^k * \\text{P(N)}^{n-k} = \\text{P(V)}^k * (1 -\\text{P(V)})^{n-k}\n\nLastly we wrap everything up by multiplying by our binomial coefficient and substituting the parameter p for \\text{P(V)}:\n\n{n \\choose k} * p^k * (1 - p)^{n-k}\n\nAnd this is the general equation for the binomial distribution! We call this a probability mass function (PMF) because it tells us how much of the total probability for a binomial distribution with fixed n and p is under the value k. Rather than writing this equation out each time we will usually use the following shorthand:\n\n\\text{Binomial}(k; n, p) = {n \\choose k} p^k (1 - p)^{n-k}\n\n\n14.2.1 Visualizing the Binomial PMF\nSometimes it can be nice to visualize a probability distribution to get a better sense of what it looks like when we change the parameters. Let’s take a look at the following binomial distribution with fixed n = 3.\n\nlibrary(tidyverse)\n\ndf <- tibble(group = c(\"prob = 0.3\", \n                       \"prob = 0.5\", \n                       \"prob = 0.7\"),\n             prob = c(0.3, 0.5, 0.7)) |> \n  crossing(k = 0:3) |> \n  mutate(y = dbinom(x = k, size = 3, prob = prob))\n\nggplot(df) +\n  aes(x = k, y = y) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\", width = 0.5) +\n  facet_wrap(~ group, nrow = 3) +\n  theme_classic() +\n  labs(x = \"k\", y = \"Probability\",\n       title = \"Binomial PMFs for n = 3\")\n\n\n\n\nThe height of each bar corresponds to the probability of a given value of k.\n\ndf <- tibble(group = c(\"n = 1\", \n                       \"n = 3\", \n                       \"n = 7\"),\n             n = c(1, 3, 7)) |> \n  crossing(k = 0:7) |> \n  mutate(y = dbinom(x = k, size = n, prob = 0.5))\n\nggplot(df) +\n  aes(x = k, y = y) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\", width = 0.5) +\n  facet_wrap(~ group, nrow = 3) +\n  theme_classic() +\n  labs(x = \"k\", y = \"Probability\",\n       title = \"Binomial PMFs for p = 0.5\") +\n  scale_x_continuous(breaks = 0:7)\n\n\n\n\nThe graph above shows a binomial distribution with p = 0.5 with different values of n."
  },
  {
    "objectID": "43_conditional.html",
    "href": "43_conditional.html",
    "title": "15  Conditional Probability",
    "section": "",
    "text": "In the last chapter we looked at how to combine independent probabilities together using AND and OR. Most real world probabilities that we would like to combine, however, are not independent of one another. On the one hand this is unfortunate because it requires us to use more complicated mathematics. But on the other hand, using what is known as conditional probability to combine non-independent probabilities together allows us to incorporate so much more information into our analyses. In fact, the rules of conditional probability are what make statistical inference possible in the first place!\nWe use the “|” symbol to denote conditionality. You can read the statement below as “the probability of A conditional on B being true”, or “given that B is true, this is the probability of A”\n\\text{P}(A|B)\nThe conditional notation allows us to make our definition of probabilistic independence more formal. The propositions A is independent of B if:\n\\text{P}(A|B) = \\text{P}(A)\nA nice way to think about the statement above is that B’s truth value provides no extra information about the truth of A. Or, B being true of false does not change our probability for A. Probabilistic independence is a tricky concept to grasp at first. In some ways, it is more natural to think of two probabilities as non-independent by default.\nLet’s use a concrete example to help us understand conditional probability. Say we want to investigate the relationship between partisanship in the US and UFO encounters. The overall percentage of Americans who claim to have had an encounter with a UFO is about 8%, but the numbers are higher for Republicans and lower for Democrats:\n\\text{P(alien encounter)} = 0.08\n\\text{P(alien encounter | Republican)} = 0.12\n\\text{P(alien encounter | Democrat)} = 0.04\nWe will also assume for the sake of simplicity that everyone in America is either a Republican or Democrat, and that they are split 50/50 in the population.\n\\text{P(Republican)} = \\text{P(Democrat)} = 0.5\nLet’s try to use our old product rule to calculate the probability that a randomly chosen American is a Republican AND claims to have had an alien encounter.\n\\text{P(Republican)} * \\text{P(alien encounter)} = 0.5 * 0.08 = 0.04\nBut if we do the same for Democrats we get the same answer!\n\\text{P(Democrat)} * \\text{P(alien encounter)} = 0.5 * 0.08 = 0.04\nThis does not make sense since we established that the probability of having an alien encounter changes conditional on an individual’s political party affiliation. So there must be something wrong with our product rule when trying to combine probabilities that are not independent.\nThe correct way to find out if a randomly chosen American is both a Republican and a UFO survivor is to multiply the partisanship probability by its conditional probability.\n\\text{P(Republican)} * \\text{P(alien encounter | Republican)} = 0.5 * 0.12 = 0.06\nThis gives us our updated product rule:\n\\text{P}(A, B) = \\text{P}(A) * \\text{P}(B|A)\nNote that this rule works for combining independent probabilities too because \\text{P}(B|A) = \\text{P}(B) in that case. While we’re at it, let’s also write down our updated sum rule:\n\\text{P}(A \\ \\text{or} \\ B) = \\text{P}(A) + \\text{P}(B) - \\text{P}(A) * \\text{P}(B|A)\nOur new product and sum rules can handle any number of propositions. The trick, like we saw in the last chapter, is to simply treat multiple probabilities as a single probability. Here is an example:\n\\begin{aligned}\n\\text{P}(A, B, C) &= \\text{P}(A, (B, C)) \\\\\n&= \\text{P}(A) * \\text{P}(B, C | A) \\\\\n&= \\text{P}(A) * \\text{P}(B|A) * \\text{P}(C|B, A)\n\\end{aligned}"
  },
  {
    "objectID": "43_conditional.html#bayes-theorem",
    "href": "43_conditional.html#bayes-theorem",
    "title": "15  Conditional Probability",
    "section": "\n15.1 Bayes’ Theorem",
    "text": "15.1 Bayes’ Theorem\nIn the previous example we were interested in finding out the probability of someone’s partisanship based on information about their UFO encounters. What if we wanted to ask the question in reverse? Given a partisan affiliation, what is the probability that someone claims to have encountered a UFO? Amazingly, thanks to conditional probability, we have everything we need to answer that question. The first step comes from realizing that \\text{P}(A, B) is the same as \\text{P}(B, A). According to the rules of logic, A AND B is the same as B AND A. Let’s apply our product rule to both of these quantities:\n$$\n\\begin{aligned}\n\n\\text{P}(B,A) &= \\text{P}(A, B) \\\\\n\\text{P}(B) * \\text{P}(A|B) &= \\text{P}(A) * \\text{P}(B|A)\n\\end{aligned}\n$$\nNow we just divide both sides by \\text{P}(B) and we get:\n\\text{P}(A|B) = \\frac{\\text{P}(A) * \\text{P}(B|A)}{\\text{P}(B)}\nThe most important formula in statistics and probability, Bayes’ theorem. To see why it is so useful let’s substitute our A and B placeholders for something more concrete. The goal of all scientific inference is to use evidence to inform our belief about whether something is true or false. In the language of conditional probability that would be \\text{P}(\\text{belief} \\ | \\ \\text{evidence}). This is the posterior probability in Bayes’ theorem. Substituting “belief” and “evidence” into the rest of the equation gives us:\n\\text{P}(\\text{belief} \\ | \\ \\text{evidence}) = \\frac{\\text{P}(\\text{belief}) * \\text{P}(\\text{evidence} \\ | \\ \\text{belief})}{\\text{P}(\\text{evidence})}\nOn the right hand side of the equation, \\text{P}(\\text{evidence} \\ | \\ \\text{belief}) is often called the “likelihood” or “sampling probability” of the evidence we have gathered. The ratio \\frac{\\text{P}(\\text{evidence} \\ | \\ \\text{belief})}{\\text{P}(\\text{evidence})} gives us a relative measure of how probable we were to gather this particular set of evidence given a certain belief. But this ratio needs to be multiplied by our prior probability of that belief being true, \\text{P}(\\text{belief}) in order to come up with a valid posterior probability of our belief."
  },
  {
    "objectID": "51_solutions-warmup.html",
    "href": "51_solutions-warmup.html",
    "title": "Warmup Questions Solutions",
    "section": "",
    "text": "Define the vectors u = \\begin{pmatrix} 1 \\\\2 \\\\3 \\end{pmatrix}, v = \\begin{pmatrix} 4\\\\5\\\\6 \\end{pmatrix}, and the scalar c = 2.\n\nu + v = \\begin{pmatrix}5\\\\7\\\\9\\end{pmatrix}\ncv = \\begin{pmatrix}8\\\\10\\\\12\\end{pmatrix}\nu \\cdot v = 1(4) + 2(5) + 3(6) = 32\n\nAre the following sets of vectors linearly independent?\n\n\nu = \\begin{pmatrix} 1\\\\ 2\\end{pmatrix}, v = \\begin{pmatrix} 2\\\\4\\end{pmatrix}\n\n\n\\leadsto No: 2u = \\begin{pmatrix} 2\\\\ 4\\end{pmatrix}, v = \\begin{pmatrix} 2\\\\ 4\\end{pmatrix} so infinitely many linear combinations of u and v that amount to 0 exist.\n\n\nu = \\begin{pmatrix} 1\\\\ 2\\\\ 5 \\end{pmatrix}, v = \\begin{pmatrix} 3\\\\ 7\\\\ 9 \\end{pmatrix}\n\n\n\\leadsto Yes: we cannot find linear combination of these two vectors that would amount to zero.\n\n\na = \\begin{pmatrix} 2\\\\ -1\\\\ 1 \\end{pmatrix}, b = \\begin{pmatrix} 3\\\\ -4\\\\ -2 \\end{pmatrix}, c = \\begin{pmatrix} 5\\\\ -10\\\\ -8 \\end{pmatrix}\n\n\n\\leadsto No: After playing around with some numbers, we can find that\n-2a = \\begin{pmatrix} -4\\\\ 2\\\\ -2 \\end{pmatrix}, 3b = \\begin{pmatrix} 9\\\\ -12\\\\ -6 \\end{pmatrix}, -1c = \\begin{pmatrix} -5\\\\ 10\\\\ 8 \\end{pmatrix}\nSo\n-2a + 3b - c = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\ni.e., a linear combination of these three vectors that would amount to zero exists.\n\n\\mathbf{A}=\\begin{pmatrix} 7 & 5 & 1 \\\\ 11 & 9 & 3 \\\\ 2 & 14 & 21 \\\\ 4 & 1 & 5 \\end{pmatrix}\nWhat is the dimensionality of matrix {\\bf A}? 4 \\times 3\nWhat is the element a_{23} of {\\bf A}? 3\nGiven that\n\\mathbf{B}=\\begin{pmatrix} 1 & 2 & 8 \\\\ 3 & 9 & 11 \\\\ 4 & 7 & 5 \\\\ 5 & 1 & 9 \\end{pmatrix}\n\\mathbf{A} + \\mathbf{B} = \\begin{pmatrix} 8 & 7 & 9 \\\\ 14 & 18 & 14 \\\\ 6 & 21 & 26 \\\\ 9 & 2 & 14 \\end{pmatrix}\nGiven that\n{\\bf C}=\\begin{pmatrix} 1 & 2 & 8 \\\\ 3 & 9 & 11 \\\\  4 & 7 & 5 \\\\ \\end{pmatrix}\n\\mathbf{A} + \\mathbf{C} = \\text{No solution, matrices non-conformable}\nGiven that\nc = 2\nc\\textbf{A} = \\begin{pmatrix}\n            14 & 10 & 2 \\\\\n            22 & 18 & 6 \\\\\n            4 & 28 & 42 \\\\\n            8 & 2 & 10\n        \\end{pmatrix}"
  },
  {
    "objectID": "51_solutions-warmup.html#operations",
    "href": "51_solutions-warmup.html#operations",
    "title": "Warmup Questions Solutions",
    "section": "Operations",
    "text": "Operations\nSummation\nSimplify the following\n\n\\sum\\limits_{i = 1}^3 i = 1 + 2+ 3 = 6\n\\sum\\limits_{k = 1}^3(3k + 2) = 3\\sum\\limits_{k=1}^3k + \\sum\\limits_{k=1}^3 2= 3\\times 6 + 3\\times 2 = 24\n\\sum\\limits_{i= 1}^4 (3k + i + 2) = 3\\sum\\limits_{i= 1}^4k + \\sum\\limits_{i= 1}^4i + \\sum\\limits_{i= 1}^42 = 12k + 10 + 8 = 12k + 18\nProducts\n\n\\prod\\limits_{i= 1}^3 i = 1\\cdot 2\\cdot 3 = 6\n\\prod\\limits_{k=1}^3(3k + 2) = (3 + 2)\\cdot (6 + 2)\\cdot (9 + 2) = 440\nLogs and exponents\nSimplify the following\n\n4^2 = 16\n4^2 2^3 = 2^{2\\cdot 2}2^{3} = 2^{4 + 3} = 128\n\\log_{10}100 = \\log_{10}10^2 = 2\n\\log_{2}4 = \\log_{2}2^2 = 2\nwhen \\log is the natural log, \\log e = \\log_{e} e^1 = 1\n\nwhen a, b, c are each constants, e^a e^b e^c = e^{a + b + c},\n\n\\log 0 = \\text{undefined} – no exponentiation of anything will result in a 0.\n\ne^0 = 1 – any number raised to the 0 is always 1.\n\ne^1 = e – any number raised to the 1 is always itself\n\\log e^2 = \\log_e e^2 = 2"
  },
  {
    "objectID": "51_solutions-warmup.html#limits",
    "href": "51_solutions-warmup.html#limits",
    "title": "Warmup Questions Solutions",
    "section": "Limits",
    "text": "Limits\nFind the limit of the following.\n\n\\lim\\limits_{x \\to 2} (x - 1) = 1\n\n\\lim\\limits_{x \\to 2} \\frac{(x - 2) (x - 1)}{(x - 2)} = 1, though note that the original function \\frac{(x - 2) (x - 1)}{(x - 2)} would have been undefined at x = 2 because of a divide by zero problem; otherwise it would have been equal to x - 1.\n\n\\lim\\limits_{x \\to 2}\\frac{x^2 - 3x + 2}{x- 2} = 1, same as above."
  },
  {
    "objectID": "51_solutions-warmup.html#calculus",
    "href": "51_solutions-warmup.html#calculus",
    "title": "Warmup Questions Solutions",
    "section": "Calculus",
    "text": "Calculus\nFor each of the following functions f(x), find the derivative f'(x) or \\frac{d}{dx}f(x)\n\n\nf(x)=c, f'(x) = 0\n\n\nf(x)=x, f'(x) = 1\n\n\nf(x)=x^2, f'(x) = 2x\n\n\nf(x)=x^3, f'(x) = 3x^2\n\n\nf(x)=3x^2+2x^{1/3}, f'(x) = 6x + \\frac{2}{3}x^{-2/3}\n\n\nf(x)=(x^3)(2x^4), f'(x) = \\frac{d}{dx}2x^7 = 14x^6"
  },
  {
    "objectID": "51_solutions-warmup.html#optimization",
    "href": "51_solutions-warmup.html#optimization",
    "title": "Warmup Questions Solutions",
    "section": "Optimization",
    "text": "Optimization\nFor each of the followng functions f(x), does a maximum and minimum exist in the domain x \\in \\mathbf{R}? If so, for what are those values and for which values of x?\n\n\nf(x) = x \\leadsto neither exists.\n\nf(x) = x^2 \\leadsto a minimum f(x) = 0 exists at x = 0, but not a maximum.\n\nf(x) = -(x - 2)^2 \\leadsto a maximum f(x) = 0 exists at x = 2, but not a minimum.\n\nIf you are stuck, please try sketching out a picture of each of the functions."
  },
  {
    "objectID": "51_solutions-warmup.html#probability",
    "href": "51_solutions-warmup.html#probability",
    "title": "Warmup Questions Solutions",
    "section": "Probability",
    "text": "Probability\n\nIf there are 12 cards, numbered 1 to 12, and 4 cards are chosen, \\binom{12}{4} = \\frac{12\\cdot 11\\cdot 10\\cdot 9}{4!} = 495 possible hands exist (unordered, without replacement) .\nLet A = \\{1,3,5,7,8\\} and B = \\{2,4,7,8,12,13\\}. Then A \\cup B = \\{1, 2, 3, 4, 5, 7, 8, 12, 13\\}, A \\cap B = \\{7, 8\\}? If A is a subset of the Sample Space S = \\{1,2,3,4,5,6,7,8,9,10\\}, then the complement A^C = \\{2, 4, 6, 9, 10\\}\nIf we roll two fair dice, what is the probability that their sum would be 11? \\leadsto \\frac{1}{18}\nIf we roll two fair dice, what is the probability that their sum would be 12? \\leadsto \\frac{1}{36}. There are two independent dice, so 6^2 = 36 options in total. While the previous question had two possibilities for a sum of 11 (5,6 and 6,5), there is only one possibility out of 36 for a sum of 12 (6,6)."
  }
]